{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "time_series_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZGfS6OjZoHFf",
        "q3ge0FR_1egd",
        "EkAtONY2838A",
        "xLSk6iTyR7-2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamrika/Deep-Learning/blob/main/time_series_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulaj1-o9bwBP"
      },
      "source": [
        "# <center> Time Series Prediction Using Recurrent Neural Network</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ACxS-bQcFXd"
      },
      "source": [
        "This notebook demonstrates the application of recurrent neural networks in solving sequence prediction problem. \n",
        "\n",
        "We will be working with the weather forecasting problem, where we have access to a timeseries of data points coming from sensors installed on the roof of a building, such as temperature, air pressure, and humidity, which we use to predict what the temperature will be 24 hours after the last data point collected. This is a fairly challenging problem that exemplifies many common difficulties encountered when working with timeseries.\n",
        "\n",
        "We first start with a simple non-recurrent fully connected network as a benchmark. Then we create our first baseline recurrent model improve it using drop out regularization, bidirectional connections, and stacking more recurrent layers.  \n",
        "\n",
        "The materials covered in this lab are mostly from \"Deep Learning with Python\" book by Francois Chollet, chapter 6, section 3 and this [tensorflow tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series) \n",
        "\n",
        "Before proceeding, at the top menu of this colab notebook select Runtime-->change runtime type--->GPU to seepd up running the code segments in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGfS6OjZoHFf"
      },
      "source": [
        "## A temperature forecasting problem\n",
        "The weather dataset used in this lab is collected by  the Max Planck Institute for Biogeochemistry.\n",
        "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book Deep Learning with Python.\n",
        "\n",
        "Let's import the libraries we need and download the data into your colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeFBAspDUWzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8ebe6d-1be4-4ef0-8262-ac089678143d"
      },
      "source": [
        "!pip install pandas\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pzoThwwUNbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89512acb-386a-4895-cc52-5a23ec26411e"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "df = pd.read_csv(zip_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "13574144/13568290 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKD5k824U_ha"
      },
      "source": [
        "Let's take a look at the few records in the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG7zLLP8Xeh_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ab80fbc-647f-4b0c-dc10-20f02a44afd8"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date Time</th>\n",
              "      <th>p (mbar)</th>\n",
              "      <th>T (degC)</th>\n",
              "      <th>Tpot (K)</th>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <th>rh (%)</th>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <th>wv (m/s)</th>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <th>wd (deg)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>01.01.2009 00:10:00</td>\n",
              "      <td>996.52</td>\n",
              "      <td>-8.02</td>\n",
              "      <td>265.40</td>\n",
              "      <td>-8.90</td>\n",
              "      <td>93.3</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.94</td>\n",
              "      <td>3.12</td>\n",
              "      <td>1307.75</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.75</td>\n",
              "      <td>152.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>01.01.2009 00:20:00</td>\n",
              "      <td>996.57</td>\n",
              "      <td>-8.41</td>\n",
              "      <td>265.01</td>\n",
              "      <td>-9.28</td>\n",
              "      <td>93.4</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.89</td>\n",
              "      <td>3.03</td>\n",
              "      <td>1309.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1.50</td>\n",
              "      <td>136.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01.01.2009 00:30:00</td>\n",
              "      <td>996.53</td>\n",
              "      <td>-8.51</td>\n",
              "      <td>264.91</td>\n",
              "      <td>-9.31</td>\n",
              "      <td>93.9</td>\n",
              "      <td>3.21</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1310.24</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.63</td>\n",
              "      <td>171.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01.01.2009 00:40:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.31</td>\n",
              "      <td>265.12</td>\n",
              "      <td>-9.07</td>\n",
              "      <td>94.2</td>\n",
              "      <td>3.26</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1309.19</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.50</td>\n",
              "      <td>198.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01.01.2009 00:50:00</td>\n",
              "      <td>996.51</td>\n",
              "      <td>-8.27</td>\n",
              "      <td>265.15</td>\n",
              "      <td>-9.04</td>\n",
              "      <td>94.1</td>\n",
              "      <td>3.27</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.92</td>\n",
              "      <td>3.09</td>\n",
              "      <td>1309.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.63</td>\n",
              "      <td>214.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>01.01.2009 01:00:00</td>\n",
              "      <td>996.50</td>\n",
              "      <td>-8.05</td>\n",
              "      <td>265.38</td>\n",
              "      <td>-8.78</td>\n",
              "      <td>94.4</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.14</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.96</td>\n",
              "      <td>3.15</td>\n",
              "      <td>1307.86</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.63</td>\n",
              "      <td>192.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>01.01.2009 01:10:00</td>\n",
              "      <td>996.50</td>\n",
              "      <td>-7.62</td>\n",
              "      <td>265.81</td>\n",
              "      <td>-8.30</td>\n",
              "      <td>94.8</td>\n",
              "      <td>3.44</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.18</td>\n",
              "      <td>2.04</td>\n",
              "      <td>3.27</td>\n",
              "      <td>1305.68</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.63</td>\n",
              "      <td>166.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>01.01.2009 01:20:00</td>\n",
              "      <td>996.50</td>\n",
              "      <td>-7.62</td>\n",
              "      <td>265.81</td>\n",
              "      <td>-8.36</td>\n",
              "      <td>94.4</td>\n",
              "      <td>3.44</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.19</td>\n",
              "      <td>2.03</td>\n",
              "      <td>3.26</td>\n",
              "      <td>1305.69</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.50</td>\n",
              "      <td>118.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>01.01.2009 01:30:00</td>\n",
              "      <td>996.50</td>\n",
              "      <td>-7.91</td>\n",
              "      <td>265.52</td>\n",
              "      <td>-8.73</td>\n",
              "      <td>93.8</td>\n",
              "      <td>3.36</td>\n",
              "      <td>3.15</td>\n",
              "      <td>0.21</td>\n",
              "      <td>1.97</td>\n",
              "      <td>3.16</td>\n",
              "      <td>1307.17</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.75</td>\n",
              "      <td>188.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>01.01.2009 01:40:00</td>\n",
              "      <td>996.53</td>\n",
              "      <td>-8.43</td>\n",
              "      <td>264.99</td>\n",
              "      <td>-9.34</td>\n",
              "      <td>93.1</td>\n",
              "      <td>3.23</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.88</td>\n",
              "      <td>3.02</td>\n",
              "      <td>1309.85</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.88</td>\n",
              "      <td>185.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Date Time  p (mbar)  T (degC)  ...  wv (m/s)  max. wv (m/s)  wd (deg)\n",
              "0  01.01.2009 00:10:00    996.52     -8.02  ...      1.03           1.75     152.3\n",
              "1  01.01.2009 00:20:00    996.57     -8.41  ...      0.72           1.50     136.1\n",
              "2  01.01.2009 00:30:00    996.53     -8.51  ...      0.19           0.63     171.6\n",
              "3  01.01.2009 00:40:00    996.51     -8.31  ...      0.34           0.50     198.0\n",
              "4  01.01.2009 00:50:00    996.51     -8.27  ...      0.32           0.63     214.3\n",
              "5  01.01.2009 01:00:00    996.50     -8.05  ...      0.21           0.63     192.7\n",
              "6  01.01.2009 01:10:00    996.50     -7.62  ...      0.18           0.63     166.5\n",
              "7  01.01.2009 01:20:00    996.50     -7.62  ...      0.19           0.50     118.6\n",
              "8  01.01.2009 01:30:00    996.50     -7.91  ...      0.28           0.75     188.5\n",
              "9  01.01.2009 01:40:00    996.53     -8.43  ...      0.59           0.88     185.0\n",
              "\n",
              "[10 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk3Ql8aVoSXo"
      },
      "source": [
        "## Preparing data for Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3ge0FR_1egd"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnIXNzs-Xk9l"
      },
      "source": [
        "The data is recorded in ten minutes intervals (rows are 10 minutes apart). We want to do **hourly prediction**, that is, we want to predict the temprature in the next hour given the previous hours. So we subsample data from 10 minutes intervals to one hour intervals by taking every 6 records starting from index 5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUyog4FLmx4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459cd39a-e74b-4718-c461-63f3efddc36f"
      },
      "source": [
        "# slice [start:stop:step], starting from index 5 take every 6th record.\n",
        "df = df[5::6]\n",
        "\n",
        "#take a look at the summary statistics of each column\n",
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>p (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>989.212842</td>\n",
              "      <td>8.358886</td>\n",
              "      <td>913.60</td>\n",
              "      <td>984.20</td>\n",
              "      <td>989.57</td>\n",
              "      <td>994.720</td>\n",
              "      <td>1015.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T (degC)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.450482</td>\n",
              "      <td>8.423384</td>\n",
              "      <td>-22.76</td>\n",
              "      <td>3.35</td>\n",
              "      <td>9.41</td>\n",
              "      <td>15.480</td>\n",
              "      <td>37.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tpot (K)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>283.493086</td>\n",
              "      <td>8.504424</td>\n",
              "      <td>250.85</td>\n",
              "      <td>277.44</td>\n",
              "      <td>283.46</td>\n",
              "      <td>289.530</td>\n",
              "      <td>311.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>4.956471</td>\n",
              "      <td>6.730081</td>\n",
              "      <td>-24.80</td>\n",
              "      <td>0.24</td>\n",
              "      <td>5.21</td>\n",
              "      <td>10.080</td>\n",
              "      <td>23.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rh (%)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>76.009788</td>\n",
              "      <td>16.474920</td>\n",
              "      <td>13.88</td>\n",
              "      <td>65.21</td>\n",
              "      <td>79.30</td>\n",
              "      <td>89.400</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>13.576576</td>\n",
              "      <td>7.739883</td>\n",
              "      <td>0.97</td>\n",
              "      <td>7.77</td>\n",
              "      <td>11.82</td>\n",
              "      <td>17.610</td>\n",
              "      <td>63.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.533968</td>\n",
              "      <td>4.183658</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6.22</td>\n",
              "      <td>8.86</td>\n",
              "      <td>12.360</td>\n",
              "      <td>28.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>4.042536</td>\n",
              "      <td>4.898549</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.87</td>\n",
              "      <td>2.19</td>\n",
              "      <td>5.300</td>\n",
              "      <td>46.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>6.022560</td>\n",
              "      <td>2.655812</td>\n",
              "      <td>0.51</td>\n",
              "      <td>3.92</td>\n",
              "      <td>5.59</td>\n",
              "      <td>7.800</td>\n",
              "      <td>18.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.640437</td>\n",
              "      <td>4.234862</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6.29</td>\n",
              "      <td>8.96</td>\n",
              "      <td>12.490</td>\n",
              "      <td>28.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>1216.061232</td>\n",
              "      <td>39.974263</td>\n",
              "      <td>1059.45</td>\n",
              "      <td>1187.47</td>\n",
              "      <td>1213.80</td>\n",
              "      <td>1242.765</td>\n",
              "      <td>1393.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wv (m/s)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>1.702567</td>\n",
              "      <td>65.447512</td>\n",
              "      <td>-9999.00</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.860</td>\n",
              "      <td>14.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>2.963041</td>\n",
              "      <td>75.597657</td>\n",
              "      <td>-9999.00</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.98</td>\n",
              "      <td>4.740</td>\n",
              "      <td>23.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wd (deg)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>174.789095</td>\n",
              "      <td>86.619431</td>\n",
              "      <td>0.00</td>\n",
              "      <td>125.30</td>\n",
              "      <td>198.10</td>\n",
              "      <td>234.000</td>\n",
              "      <td>360.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   count         mean        std  ...      50%       75%      max\n",
              "p (mbar)         70091.0   989.212842   8.358886  ...   989.57   994.720  1015.29\n",
              "T (degC)         70091.0     9.450482   8.423384  ...     9.41    15.480    37.28\n",
              "Tpot (K)         70091.0   283.493086   8.504424  ...   283.46   289.530   311.21\n",
              "Tdew (degC)      70091.0     4.956471   6.730081  ...     5.21    10.080    23.06\n",
              "rh (%)           70091.0    76.009788  16.474920  ...    79.30    89.400   100.00\n",
              "VPmax (mbar)     70091.0    13.576576   7.739883  ...    11.82    17.610    63.77\n",
              "VPact (mbar)     70091.0     9.533968   4.183658  ...     8.86    12.360    28.25\n",
              "VPdef (mbar)     70091.0     4.042536   4.898549  ...     2.19     5.300    46.01\n",
              "sh (g/kg)        70091.0     6.022560   2.655812  ...     5.59     7.800    18.07\n",
              "H2OC (mmol/mol)  70091.0     9.640437   4.234862  ...     8.96    12.490    28.74\n",
              "rho (g/m**3)     70091.0  1216.061232  39.974263  ...  1213.80  1242.765  1393.54\n",
              "wv (m/s)         70091.0     1.702567  65.447512  ...     1.76     2.860    14.01\n",
              "max. wv (m/s)    70091.0     2.963041  75.597657  ...     2.98     4.740    23.50\n",
              "wd (deg)         70091.0   174.789095  86.619431  ...   198.10   234.000   360.00\n",
              "\n",
              "[14 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDMHYliqzgvx"
      },
      "source": [
        "One thing that should stand out is the min value of the wind velocity, wv (m/s) and max. wv (m/s) columns. This -9999 is likely erroneous. There's a separate wind direction column, so the velocity should be >=0. Replace it with zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwrNHO6koU-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ebc497-5681-46cc-95b1-bbc90f4aa64f"
      },
      "source": [
        "#replacing -9999 with 0 in wv column\n",
        "wv= df['wv (m/s)']\n",
        "df.loc[wv==-9999.00, 'wv (m/s)']=0\n",
        "\n",
        "#replacing -9999 with 0 in mwv column\n",
        "mwv=df['max. wv (m/s)']\n",
        "df.loc[mwv==-9999.00,'max. wv (m/s)']=0\n",
        "\n",
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>p (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>989.212842</td>\n",
              "      <td>8.358886</td>\n",
              "      <td>913.60</td>\n",
              "      <td>984.20</td>\n",
              "      <td>989.57</td>\n",
              "      <td>994.720</td>\n",
              "      <td>1015.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T (degC)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.450482</td>\n",
              "      <td>8.423384</td>\n",
              "      <td>-22.76</td>\n",
              "      <td>3.35</td>\n",
              "      <td>9.41</td>\n",
              "      <td>15.480</td>\n",
              "      <td>37.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tpot (K)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>283.493086</td>\n",
              "      <td>8.504424</td>\n",
              "      <td>250.85</td>\n",
              "      <td>277.44</td>\n",
              "      <td>283.46</td>\n",
              "      <td>289.530</td>\n",
              "      <td>311.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>4.956471</td>\n",
              "      <td>6.730081</td>\n",
              "      <td>-24.80</td>\n",
              "      <td>0.24</td>\n",
              "      <td>5.21</td>\n",
              "      <td>10.080</td>\n",
              "      <td>23.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rh (%)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>76.009788</td>\n",
              "      <td>16.474920</td>\n",
              "      <td>13.88</td>\n",
              "      <td>65.21</td>\n",
              "      <td>79.30</td>\n",
              "      <td>89.400</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>13.576576</td>\n",
              "      <td>7.739883</td>\n",
              "      <td>0.97</td>\n",
              "      <td>7.77</td>\n",
              "      <td>11.82</td>\n",
              "      <td>17.610</td>\n",
              "      <td>63.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.533968</td>\n",
              "      <td>4.183658</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6.22</td>\n",
              "      <td>8.86</td>\n",
              "      <td>12.360</td>\n",
              "      <td>28.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>4.042536</td>\n",
              "      <td>4.898549</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.87</td>\n",
              "      <td>2.19</td>\n",
              "      <td>5.300</td>\n",
              "      <td>46.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>6.022560</td>\n",
              "      <td>2.655812</td>\n",
              "      <td>0.51</td>\n",
              "      <td>3.92</td>\n",
              "      <td>5.59</td>\n",
              "      <td>7.800</td>\n",
              "      <td>18.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.640437</td>\n",
              "      <td>4.234862</td>\n",
              "      <td>0.81</td>\n",
              "      <td>6.29</td>\n",
              "      <td>8.96</td>\n",
              "      <td>12.490</td>\n",
              "      <td>28.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>1216.061232</td>\n",
              "      <td>39.974263</td>\n",
              "      <td>1059.45</td>\n",
              "      <td>1187.47</td>\n",
              "      <td>1213.80</td>\n",
              "      <td>1242.765</td>\n",
              "      <td>1393.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wv (m/s)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>2.130539</td>\n",
              "      <td>1.543098</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.99</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.860</td>\n",
              "      <td>14.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max. wv (m/s)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>3.533671</td>\n",
              "      <td>2.343417</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.76</td>\n",
              "      <td>2.98</td>\n",
              "      <td>4.740</td>\n",
              "      <td>23.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wd (deg)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>174.789095</td>\n",
              "      <td>86.619431</td>\n",
              "      <td>0.00</td>\n",
              "      <td>125.30</td>\n",
              "      <td>198.10</td>\n",
              "      <td>234.000</td>\n",
              "      <td>360.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   count         mean        std  ...      50%       75%      max\n",
              "p (mbar)         70091.0   989.212842   8.358886  ...   989.57   994.720  1015.29\n",
              "T (degC)         70091.0     9.450482   8.423384  ...     9.41    15.480    37.28\n",
              "Tpot (K)         70091.0   283.493086   8.504424  ...   283.46   289.530   311.21\n",
              "Tdew (degC)      70091.0     4.956471   6.730081  ...     5.21    10.080    23.06\n",
              "rh (%)           70091.0    76.009788  16.474920  ...    79.30    89.400   100.00\n",
              "VPmax (mbar)     70091.0    13.576576   7.739883  ...    11.82    17.610    63.77\n",
              "VPact (mbar)     70091.0     9.533968   4.183658  ...     8.86    12.360    28.25\n",
              "VPdef (mbar)     70091.0     4.042536   4.898549  ...     2.19     5.300    46.01\n",
              "sh (g/kg)        70091.0     6.022560   2.655812  ...     5.59     7.800    18.07\n",
              "H2OC (mmol/mol)  70091.0     9.640437   4.234862  ...     8.96    12.490    28.74\n",
              "rho (g/m**3)     70091.0  1216.061232  39.974263  ...  1213.80  1242.765  1393.54\n",
              "wv (m/s)         70091.0     2.130539   1.543098  ...     1.76     2.860    14.01\n",
              "max. wv (m/s)    70091.0     3.533671   2.343417  ...     2.98     4.740    23.50\n",
              "wd (deg)         70091.0   174.789095  86.619431  ...   198.10   234.000   360.00\n",
              "\n",
              "[14 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkAtONY2838A"
      },
      "source": [
        "### Feature Engineering\n",
        "There are a couple features that need further treatment.  The last column of the data, wd (deg), gives the wind direction in units of degrees. Angles do not make good model inputs, 360° and 0° should be close to each other, and wrap around smoothly. Direction shouldn't matter if the wind is not blowing. \n",
        "The [tensorflow tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series) does a couple useful feature engineering on the wind direction and its velacity. The wind forms a vector ( the direction is the angle and the velocity is the magnitude of the vector). We can convert this vector to its X and Y. We can get the x and y coordinates by multiplying the velocity by the cosine and sine of the wind radians respectively.  This conversion will make the distance between the wind vectors meaningful for the model.\n",
        "\n",
        "We can ndo this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hsHm1Ai-_aO"
      },
      "source": [
        "#removing the wind velocity (wv (m/s)) and max wind velocity (max. wv (m/s))\n",
        "wv = df.pop('wv (m/s)')\n",
        "max_wv = df.pop('max. wv (m/s)')\n",
        "\n",
        "# remove the wind degree column and convert it to radians.\n",
        "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
        "\n",
        "# Calculate the wind x and y components of the wind vector.\n",
        "df['Wx'] = wv*np.cos(wd_rad)\n",
        "df['Wy'] = wv*np.sin(wd_rad)\n",
        "\n",
        "# Calculate the max wind x and y components of the max wind vector.\n",
        "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
        "df['max Wy'] = max_wv*np.sin(wd_rad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rzgC-XCAEQ4"
      },
      "source": [
        "Another useful feature engineering suggested by tensorflow is to convert the date time to time of the day and time of the year. These two features are intuitively more predictive of the temprature than the absolute date/time value.\n",
        "\n",
        "Since the weather data has clear daily and yearly periodicity, we can look at day and year as  unit circles and then model the time of day and day of the year as a vector in that circle. similar to the wind feature, we can use sin and cosine to get the x and y components of the time vector.\n",
        "\n",
        "The following code segment first converts the time stamp to the seconds. \n",
        "It then divides the time stamp by the number of seconds in a day and the number of seconds in a year and  multiply them by $2\\pi$ to get the radians for the time of day and day of year vectors. Finally it takes the sin and cosine of the radians to compute the x and y components for each vector (similar to what we did for the wind)\n",
        "\n",
        "converting the time stamp this way captures the daily and yearly periodicity of the temprature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJgMBGiTEmG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "facbf9ee-e6a4-4a68-92bd-cb087b70a041"
      },
      "source": [
        "#convert date time from string to datetime object. \n",
        "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "#get the time stamp in seconds\n",
        "timestamp_seconds = date_time.astype('int64') // 10**9\n",
        "\n",
        "# compute the number of seconds in a day and year\n",
        "seconds_in_a_day= 24*60*60\n",
        "seconds_in_a_year=(365.2425)*seconds_in_a_day\n",
        "\n",
        "#conver the timestamp to time of day and time of year x and y components\n",
        "df['Day y'] = np.sin(timestamp_seconds * (2 * np.pi / seconds_in_a_day))\n",
        "df['Day x'] = np.cos(timestamp_seconds * (2 * np.pi / seconds_in_a_day))\n",
        "df['Year y'] = np.sin(timestamp_seconds * (2 * np.pi / seconds_in_a_year))\n",
        "df['Year x'] = np.cos(timestamp_seconds * (2 * np.pi / seconds_in_a_year))\n",
        "\n",
        "#let's look at the data again\n",
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>p (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>989.212842</td>\n",
              "      <td>8.358886</td>\n",
              "      <td>913.600000</td>\n",
              "      <td>984.200000</td>\n",
              "      <td>9.895700e+02</td>\n",
              "      <td>994.720000</td>\n",
              "      <td>1015.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T (degC)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.450482</td>\n",
              "      <td>8.423384</td>\n",
              "      <td>-22.760000</td>\n",
              "      <td>3.350000</td>\n",
              "      <td>9.410000e+00</td>\n",
              "      <td>15.480000</td>\n",
              "      <td>37.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tpot (K)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>283.493086</td>\n",
              "      <td>8.504424</td>\n",
              "      <td>250.850000</td>\n",
              "      <td>277.440000</td>\n",
              "      <td>2.834600e+02</td>\n",
              "      <td>289.530000</td>\n",
              "      <td>311.210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tdew (degC)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>4.956471</td>\n",
              "      <td>6.730081</td>\n",
              "      <td>-24.800000</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>5.210000e+00</td>\n",
              "      <td>10.080000</td>\n",
              "      <td>23.060000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rh (%)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>76.009788</td>\n",
              "      <td>16.474920</td>\n",
              "      <td>13.880000</td>\n",
              "      <td>65.210000</td>\n",
              "      <td>7.930000e+01</td>\n",
              "      <td>89.400000</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPmax (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>13.576576</td>\n",
              "      <td>7.739883</td>\n",
              "      <td>0.970000</td>\n",
              "      <td>7.770000</td>\n",
              "      <td>1.182000e+01</td>\n",
              "      <td>17.610000</td>\n",
              "      <td>63.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPact (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.533968</td>\n",
              "      <td>4.183658</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>6.220000</td>\n",
              "      <td>8.860000e+00</td>\n",
              "      <td>12.360000</td>\n",
              "      <td>28.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VPdef (mbar)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>4.042536</td>\n",
              "      <td>4.898549</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>2.190000e+00</td>\n",
              "      <td>5.300000</td>\n",
              "      <td>46.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sh (g/kg)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>6.022560</td>\n",
              "      <td>2.655812</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>3.920000</td>\n",
              "      <td>5.590000e+00</td>\n",
              "      <td>7.800000</td>\n",
              "      <td>18.070000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H2OC (mmol/mol)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>9.640437</td>\n",
              "      <td>4.234862</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>6.290000</td>\n",
              "      <td>8.960000e+00</td>\n",
              "      <td>12.490000</td>\n",
              "      <td>28.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rho (g/m**3)</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>1216.061232</td>\n",
              "      <td>39.974263</td>\n",
              "      <td>1059.450000</td>\n",
              "      <td>1187.470000</td>\n",
              "      <td>1.213800e+03</td>\n",
              "      <td>1242.765000</td>\n",
              "      <td>1393.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Wx</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-0.627813</td>\n",
              "      <td>1.987440</td>\n",
              "      <td>-11.305514</td>\n",
              "      <td>-1.470727</td>\n",
              "      <td>-6.331423e-01</td>\n",
              "      <td>0.299975</td>\n",
              "      <td>8.244699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Wy</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-0.407068</td>\n",
              "      <td>1.552621</td>\n",
              "      <td>-8.274385</td>\n",
              "      <td>-1.364699</td>\n",
              "      <td>-2.934675e-01</td>\n",
              "      <td>0.450077</td>\n",
              "      <td>7.733831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max Wx</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-1.018681</td>\n",
              "      <td>3.095279</td>\n",
              "      <td>-19.641473</td>\n",
              "      <td>-2.469210</td>\n",
              "      <td>-1.117029e+00</td>\n",
              "      <td>0.627619</td>\n",
              "      <td>11.913133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max Wy</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-0.733589</td>\n",
              "      <td>2.611890</td>\n",
              "      <td>-14.883367</td>\n",
              "      <td>-2.322709</td>\n",
              "      <td>-5.270212e-01</td>\n",
              "      <td>0.822895</td>\n",
              "      <td>14.302308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Day y</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>0.707099</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.707107</td>\n",
              "      <td>1.321740e-14</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Day x</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>0.707124</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.707107</td>\n",
              "      <td>-9.732065e-15</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year y</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.706813</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.704504</td>\n",
              "      <td>3.316307e-03</td>\n",
              "      <td>0.708301</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year x</th>\n",
              "      <td>70091.0</td>\n",
              "      <td>-0.000672</td>\n",
              "      <td>0.707408</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.708115</td>\n",
              "      <td>-1.121764e-03</td>\n",
              "      <td>0.707423</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   count         mean  ...          75%          max\n",
              "p (mbar)         70091.0   989.212842  ...   994.720000  1015.290000\n",
              "T (degC)         70091.0     9.450482  ...    15.480000    37.280000\n",
              "Tpot (K)         70091.0   283.493086  ...   289.530000   311.210000\n",
              "Tdew (degC)      70091.0     4.956471  ...    10.080000    23.060000\n",
              "rh (%)           70091.0    76.009788  ...    89.400000   100.000000\n",
              "VPmax (mbar)     70091.0    13.576576  ...    17.610000    63.770000\n",
              "VPact (mbar)     70091.0     9.533968  ...    12.360000    28.250000\n",
              "VPdef (mbar)     70091.0     4.042536  ...     5.300000    46.010000\n",
              "sh (g/kg)        70091.0     6.022560  ...     7.800000    18.070000\n",
              "H2OC (mmol/mol)  70091.0     9.640437  ...    12.490000    28.740000\n",
              "rho (g/m**3)     70091.0  1216.061232  ...  1242.765000  1393.540000\n",
              "Wx               70091.0    -0.627813  ...     0.299975     8.244699\n",
              "Wy               70091.0    -0.407068  ...     0.450077     7.733831\n",
              "max Wx           70091.0    -1.018681  ...     0.627619    11.913133\n",
              "max Wy           70091.0    -0.733589  ...     0.822895    14.302308\n",
              "Day y            70091.0    -0.000061  ...     0.707107     1.000000\n",
              "Day x            70091.0    -0.000143  ...     0.707107     1.000000\n",
              "Year y           70091.0     0.001614  ...     0.708301     1.000000\n",
              "Year x           70091.0    -0.000672  ...     0.707423     1.000000\n",
              "\n",
              "[19 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLSk6iTyR7-2"
      },
      "source": [
        "### Split and Normalize\n",
        "Let's split and normalize the data. \n",
        "\n",
        "When splitting time series data, we should maintain the order of the sequence. this is typically done by taking first n records as trianing, the next m records as validation and the last k records for testing. \n",
        "\n",
        "Splitting this way does not distort the sequential order in the samples and ensures that the validation/test results are more realistic, being evaluated on data collected afte[link text](https://)r the model was trained.\n",
        "\n",
        "let's ake the first 70% of data for training, the next 20% samples for validation, and the last 10% samples for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fpa0QuJUDkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7f085c-704a-48bb-a967-c857f7d6e86a"
      },
      "source": [
        "#get the number of samples\n",
        "n = len(df)\n",
        "\n",
        "# use,  the first 70% of samples for training\n",
        "train = df[0:int(n*0.7)]\n",
        "\n",
        "#use the next 20% of samples for testing\n",
        "val = df[int(n*0.7):int(n*0.9)]\n",
        "\n",
        "#use the last 10% of samples for testing\n",
        "test = df[int(n*0.9):]\n",
        "\n",
        "print(\"train shape\", train.shape)\n",
        "print(\"validation shape\", val.shape)\n",
        "print(\"test shape\", test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape (49063, 19)\n",
            "validation shape (14018, 19)\n",
            "test shape (7010, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLBnDUPUvjO"
      },
      "source": [
        "As always, we need to scale the features before training a neural network model. Note that the validation and test sets are normalized based on the statistics (mean and std) computed from the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z95DzdmiVN8w"
      },
      "source": [
        "train_mean = train.mean()\n",
        "train_std = train.std()\n",
        "\n",
        "train_df = (train - train_mean) / train_std\n",
        "val_df = (val - train_mean) / train_std\n",
        "test_df = (test - train_mean) / train_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmz0EX-Voxew"
      },
      "source": [
        "### Data Windowing: Preparing Data for Supervised Learning\n",
        "In a time series prediction data, we typically want to make a set of predictions based on a window of consecutive samples in the data.  The main feature of the window are:\n",
        "* The width (number of time steps) of the input and label windows\n",
        "* The time offset between them.\n",
        "* Which features are used as inputs, labels, or both.\n",
        "For example, to make a single prediction 24 time steps into the future, given 24 time steps of history we might define a window like this:\n",
        "\n",
        "\n",
        "![window](https://www.tensorflow.org/tutorials/structured_data/images/raw_window_24h.png)\n",
        "\n",
        "\n",
        "For the temprature prediction problem, we want to predict the temprature 24 hours in the future. Suppose that the sample at time step $t_0$  (the first row in the data) is $s_0$ and we have 49063 hourly samples in our training data. We can convert this problem to a supervised learning problem by creating windows/data samples of the following form:\n",
        "\n",
        "$input= (s_0 , s_1, ...,s_{24})$    $output=t_{48}$  where $t_{48}$ is the temprature at time step 48 (that is the temprature column in row 48 of the data)\n",
        "The next input/output pairs would be:\n",
        "$input=(s_1,s_2,...,s_{25})$, $output=t_{49}$\n",
        "\n",
        "$input=(s_2,s_3,...,s_{26})$, $output=t_{50}$\n",
        "\n",
        "and so on until\n",
        "\n",
        "$input=(s_{49015}, s_{49016},....,s_{49039}$, output=t_{49063}\n",
        "\n",
        "As you can see we slide the window to the right by one row to create the next input/output pairs.\n",
        "\n",
        "Let's  define a simple python generator, we call it <code>windowGenerator</code>, that takes the following parameters and creates batches of input/output pairs in the above form. We can then feed these batches as input to our neural network models.\n",
        "\n",
        "* <code> data</code>: An array of sequential data where rows are consecutive data points and columns are features.\n",
        "* <code>target</code> An array of consecutive target values corresponding to the data. It must have the same length as data.\n",
        "* <code>lookback</code>: the number of past observations that we want to use to predict the target, that is the width of the window\n",
        "*<code> offset </code>: the number of time steps in the future to predit the target.\n",
        "*<code>batch_size</code> the number of training examples we want to have in each mini-batch of SGD. Each training example is a window of the form explained above.\n",
        "\n",
        "The number of windows/samples generated is equal to <code>data.shape[0]-lookback-offset</code>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC3KfhWXJT5F"
      },
      "source": [
        "def windowGenerator(data, target, lookback, offset,batch_size=16):\n",
        "  min_index=0\n",
        "  max_index = data.shape[0]-offset\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if i + batch_size >= max_index:\n",
        "      i = min_index + lookback\n",
        "    rows = np.arange(i, min(i + batch_size, max_index))\n",
        "    i += len(rows)\n",
        "    samples = np.zeros((len(rows), lookback, data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j])\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = target[rows[j] + offset]\n",
        "\n",
        "    yield samples, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab1-isOKaR8y"
      },
      "source": [
        "I find codes for generating windows of this a bit confusing so if you have a hard time understanding the above code, you are not alone :) but to see how this windowGenerator function works, let's try it on a simple data consisting of two variables measured in 9 consecutive time steps.\n",
        "Suppose that the goal is to predict the second variable/column two time steps in the future (offset=2) by using the past three observations (lookback=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El3YsjNvaO2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c78e3090-c82c-47d6-ae0b-b1501c219cc0"
      },
      "source": [
        "example_df=np.array([[1,0.1],[2,0.2], [3,0.3], [4,0.4], [5,0.5], [6,0.6], [7,0.7],[8,0.8],[9,0.9],[10,0.7]])\n",
        "lookback=3\n",
        "BATCH_SIZE=2\n",
        "offset=2\n",
        "windows = windowGenerator(data=example_df, target=example_df[:,1], offset=offset, lookback=lookback,batch_size=BATCH_SIZE)\n",
        "\n",
        "#The number of total batches are equal to the number of (training examples - lookback-offset)/batch_size \n",
        "no_batches=int((example_df.shape[0]-lookback-offset)/BATCH_SIZE)\n",
        "\n",
        "#print the batches\n",
        "for i in range(no_batches):\n",
        "  #get the next batch from the windowGenerator\n",
        "  input,output=next(windows)\n",
        "  print(\"{}th batch: \\ninput is:\\n{}\\n and \\ntarget is:\\n{}\\n\".format(i+1, input, output))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1th batch: \n",
            "input is:\n",
            "[[[1.  0.1]\n",
            "  [2.  0.2]\n",
            "  [3.  0.3]]\n",
            "\n",
            " [[2.  0.2]\n",
            "  [3.  0.3]\n",
            "  [4.  0.4]]]\n",
            " and \n",
            "target is:\n",
            "[0.6 0.7]\n",
            "\n",
            "2th batch: \n",
            "input is:\n",
            "[[[3.  0.3]\n",
            "  [4.  0.4]\n",
            "  [5.  0.5]]\n",
            "\n",
            " [[4.  0.4]\n",
            "  [5.  0.5]\n",
            "  [6.  0.6]]]\n",
            " and \n",
            "target is:\n",
            "[0.8 0.9]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnDYDAksmSL9"
      },
      "source": [
        "As you can see each batch generated by windowGenerator function contains two  windows/samples where each sample is of dimension (lookback=3, number of features=2). \n",
        "\n",
        "Now that we know how the windowGenerator function works, let's apply it to the temprature data to create batches of input output pairs to predict the temprature 24 hours in the future (offset=24) based on the past 24 hour observations (lookbach=24)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zfrboqz4Wx7"
      },
      "source": [
        "BATCH_SIZE=32\n",
        "LOOKBACK=24\n",
        "OFFSET=24\n",
        "train_gen = windowGenerator(data=train_df.to_numpy(), target=train_df['T (degC)'].to_numpy(),lookback=LOOKBACK, offset=OFFSET,batch_size=BATCH_SIZE)\n",
        "val_gen = windowGenerator(data=val_df.to_numpy(),target=val_df['T (degC)'].to_numpy(),lookback=LOOKBACK, offset=OFFSET,batch_size=BATCH_SIZE)\n",
        "test_gen = windowGenerator(data=test_df.to_numpy(), target=test_df['T (degC)'].to_numpy(),lookback=LOOKBACK, offset=OFFSET,batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tg1VfB381io"
      },
      "source": [
        "## A common-sense, None-machine learning baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu0dnYh48xhF"
      },
      "source": [
        "Before you start using black-box deep-learning models to solve the temperatureprediction\n",
        "problem, let’s try a simple, common-sense approach. It will serve as a sanity\n",
        "check, and it will establish a baseline that you’ll have to beat in order to demonstrate\n",
        "the usefulness of more-advanced machine-learning models. Such common-sense baselines\n",
        "can be useful when you’re approaching a new problem for which there is no\n",
        "known solution (yet). A classic example is that of unbalanced classification tasks,\n",
        "where some classes are much more common than others. If your dataset contains 90%\n",
        "instances of class A and 10% instances of class B, then a common-sense approach to\n",
        "the classification task is to always predict “A” when presented with a new sample. Such\n",
        "a classifier is 90% accurate overall, and any learning-based approach should therefore\n",
        "beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary\n",
        "baselines can prove surprisingly hard to beat.\n",
        "In this case, the temperature timeseries can safely be assumed to be continuous\n",
        "(the temperatures tomorrow are likely to be close to the temperatures today) as well\n",
        "as periodical with a daily period. Thus a common-sense approach is to always predict\n",
        "that the temperature 24 hours from now will be equal to the temperature right now.\n",
        "Let’s evaluate this approach, using the mean absolute error (MAE) metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShZi2ZCv8v_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6da6f3-c364-42db-87d8-58ff93ff1368"
      },
      "source": [
        "def evaluate_naive_method():\n",
        "  #an array to store mean absolute value per batch\n",
        "  batch_maes = []\n",
        "  for step in range(int((val_df.shape[0]-LOOKBACK-OFFSET)/BATCH_SIZE)):\n",
        "    #take the next batch from validation generator\n",
        "    inputs, targets = next(val_gen)\n",
        "\n",
        "    #set the prediction to the last temprature in the input window. The second index (-1 )means the last time step in the window and the last index (1) means the second column/feature in the input (which is the temprature column)\n",
        "    preds = inputs[:, -1, 1]\n",
        "\n",
        "    #compute the mae for the current batch\n",
        "    mae = np.mean(np.abs(preds - targets))\n",
        "\n",
        "    #store mae in batch_maes\n",
        "    batch_maes.append(mae)\n",
        "  #print the mean of mae over batches\n",
        "  return np.mean(batch_maes)\n",
        "\n",
        "#call the method\n",
        "mae_baseline=evaluate_naive_method()\n",
        "print(mae_baseline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.31841049455348175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnfHDQ30w66c"
      },
      "source": [
        "This yields an MAE of 0.32. Because the temperature data has been normalized to be centered on 0 and have a standard deviation of 1, this number isn’t immediately interpretable. To get the MAE in the original temprature scale we need to multiply it by the std of temprature in the original validation data as follows\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaWWLLVdx5ym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb821dce-0dff-476e-cd7e-8cfc0632d4ed"
      },
      "source": [
        "val['T (degC)'].std(axis=0)*mae_baseline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.435033459602708"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O5CLFoj0Ikc"
      },
      "source": [
        "So the baseline model gives us an avereage absolute error of about $2.43^{\\circ}$ degree celcius.  That’s a fairly large average absolute error. Now the game is to use your knowledge of deep learning to do better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddbL8XC70mz1"
      },
      "source": [
        "## A Small Fully Connected Network\n",
        "In the same way that it’s useful to establish a common-sense baseline before trying machine-learning approaches, it’s useful to try simple, cheap machine-learning models (such as a small fully connected network) before looking into complicated and computationally expensive models such as RNNs. This is the best way to make sure any further complexity you throw at the problem is legitimate and delivers real benefits.\n",
        "\n",
        "The following code segment creates a  fully connected model that starts by flattening the input data and then runs it through two Dense layers. The reason we need to flatten the input is because the windowGenerator creates batches of 3D arrays with dimension **[number of examples in the batch, lookback, features]** whereas a dense layer expects batches of 2D arrays with dimension **[number of examples, features]**. In essence the dense layer is blind to the sequential structure in the data and hence we have to flatten the temporal dimension. \n",
        "\n",
        "In contrast, a recurrent layer ( such as LSTM, GRU, etc.) utilizes the temporal dimension in the data and is not blind to sequential patterns.\n",
        "\n",
        " Note the lack of activation function on the last Dense layer, which is typical for a regression problem. Let's use mean absolute error, 'mae' as the loss.\n",
        "Because you evaluate on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGk_rHv55P79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dd837d-995d-4284-812c-6b63710061ef"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-3, restore_best_weights=True)\n",
        "\n",
        "model=keras.Sequential(\n",
        "    [\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "model.compile(optimizer='Adam', loss='mae')\n",
        "\n",
        "train_steps= int((train_df.shape[0]-LOOKBACK-OFFSET)/BATCH_SIZE)\n",
        "val_steps= int((val_df.shape[0]-LOOKBACK-OFFSET)/BATCH_SIZE)\n",
        "\n",
        "history = model.fit(train_gen, steps_per_epoch=train_steps, epochs=50, validation_data=val_gen, validation_steps=val_steps, callbacks=[early_stopping])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.5343 - val_loss: 0.4190\n",
            "Epoch 2/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.3364 - val_loss: 0.3299\n",
            "Epoch 3/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.3087 - val_loss: 0.3245\n",
            "Epoch 4/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.3028 - val_loss: 0.3179\n",
            "Epoch 5/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.3021 - val_loss: 0.3159\n",
            "Epoch 6/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2999 - val_loss: 0.3086\n",
            "Epoch 7/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2964 - val_loss: 0.3036\n",
            "Epoch 8/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2959 - val_loss: 0.3032\n",
            "Epoch 9/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2939 - val_loss: 0.3063\n",
            "Epoch 10/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2941 - val_loss: 0.3027\n",
            "Epoch 11/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2929 - val_loss: 0.3020\n",
            "Epoch 12/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2927 - val_loss: 0.3034\n",
            "Epoch 13/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2912 - val_loss: 0.3110\n",
            "Epoch 14/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2900 - val_loss: 0.2991\n",
            "Epoch 15/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2920 - val_loss: 0.3004\n",
            "Epoch 16/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2907 - val_loss: 0.3077\n",
            "Epoch 17/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2901 - val_loss: 0.2963\n",
            "Epoch 18/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2901 - val_loss: 0.2925\n",
            "Epoch 19/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2894 - val_loss: 0.3015\n",
            "Epoch 20/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2885 - val_loss: 0.2983\n",
            "Epoch 21/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2884 - val_loss: 0.3005\n",
            "Epoch 22/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2883 - val_loss: 0.2888\n",
            "Epoch 23/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2861 - val_loss: 0.2890\n",
            "Epoch 24/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2871 - val_loss: 0.2967\n",
            "Epoch 25/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2880 - val_loss: 0.3058\n",
            "Epoch 26/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2861 - val_loss: 0.3007\n",
            "Epoch 27/50\n",
            "1531/1531 [==============================] - 5s 3ms/step - loss: 0.2857 - val_loss: 0.3029\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, None)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                14624     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 14,657\n",
            "Trainable params: 14,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ydja0sgKNzq"
      },
      "source": [
        "Let's draw the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMdi14I4LpvX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f8dcefb0-cb51-4911-f9a0-9d04f5319f98"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrFEI5xpay_I"
      },
      "source": [
        "The fully connected baseline performs better than the commonsense model. The best validation MAE is around 0.288 which translates to $2.20^{\\circ}$ degree celcius\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-6Vsy0DRDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae1cac6e-f303-402e-ba77-bd7ed3eee025"
      },
      "source": [
        "mae_fullyconnected=model.evaluate(val_gen, steps=val_steps)\n",
        "val['T (degC)'].std(axis=0)*mae_fullyconnected"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "436/436 [==============================] - 1s 2ms/step - loss: 0.2888\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.208720751702966"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz1NJ51-B_fF"
      },
      "source": [
        "## The first Recurrent Baseline\n",
        "We can increase model capacity by adding more layers until we overfit and then regularize but the performance gain for the fully connected network is likely going to be small. This is because, as explained in the lectures, fully connected networks don't have memory and are not optimized for sequence data. the fully connected layer first flattened the timeseries which removed the notion of time from the input data.\n",
        "\n",
        "Let’s instead look at the data as what it is: a sequence, where causality and order matter. You’ll try a recurrent-sequence processing model it should be the perfect fit for such sequence data, precisely because it exploits the temporal ordering of data points, unlike the first approach.\n",
        "\n",
        "In the following code segment, we replace the first Dense Layer with a GRU (Gated Recurrent Unit) layer. The flatten layer is no longer needed because GRU requires a batch of 3D arrays of dimenson **[number of examples, look back time steps,features]** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__4BUd_CCbTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e0aea5-ccef-4c30-ee90-959d5ccf69b4"
      },
      "source": [
        "\n",
        "model_recurrent=keras.Sequential(\n",
        "    [\n",
        "        layers.GRU(32),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "model_recurrent.compile(optimizer='Adam', loss='mae')\n",
        "\n",
        "history = model_recurrent.fit(train_gen, steps_per_epoch=train_steps, epochs=50, validation_data=val_gen, validation_steps=val_steps, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.3161 - val_loss: 0.3319\n",
            "Epoch 2/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2887 - val_loss: 0.2962\n",
            "Epoch 3/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2827 - val_loss: 0.2857\n",
            "Epoch 4/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2792 - val_loss: 0.2825\n",
            "Epoch 5/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2767 - val_loss: 0.2810\n",
            "Epoch 6/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2744 - val_loss: 0.2809\n",
            "Epoch 7/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2725 - val_loss: 0.2796\n",
            "Epoch 8/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2708 - val_loss: 0.2793\n",
            "Epoch 9/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2691 - val_loss: 0.2792\n",
            "Epoch 10/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2674 - val_loss: 0.2806\n",
            "Epoch 11/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2658 - val_loss: 0.2821\n",
            "Epoch 12/50\n",
            "1531/1531 [==============================] - 7s 5ms/step - loss: 0.2642 - val_loss: 0.2814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaeNZLlKJ5VT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "0445b5ea-cab2-4eb7-8d52-b6af8718b2cb"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss recurrent baseline')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5dn48e9NWCL7FlxACAoIKJBAWBRBlvEVRMG1QlHkdUGsVsW2imKVYulrlddqK9pSq/ZtsXH5VYpViguioFjZIgiECgiCggaQzbBz//54zoSTYZJMkklOZnJ/rmuuOec5230mk3vOeZ5zniOqijHGmORVI+gAjDHGVCxL9MYYk+Qs0RtjTJKzRG+MMUnOEr0xxiQ5S/TGGJPkLNHHkYjMEZHr4z1vkERko4iEKmC9KiLtvOHfi8jPY5m3DNsZLSJvlTXOYtY7QES2xHu9BkTkBRH5ZSVvc76I3OQNV8h3Jkg1gw4gaCKyzzdaFzgIHPXGb1HVmbGuS1WHVsS8yU5Vx8djPSKSDnwB1FLVI966ZwIx/w1NxRKRAcBfVbVV0LEUJRm/M9U+0atq/fCwiGwEblLVdyLnE5Ga4eRhTJAq67soIgKIqh6r7G2b+LKqmyKET81F5F4R2QY8LyJNROSfIpInIt95w618y/hP/8aKyEIRmebN+4WIDC3jvG1F5AMR2Ssi74jIdBH5axFxxxLjwyLyobe+t0SkuW/6dSKySUR2iMikYj6f3iKyTURSfGWXi8gKb7iXiCwSkV0islVEnhKR2kWsq9Cpuoj8zFvmaxG5IWLeYSKyXET2iMhmEZnsm/yB975LRPaJyLnhz9a3/HkislhEdnvv58X62RRHRDp5y+8SkVUiMtw37WIRWe2t8ysR+alX3tz7++wSkZ0iskBEov5PetVXt4nI58DnXtklIpLjLf+RiHT1zX+6iPzd+x7sEJGnvPLJ/u+OiKR7667p+wymisiHQD5wRhm2vVFEfioiK7zP+SURSRWResAc4DTv77NPRE4r4iNtLiJve5/Z+yLSxrf+J72//R4RWSoi/XzTeonIEm/aNyLyuG9aHy/WXSLyqbizi2ifdeR3RkVkvIh87i07XUTEN/0GEVkj7v9trj/WKkNV7eW9gI1AyBseABwBfg3UAU4CmgFX4qp4GgCvALN8y8/HnREAjAUOAzcDKcCtwNe4I6TSzrsImAbUBs4H9uBOf6PtQywxrgc6ePs0H3jEm9YZ2Af09/b5ce8zCBWxrfXAhb7xV4CJ3nAPoA/urDEdWAPc5ZtXgXbe8AvAL73hIcA3wDlAPeDFiHkHAF1wByldvXkv86ale/PW9G1nLLDQG24KfAdc58U1yhtvVtJnE2XfBwBbvOFawDrgfu9vNAjYC5zlTd8K9POGmwDdveH/AX7vLV8L6Bf+m0fZngJve/twEpAJfAv0xn1nrsd9f+t4458Cv/E+w1TgfG89k/F9dyI/M2+fvwTO9j6jWqXZtu//6BPgNG+ZNcD4yM+tmP/DF7zPL/w9fDL8N/SmX4v7ntcEfgJsA1J9/yvXecP1gT7ecEtgB3Ax7rtzoTeeVsT/48KIz/6fQGOgNZAHDPGmjfD+9p28eB4APgo6l53wmQYdQFV6cWKiPxT+AhUxfwbwnW888suyzjetrveFOaU083pfrCNAXd/0v1JEoo8xxgd84z8C/uUNPwhk+6bV8z6DohL9L4HnvOEGwPdAmyLmvQt4zTdeVKJ/Dl9yxSXdgnmjrPcJ4DfecDrFJ/rrgE8ill8EjC3ps4my3QEcT/T9cMmmhm/634DJ3vCXwC1Aw4h1TAH+UdS+RcyrwCDf+DPAwxHzrAUuAM7FJaOaUdYzmZIT/ZSybtv3f3Stb9qjwO8jP7di9vWFiO9hfVy72elFzP8d0M0b/gD4BdA8Yp57gb9ElM0Fri/i/zEy0Z/vG3+Z4wc0c4AbfdNq4M6Eov4fBPWyqpvi5anqgfCIiNQVkT+Iq9rYg/tSNRZf9UWEbeEBVc33BuuXct7TgJ2+MoDNRQUcY4zbfMP5vphO869bVb/HHfUU5UXgChGpA1wBLFPVTV4cHbxqiW1eHL8CYqkGKRQDsCli/3qLyHtelcRuYHyM6w2ve1NE2Sbc0V5YUZ9NiTGrry47Yr1X4o4kN3nVEOd65Y/hjgbfEpENIjKxhO34P5c2wE+8qoRdIrILON2L5XRgk5a9Lj3a9yvWbYeV5XOMuj1V3QfsDK/fqxZa41UL7QIacfw7cCPu4CBXXNXcJb6Yr46I+Xzg1BjjKWp/2gBP+ta5ExAKf6cCZ4m+eBox/hPgLKC3qjbEnVqC+8NWlK1AUxGp6ys7vZj5yxPjVv+6vW02K2pmVV2NS2hDgR/iEn/YM0Au0N6L4/6yxIA7o/F7EZiNO7prhKv6CK838u8V6WvcP6Zfa+CrGOIqab2nR9SvF6xXVRer6gigBTALd0SIqu5V1Z+o6hnAcOBuERlczHb8+7cZmKqqjX2vuqr6N29a63C9e4TvcWeMYaeUsJ3SbrskJf2Nwvzfw/q4KqCvvfr4e4AfAE1UtTGwG+87oKqfq+oo3Gf9a+BVr21gM+6I3h9zPVV9JMZ4irIZd3Wef70nqepH5VxvXFmiL50GwH5cY19T4KGK3qB3hLwEmCwitb2jwUsrKMZXgUtE5HxxDadTKPk78iJwJ+4H5ZWIOPYA+0SkI67dIRYvA2NFpLP3QxMZfwPcGc4BEemF+4EJywOOAWcUse43gQ4i8kMRqSki1+DaJf4ZY2xF+TfuKO8eEanlNfJdCmR7f7PRItJIVQ/jPpNjUNCg2c5r2NuNq544Fn0TJ/gjMN47wxERqSeuoboBrn58K/CIV54qIn295XKA/iLSWkQaAfeVYX+L23ZJvgGaedsuzsW+7+HDwMequhn39z+CVzUlIg8CDcMLici1IpLmnV3t8oqP4ao7LxWRi0QkxftMBojvQoUy+j1wn4ic7W2/kYhcXc51xp0l+tJ5AtcYtR34GPhXJW13NK7edQeuXvwl3PX+0ZQ5RlVdBdyGS95bcXWfJd0U9DdcvfA8Vd3uK/8pLgnvxSWGl2KMYY63D/Nw1RrzImb5ETBFRPbi2hRe9i2bD0wFPvROpftErHsHcAnurGcH7sjwkoi4S01VD+ES+1Dc5/40MEZVc71ZrgM2elVY43F/T4D2wDu4BvBFwNOq+l6M21yCa7x/Cvd3WoerW0ZVj3rxtMO1D2wBrvGmvY37W6wAllKGH7nith3Dsrm478wG729U1FU3L+J+5HfiGvav9crn4r7T/8GdTR6gcLXSEGCVuPtjngRGqup+70diBO7MMs9b5meUMweq6mu4M4ds7+/7Ge57UKWEr+owCUREXgJyVbXCzyiMMYnPjugTgIj0FJEzRaSGiAzBHZnMCjouY0xiqPZ3xiaIU4C/4xpGtwC3quryYEMyxiQKq7oxxpgkZ1U3xhiT5Kpc1U3z5s01PT096DCMMSahLF26dLuqpkWbVuUSfXp6OkuWLAk6DGOMSSgiEnnXdwGrujHGmCRnid4YY5KcJXpjjElyVa6O3hhT+Q4fPsyWLVs4cOBAyTObQKWmptKqVStq1aoV8zKW6I0xbNmyhQYNGpCeno5IRXbGaspDVdmxYwdbtmyhbdu2MS9nVTfGGA4cOECzZs0syVdxIkKzZs1KfeZlid4YA2BJPkGU5e+UNIl+5054+GFYbj3AGGNMIUmT6FNSYPJkeO21oCMxxpTWjh07yMjIICMjg1NOOYWWLVsWjB86dKjYZZcsWcIdd9xR4jbOO++8uMQ6f/58LrnkkpJnrEKSJtE3agS9esE77wQdiTHJb+ZMSE+HGjXc+8yZ5Vtfs2bNyMnJIScnh/HjxzNhwoSC8dq1a3PkSNGPv83KyuK3v/1tidv46KMq9XS/SpU0iR4gFIJPPoHdu4OOxJjkNXMmjBsHmzaBqnsfN678yT7S2LFjGT9+PL179+aee+7hk08+4dxzzyUzM5PzzjuPtWvXAoWPsCdPnswNN9zAgAEDOOOMMwr9ANSvX79g/gEDBnDVVVfRsWNHRo8eTbgX3zfffJOOHTvSo0cP7rjjjhKP3Hfu3Mlll11G165d6dOnDytWrADg/fffLzgjyczMZO/evWzdupX+/fuTkZHBOeecw4IFC+L7gRUjpkQvIkNEZK2IrIv2pHoRGS8iK0UkR0QWikhnr7yXV5YjIp+KyOXx3gG/UAiOHoX336/IrRhTvU2aBPn5hcvy8115vG3ZsoWPPvqIxx9/nI4dO7JgwQKWL1/OlClTuP/++6Muk5uby9y5c/nkk0/4xS9+weHDh0+YZ/ny5TzxxBOsXr2aDRs28OGHH3LgwAFuueUW5syZw9KlS8nLyysxvoceeojMzExWrFjBr371K8aMGQPAtGnTmD59Ojk5OSxYsICTTjqJF198kYsuuoicnBw+/fRTMjIyyvfhlEKJiV5EUoDpuOcgdgZGhRO5z4uq2kVVM4BHgce98s+ALK98CPCHIp5MHxd9+kDdulZ9Y0xF+vLL0pWXx9VXX01KSgoAu3fv5uqrr+acc85hwoQJrFq1Kuoyw4YNo06dOjRv3pwWLVrwzTffnDBPr169aNWqFTVq1CAjI4ONGzeSm5vLGWecUXB9+qhRo0qMb+HChVx33XUADBo0iB07drBnzx769u3L3XffzW9/+1t27dpFzZo16dmzJ88//zyTJ09m5cqVNGgQy7PU4yOWI/pewDpV3eA9BDkb9yi7Aqq6xzdaD1CvPF9Vw5VrqeHyilKnDvTvb4nemIrUunXpysujXr16BcM///nPGThwIJ999hmvv/56kdeS16lTp2A4JSUlav1+LPOUx8SJE3n22WfZv38/ffv2JTc3l/79+/PBBx/QsmVLxo4dy//93//FdZvFiSXRt6TwU9a3eGWFiMhtIrIed0R/h6+8t4isAlYC432J37/sOBFZIiJLYjldKk4oBGvWwFdflWs1xpgiTJ3qzpz96tZ15RVp9+7dtGzpUs8LL7wQ9/WfddZZbNiwgY0bNwLw0ksvlbhMv379mOk1TsyfP5/mzZvTsGFD1q9fT5cuXbj33nvp2bMnubm5bNq0iZNPPpmbb76Zm266iWXLlsV9H4oSt8ZYVZ2uqmcC9wIP+Mr/rapnAz2B+0QkNcqyM1Q1S1Wz0tKi9psfs1DIvb/7brlWY4wpwujRMGMGtGkDIu59xgxXXpHuuece7rvvPjIzM+N+BA5w0kkn8fTTTzNkyBB69OhBgwYNaNSoUbHLTJ48maVLl9K1a1cmTpzIn//8ZwCeeOIJzjnnHLp27UqtWrUYOnQo8+fPp1u3bmRmZvLSSy9x5513xn0filLiM2NF5Fxgsqpe5I3fB6Cq/1PE/DWA71T1hE9IROYB96hqkU8WycrK0vI8eOTYMTjlFBgyBCrxzMiYhLZmzRo6deoUdBiB27dvH/Xr10dVue2222jfvj0TJkwIOqwTRPt7ichSVc2KNn8sR/SLgfYi0lZEagMjgdkRG2jvGx0GfO6Vtw03vopIG6AjsDG2XSmbGjVg0CBXT2/PPTfGlMYf//hHMjIyOPvss9m9eze33HJL0CHFRYlXwKjqERG5HZgLpADPqeoqEZkCLFHV2cDtIhICDgPfAdd7i58PTBSRw8Ax4Eequr0idsQvFIKXXoLcXLCDFGNMrCZMmFAlj+DLK6ZLHVX1TeDNiLIHfcNRK5tU9S/AX8oTYFmE6+nfeccSvTHGJNWdsWHp6XDmmXaZpTHGQJImenBH9e+9BxXQOG+MMQklqRP93r2weHHQkRhjTLCSNtEPHOiu8bXqG2OqvoEDBzJ37txCZU888QS33nprkcsMGDCA8KXYF198Mbt27TphnsmTJzNt2rRitz1r1ixWr15dMP7ggw/yThwSR1XqzjhpE32zZtC9uyV6YxLBqFGjyM7OLlSWnZ0dU38z4HqdbNy4cZm2HZnop0yZQih8RUeSSNpED676ZtEi2Lcv6EiMMcW56qqreOONNwoeMrJx40a+/vpr+vXrx6233kpWVhZnn302Dz30UNTl09PT2b7dXbk9depUOnTowPnnn1/QlTG4a+R79uxJt27duPLKK8nPz+ejjz5i9uzZ/OxnPyMjI4P169czduxYXn31VQDeffddMjMz6dKlCzfccAMHDx4s2N5DDz1E9+7d6dKlC7m5ucXuX9DdGVdYT5JVQSgEv/41LFgAQ4cGHY0xieGuuyAnJ77rzMiAJ54oenrTpk3p1asXc+bMYcSIEWRnZ/ODH/wAEWHq1Kk0bdqUo0ePMnjwYFasWEHXrl2jrmfp0qVkZ2eTk5PDkSNH6N69Oz169ADgiiuu4OabbwbggQce4E9/+hM//vGPGT58OJdccglXXXVVoXUdOHCAsWPH8u6779KhQwfGjBnDM888w1133QVA8+bNWbZsGU8//TTTpk3j2WefLXL/wt0Zz5o1i3nz5jFmzBhycnIKujPu27cv+/btIzU1lRkzZnDRRRcxadIkjh49Sn5kn9BlkNRH9H37uh4trfrGmKrPX33jr7Z5+eWX6d69O5mZmaxatapQNUukBQsWcPnll1O3bl0aNmzI8OHDC6Z99tln9OvXjy5dujBz5swiuzkOW7t2LW3btqVDhw4AXH/99XzwwQcF06+44goAevToUdARWlGC7s44qY/oTzoJzj/fEr0xpVHckXdFGjFiBBMmTGDZsmXk5+fTo0cPvvjiC6ZNm8bixYtp0qQJY8eOLbJ74pKMHTuWWbNm0a1bN1544QXmz59frnjDXR2Xp5vjiRMnMmzYMN5880369u3L3LlzC7ozfuONNxg7dix33313wQNNyiqpj+jBVd+sWAFRnj1gjKlC6tevz8CBA7nhhhsKjub37NlDvXr1aNSoEd988w1z5swpdh39+/dn1qxZ7N+/n7179/L6668XTNu7dy+nnnoqhw8fLuhaGKBBgwbs3bv3hHWdddZZbNy4kXXr1gHwl7/8hQsuuKBM+xZ0d8bVItEDzJsXbBzGmJKNGjWKTz/9tCDRh7v17dixIz/84Q/p27dvsct3796da665hm7dujF06FB69uxZMO3hhx+md+/e9O3bl44dOxaUjxw5kscee4zMzEzWr19fUJ6amsrzzz/P1VdfTZcuXahRowbjx48v034F3Z1xid0UV7bydlMc6ehRSEuDyy+HP/0pbqs1JqlYN8WJpSK6KU5oKSmu2+K337Zui40x1VPSJ3qAwYNh82bwqtqMMaZaqRaJ3h4vaEzJqlo1romuLH+napHo27VzT6i3yyyNiS41NZUdO3ZYsq/iVJUdO3aQmnrCo7eLldTX0YeJuKP6115zjbMpKUFHZEzV0qpVK7Zs2UJeXl7QoZgSpKam0qpVq1ItUy0SPbhE/9xzsHw5ZEVtlzam+qpVqxZt27YNOgxTQapF1Q24K2/Aqm+MMdVPtUn0J58MXbtaojfGVD8xJXoRGSIia0VknYhMjDJ9vIisFJEcEVkoIp298gtFZKk3bamIDIr3DpRGKAQLF8L+/UFGYYwxlavERC8iKcB0YCjQGRgVTuQ+L6pqF1XNAB4FHvfKtwOXqmoX4HrgL3GLvAxCITh4ED78MMgojDGmcsVyRN8LWKeqG1T1EJANjPDPoKp7fKP1APXKl6vq1175KuAkEalT/rDLpl8/qFXLqm+MMdVLLFfdtAQ2+8a3AL0jZxKR24C7gdpAtCqaK4FlqnowyrLjgHEArVu3jiGksqlfH8491xK9MaZ6iVtjrKpOV9UzgXuBB/zTRORs4NfALUUsO0NVs1Q1Ky0tLV4hRRUKwbJlsGNHhW7GGGOqjFgS/VfA6b7xVl5ZUbKBy8IjItIKeA0Yo6rri1yqkoRCrnOz994LOhJjjKkcsST6xUB7EWkrIrWBkcBs/wwi0t43Ogz43CtvDLwBTFTVKtEE2rMnNGhg1TfGmOqjxESvqkeA24G5wBrgZVVdJSJTRCT8QMbbRWSViOTg6umvD5cD7YAHvUsvc0SkRfx3I3Y1a8LAgZbojTHVR9I/eCSa3/0O7rgDvvgC0tMrdFPGGFMpqvWDR6IZPNi9W7fFxpjqoFom+k6d4NRTrfrGGFM9JE2inznTVcPUqOHefQ95P0G42+J334VjxyorQmOMCUZSJPqZM2HcONi0yV06uWmTGy8u2YdCkJcHK1dWXpzGGBOEpEj0kyZBfn7hsvx8V16UcD29Vd8YY5JdUiT6L78sXTlAy5aurt4SvTEm2SVFoi+qe5ySus0JheCDD1yPlsYYk6ySItFPnQp16xYuq1vXlRcnFHJVPB9/XHGxGWNM0JIi0Y8eDTNmQJs27oqaNm3c+OjRxS93wQXuQeFWfWOMSWbV8s5Yv/POc1fqLFpUaZs0xpi4sztjixEKwSefwO7dQUdijDEVwxJ9yN00NX9+0JEYY0zFqPaJvk8f13Br9fTGmGRV7RN97dquUdYSvTEmWVX7RA+u+iY3F7ZsCToSY4yJP0v0WLfFxpjkZoke6NIF0tIs0RtjkpMlelzXxoMHu3r6KnZbgTHGlJslek8oBFu3wpo1QUdijDHxZYneEwq5d7v6xhiTbGJK9CIyRETWisg6EZkYZfp4EVkpIjkislBEOnvlzUTkPRHZJyJPxTv4eGrTBtq1s0RvjEk+JSZ6EUkBpgNDgc7AqHAi93lRVbuoagbwKPC4V34A+Dnw0/iFXHFCIXeH7OHDQUdijDHxE8sRfS9gnapuUNVDQDYwwj+Dqu7xjdYD1Cv/XlUX4hJ+lRcKwd69sHhx0JEYY0z8xJLoWwKbfeNbvLJCROQ2EVmPO6K/ozRBiMg4EVkiIkvy8vJKs2hcDRzoujm26htjTDKJW2Osqk5X1TOBe4EHSrnsDFXNUtWstLS0eIVUak2bQo8eluiNMckllkT/FXC6b7yVV1aUbOCy8gQVpFDI9U2/b1/QkRhjTHzEkugXA+1FpK2I1AZGArP9M4hIe9/oMODz+IVYuUIhOHLEPUvWGGOSQc2SZlDVIyJyOzAXSAGeU9VVIjIFWKKqs4HbRSQEHAa+A64PLy8iG4GGQG0RuQz4L1VdHf9diY++fSE11VXfXHxx0NEYY0z5VftHCUZz4YXwzTewYkWgYRhjTMzsUYKlFArBypWwbVvQkRhjTPlZoo8i3B3CvHnBxmGMMfFgiT6KjAxo0sS6LTbGJAdL9FGkpMCgQfD229ZtsTEm8VmiL0IoBJs3w7p1QUdijDHlY4m+CNZtsTEmWViiL8KZZ7quiy3RG2MSnSX6Ioi4o/p58+Do0aCjMcaYsrNEX4xQCHbtgmXLgo7EGGPKzhJ9MQYNcu9WfWOMSWSW6IvRogV062aJ3hiT2CzRlyAUgoULIT8/6EiMMaZsLNGXIBSCQ4fgww+DjsQYY8rGEn0J+vWDWrWs+sYYk7gs0ZegXj047zxL9MaYxGWJPgahECxfDtu3Bx2JMcaUniX6GIRCrnOz994LOhJjjCk9S/QxyMqChg2t+sYYk5gs0cegZk0YMMD6pzfGJCZL9DEKhWD9evjii6AjMcaY0okp0YvIEBFZKyLrRGRilOnjRWSliOSIyEIR6eybdp+33FoRuSiewVemcLfFdlRvjEk0JSZ6EUkBpgNDgc7AKH8i97yoql1UNQN4FHjcW7YzMBI4GxgCPO2tL+F07AinnWb19MaYxBPLEX0vYJ2qblDVQ0A2MMI/g6ru8Y3WA8IP4BsBZKvqQVX9AljnrS/hhLstfvddOHYs6GiMMSZ2sST6lsBm3/gWr6wQEblNRNbjjujvKOWy40RkiYgsycvLizX2ShcKuWvpV6wIOhJjjIld3BpjVXW6qp4J3As8UMplZ6hqlqpmpaWlxSukuBs82L1b9Y0xJpHEkui/Ak73jbfyyoqSDVxWxmWrtNNOg86dLdEbYxJLLIl+MdBeRNqKSG1c4+ps/wwi0t43Ogz43BueDYwUkToi0hZoD3xS/rCDEwrBBx/AwYNBR2KMMbEpMdGr6hHgdmAusAZ4WVVXicgUERnuzXa7iKwSkRzgbuB6b9lVwMvAauBfwG2qmtBPYA2FYP9+WLQo6EiMMSY2oqolz1WJsrKydMmSJUGHUaQ9e6BpU5g4EX75y6CjMcYYR0SWqmpWtGl2Z2wpNWwIvXtbPb0xJnFYoi+DUAgWL4Zdu4KOxBhjSmaJvgxCIXfT1Pz5QUdijDEls0RfBr17uydPWfWNMSYRWKIvg9q1oX9/6+DMGJMYLNGX0UUXQW4uZGcHHYkxxhTPEn0Z3XIL9OsHY8ZYFY4xpmqzRF9Gqakwe7brvvjyy2HZsqAjMsaY6CzRl0PjxvCvf7kbqIYOdU+gMsaYqsYSfTmddhrMnQtHj8J//Rd8803QERljTGGW6OOgY0d44w3Yts0d2e/ZU/IyxhhTWSzRx0nv3vDqq+6hJFdcYb1bGmOqDkv0cTR0KDz3nLu+fswYe+SgMaZqqBl0AMlmzBhXT3/PPXDyyfDkk+55s8YYExRL9BXgpz+FrVvhN7+BU0+F++4LOiJjTHVmib4CiMC0ae7I/v774ZRT4L//O+iojDHVlSX6ClKjBjz/PGzfDjffDGlpcMklQUdljKmOrDG2AtWu7a7EycyEH/wAPvoo6IiMMdWRJfoK1qCBu8a+ZUt3RL96ddARGWOqG0v0laBFC3jrLahTx/V6uXlz0BEZY6qTmBK9iAwRkbUisk5EJkaZfreIrBaRFSLyroi08U37tYh85r2uiWfwiaRtW5gzB3bvhiFDYOfOoCMyxlQXJSZ6EUkBpgNDgc7AKBHpHDHbciBLVbsCrwKPessOA7oDGUBv4Kci0jB+4SeWjAz4xz9g3Tq49FLIzw86ImNMdRDLEX0vYJ2qblDVQ0A2MMI/g6q+p6rhtPUx0Mob7gx8oKpHVPV7YAUwJD6hJ6aBA2HmTFi0CEaOhCNHgo7IGJPsYkn0LQF/rfIWr6woNwJzvOFPgSEiUldEmgMDgdPLEmgyueoqeOopeP119wAT1aAjMsYks7g2xorItUAW8BiAqr4FvAl8BPwNWAQcjbLcOBFZIiJL8vLy4hlShZg5E9LT3bXy6eluvLR+9CN44L0omksAAA94SURBVAHXN87Pfx7vCI0x5rhYEv1XFD4Kb+WVFSIiIWASMFxVC/puVNWpqpqhqhcCAvwncllVnaGqWaqalZaWVtp9qFQzZ8K4cbBpkzsS37TJjZcl2U+ZAjfdBFOnwu9+F/9YjTEGYkv0i4H2ItJWRGoDI4HZ/hlEJBP4Ay7Jf+srTxGRZt5wV6Ar8Fa8gg/CpEknNqLm57vy0hKBZ56B4cPhzjvh5ZfjE6MxxviV2AWCqh4RkduBuUAK8JyqrhKRKcASVZ2Nq6qpD7wirqvGL1V1OFALWOCV7QGuVdWEbn788svSlZekZk3IznZPp7ruOmjeHAYNKnt8xhgTSbSKtQRmZWXpkiVLgg6jSOnprromUps2sHFj2df73XfQr5/7wXj/fddtgjHGxEpElqpqVrRpdmdsKU2dCnXrFi6rW9eVl0eTJu5B440buweYbNhQvvUZY0yYJfpSGj0aZsxwR/Ai7n3GDFdeXq1auQeNHz5sDxo3xsSPVd1UQYsWweDB0KkTzJ/vOkYzxpjiWNVNgjn3XHjlFfj0U/eg8UOHgo7IGJPILNFXUcOGwbPPwjvvwNix9qBxY0zZ2ROmqrCxY2HbNvfM2RYt3DNo7UHjxpjSskRfxd17r0v2Tz7pqnB+9CM455ygozLGJBKruqniRODxx103C3/4A3Tp4hL9L38Jn38edHTGmERgiT4B1KjhkvzXX7teL5s0cR2hdegAPXrAY49Fv4nLGGPAEn1COflkuO02WLDA3UH7v/8LKSlwzz3ujt2+fV3naNu2BR2pMaYqsUSfoE4/He6+Gz75xD2xaupU2LsX7rgDTjvN9ZczYwZs3x50pMaYoFmiTwJnngn33w8rVsCqVa5a56uv3ENNTj0VLr4Y/vxn97xaY0z1Y4k+yXTuDL/4BeTmwrJl8JOfwOrV7lLNFi3g8svhpZfg+++DjtQYU1ks0SeAsjzRSsT1gPnII/DFF65bhVtvhX//2z2rtkUL9z5rFhw4UNF7YIwJkvV1U8WFn2jlf9hJ3bpl70jt6FFYuND1gf/KK7BjBzRs6I70R450fezUqhW/+I0xlaO4vm4s0VdxFdX/PbheMufNc0n/tddcHX6zZnDllXDNNdCnz4ldMhtjqiZL9AmsRg33bNpIIvHt/+bgQddFcnY2/OMfx88g2rSBjh3dq1On48MtWlh3DCa57dnjLmpo3NgdANWuHXRExSsu0VsXCFVc69bRj+hbt47vdurUcc+uHT7cNdS+/TasXOkadXNz3bX7/uqjxo2PJ33/64wzrOrHJJa8PFizxl204H//6qvC8zVs6B71mZbm3kt6NWni7nOpCuyIvoqLdx19WR075r74ubnunyD8A5CbC1u3Hp+vZk1o1y76j0CjRpUXrzF+qu7O8shkvnp14XtN6tVzZ66dO7v31q1dleb27UW//P+bfjVqQNOmsf0ohH9AGjQo+5myVd0kuJkzYdIkdzds69bu5qjKTPIl2b0b1q4tnPxzc11fPEd8j4I/9dToPwCtWrl/CmPK69gx13YV7Qh9z57j8zVpcjyZd+58fLgs38X8/OJ/CKK9Dh+Ovq6ePd1NkGVhid4E4vBhd2ln5A/AmjWwa9fx+erWdY3OtWu7U92aNd17aV6lXaZpU3fm0a6da4eo6vWvprDDh2H9+sLJfPVqd8Cxf//x+U45pXBCD78H2cak6n50ov0ANG4MN95YtvWWO9GLyBDgSSAFeFZVH4mYfjdwE3AEyANuUNVN3rRHgWG4a/bfBu7UYjZqiT75qbp6UX/y37jR/fMePVq615EjpV8mUo0aLtm3a+fuMg7/ALRr59ocTjqp0j8i49m1yyVv/2vNGne26D8qbtPmxGTeqZM7cq8uytUYKyIpwHTgQmALsFhEZqvqat9sy4EsVc0XkVuBR4FrROQ8oC/Q1ZtvIXABML+sO2MSn4g7omrRAvr3r9xtq7rT++3bXR9B69e79/Dr5Zdh587Cy7RsGf1H4MwzXQOdKZ/Dh2HDhsLJ/D//ce/ffnt8vpo13Q9vp04wYsTxhH7WWVC/fnDxJ4JYrrrpBaxT1Q0AIpINjAAKEr2qvueb/2Pg2vAkIBWoDQhQC/im/GGbilTV2wTKQ8RV3Zx8snv17XviPDt3uh8A/4/A+vXw5psn9gyallY48fuHmzWzS1DDwmdxkUfna9e6JO9vy0lLc8n70kvde/hlV3SVXSyJviWw2Te+BehdzPw3AnMAVHWRiLwHbMUl+qdUdU3kAiIyDhgH0Dre1w2aUom8ymfTJjcOyZPsS9K0qXv17HnitH37ov8IvP8+/PWvhe95aNToeNJv3BhSU+PzqlOn6jZeHzjgPpNoCd3fLlOnDrRv7x6kc9VVLpF36ODeq1N1S2WJ63X0InItkIWrnkFE2gGdgFbeLG+LSD9VXeBfTlVnADPA1dHHMyZTOpMmnXi5WH6+K68uib449etDt27uFenAAdfW4P8BWLcOcnJcF9IHDriXv7GwrGrXLvqHINyonZLifhD879HKyjtt+/bjyXzTpsI/di1buuQ9alTho/PWravONebVQSyJ/ivgdN94K6+sEBEJAZOAC1T1oFd8OfCxqu7z5pkDnAssiFzeVA1fflm6cnNcaurxS0aLo+rqpcOJv6jX/v0lzxPtdfCga3Q+dMi9Hzt2vCE6PBz5XpZp4YRer547Gu/TB66//ngy79DB6s6rilgS/WKgvYi0xSX4kcAP/TOISCbwB2CIqvqaT/gSuFlE/gdXdXMB8EQ8AjcVo7LuxK3ORNxRd+3aid2YG27YrlHD2iKquhJr+lT1CHA7MBdYA7ysqqtEZIqIDPdmewyoD7wiIjkiMtsrfxVYD6wEPgU+VdXX470TJn6mTj2xI7O6dV25MX7hhm1L8lWf3TBlTpDMV90Yk6ysUzNTKqNHW2I3JplU0Yu0THVRlqdnGWNKx47oTWDsmn1jKocd0ZvAFHfNvjEmfizRm8DYNfvGVA5L9CYwRV2bb9fsGxNfluhNYOyafWMqhyV6E5jRo90jEdu0cTfdtGlT8Y9ItKt8THVkV92YQFXmNft2lY+pruyI3lQbdpWPqa4s0Ztqw67yMdWVJXpTbdhVPqa6skRvqg27ysdUV5boTbVhV/mY6squujHVil3lY6ojO6I3poLYVT6mqrBEb0wFsat8TFVhid6YCmJX+ZiqwhK9MRWksq/ysYZfUxRL9MZUkMq8yifc8LtpE6geb/i1ZG8gxoeDi8gQ4EkgBXhWVR+JmH43cBNwBMgDblDVTSIyEPiNb9aOwEhVnVXUtuzh4MaUXnq6S+6R2rSBjRsrOxoThOIeDl7iEb2IpADTgaFAZ2CUiHSOmG05kKWqXYFXgUcBVPU9Vc1Q1QxgEJAPvFXmPTHGRGUNv6Y4sVTd9ALWqeoGVT0EZAMj/DN4CT18IdnHQKso67kKmOObzxgTJ9bwa4oTS6JvCWz2jW/xyopyIzAnSvlI4G/RFhCRcSKyRESW5OXlxRCSMcYviO4drPE3ccS1MVZErgWygMciyk8FugBzoy2nqjNUNUtVs9LS0uIZkjHVQmV372CNv4kllkT/FXC6b7yVV1aIiISAScBwVT0YMfkHwGuqerisgRpjijd6tGt4PXbMvVdkNwt2129iiSXRLwbai0hbEamNq4KZ7Z9BRDKBP+CS/LdR1jGKIqptjDGJxxp/E0uJiV5VjwC346pd1gAvq+oqEZkiIsO92R4D6gOviEiOiBT8EIhIOu6M4P04x26MCYg1/iaWmOroVfVNVe2gqmeq6lSv7EFVne0Nh1T15PCllKo63LfsRlVtqarHKmYXjDGVzRp/E4vdGWuMKTVr/E0sMd0ZW5nszlhjTCS787dk5boz1hhjgmaNv+Vjid4YU+VVduNvsrUHWKI3xlR5ldn4m4ztAZbojTFVXmU2/ibjzWDWGGuMMT41argj+Ugi7q7jqsoaY40xJkbJeDOYJXpjjPFJxpvBLNEbY4xPMt4MZnX0xhgToHjdDGZ19MYYU0VVxs1gluiNMSZAldH4a4neGGMCVBmNv5bojTEmQJXR+FszfqsyxhhTFqNHV+yjH+2I3hhjkpwlemOMSXKW6I0xJslZojfGmCRnid4YY5JclesCQUTygCg3BFdJzYHtQQdRgZJ5/2zfElcy71959q2NqqZFm1DlEn0iEZElRfUtkQySef9s3xJXMu9fRe2bVd0YY0ySs0RvjDFJzhJ9+cwIOoAKlsz7Z/uWuJJ5/ypk36yO3hhjkpwd0RtjTJKzRG+MMUnOEn0ZiMjpIvKeiKwWkVUicmfQMcWbiKSIyHIR+WfQscSbiDQWkVdFJFdE1ojIuUHHFC8iMsH7Tn4mIn8TkdSgYyoPEXlORL4Vkc98ZU1F5G0R+dx7bxJkjGVVxL495n0vV4jIayLSOB7bskRfNkeAn6hqZ6APcJuIdA44pni7E1gTdBAV5EngX6raEehGkuyniLQE7gCyVPUcIAUYGWxU5fYCMCSibCLwrqq2B971xhPRC5y4b28D56hqV+A/wH3x2JAl+jJQ1a2quswb3otLFC2DjSp+RKQVMAx4NuhY4k1EGgH9gT8BqOohVd0VbFRxVRM4SURqAnWBrwOOp1xU9QNgZ0TxCODP3vCfgcsqNag4ibZvqvqWqh7xRj8GWsVjW5boy0lE0oFM4N/BRhJXTwD3AMeCDqQCtAXygOe9qqlnRaRe0EHFg6p+BUwDvgS2ArtV9a1go6oQJ6vqVm94G3BykMFUoBuAOfFYkSX6chCR+sD/A+5S1T1BxxMPInIJ8K2qLg06lgpSE+gOPKOqmcD3JO6pfyFeXfUI3I/ZaUA9Ebk22Kgqlrrrw5PuGnERmYSrIp4Zj/VZoi8jEamFS/IzVfXvQccTR32B4SKyEcgGBonIX4MNKa62AFtUNXwG9iou8SeDEPCFquap6mHg78B5AcdUEb4RkVMBvPdvA44nrkRkLHAJMFrjdKOTJfoyEBHB1fGuUdXHg44nnlT1PlVtparpuIa8eaqaNEeFqroN2CwiZ3lFg4HVAYYUT18CfUSkrvcdHUySNDRHmA1c7w1fD/wjwFjiSkSG4KpNh6tqfrzWa4m+bPoC1+GOdnO818VBB2Vi9mNgpoisADKAXwUcT1x4ZymvAsuAlbj/74TuLkBE/gYsAs4SkS0iciPwCHChiHyOO4t5JMgYy6qIfXsKaAC87eWV38dlW9YFgjHGJDc7ojfGmCRnid4YY5KcJXpjjElyluiNMSbJWaI3xpgkZ4neGGOSnCV6Y4xJcv8f14JcFjX/4EYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reLKr4w0Yu1A"
      },
      "source": [
        "The result is better than the fully connected network. The best validation MAE is 0.2796 which translates to about $2.13^{\\circ}$ .However, the model starts overfitting after the third epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49OWEHqcZsVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29bb504a-3b09-408a-e865-3411123c5248"
      },
      "source": [
        "mae_recurreent_baseline=model_recurrent.evaluate(val_gen, steps=val_steps)\n",
        "val['T (degC)'].std(axis=0)*mae_recurreent_baseline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "436/436 [==============================] - 1s 3ms/step - loss: 0.2796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1383074703337903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJNmzvPnfvaE"
      },
      "source": [
        "## Battling Overfitting\n",
        "We can add drop out and weight decay to help reduce overfitting. in pratice, it is best if you try one technique at a time to see which one causes your model to improve but for the sake of brevity I will try both in the following model. \n",
        "\n",
        "You are already familiar with dropout as a classic technique for fighting overfitting in dense layers. The standard dropout randomly drops some neurons in each layer by setting their outputs to zero. In a recurrent layer, the drop out can also be applied in the temporal direcion between time steps, that is, we can randomly drop the recurrent connections by setting the hidden states at some time steps to zero. This is known as *recurrent dropout*. \n",
        "\n",
        "It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with regularization. In 2015, [Gal et. al. ](https://arxiv.org/abs/1512.05287) showed that the proper way to use dropout with a recurrent layer is to apply the same dropout mask ( that is the same patten of dropping neurons) in each time step. You can find more details about recurrent drop out in Gal's paper.\n",
        "\n",
        "Every recurrent layer in Keras has two dropout-related\n",
        "arguments: <code>dropout</code>, a float specifying the dropout rate for input units of the layer,and <code>recurrent_dropout</code>, specifying the dropout rate along the temporal dimension. Let’s add dropout and recurrent dropout to the GRU layer and see how doing so impacts overfitting.\n",
        "\n",
        "Recall from our regularization lectures that another technique we have in our toolbox for fighting against overfitting is weight decay. We apply the weight decay penalty to the input weights  via <code>kernel_regularizer</code> or we can apply it to the recurrent weights via <code>recurrent_kernel_regularizer</code>\n",
        "s.\n",
        "\n",
        "This model runs much slower compared to the previous model. This is because after adding the recurrent_dropout, the model no longer meets the criteria to run on CUDNN kernel which is GPU-accelerated library of primitives for deep neural networks (see [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) for details)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iyBOip_mBHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e87e1e5-1c31-48d2-b81c-650b5f7927c5"
      },
      "source": [
        "\n",
        "model_regularized=keras.Sequential(\n",
        "    [\n",
        "        layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(0.001), recurrent_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "model_regularized.compile(optimizer='Adam', loss='mae')\n",
        "\n",
        "\n",
        "history = model_regularized.fit(train_gen, steps_per_epoch=train_steps, epochs=20, validation_data=val_gen, validation_steps=val_steps, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/20\n",
            "1531/1531 [==============================] - 125s 82ms/step - loss: 0.3660 - val_loss: 0.3404\n",
            "Epoch 2/20\n",
            "1531/1531 [==============================] - 123s 81ms/step - loss: 0.3244 - val_loss: 0.3166\n",
            "Epoch 3/20\n",
            "1531/1531 [==============================] - 123s 81ms/step - loss: 0.3141 - val_loss: 0.3084\n",
            "Epoch 4/20\n",
            "1531/1531 [==============================] - 124s 81ms/step - loss: 0.3091 - val_loss: 0.3081\n",
            "Epoch 5/20\n",
            "1531/1531 [==============================] - 124s 81ms/step - loss: 0.3066 - val_loss: 0.3055\n",
            "Epoch 6/20\n",
            "1531/1531 [==============================] - 124s 81ms/step - loss: 0.3045 - val_loss: 0.3084\n",
            "Epoch 7/20\n",
            "1531/1531 [==============================] - 121s 79ms/step - loss: 0.3025 - val_loss: 0.3059\n",
            "Epoch 8/20\n",
            "1531/1531 [==============================] - 118s 77ms/step - loss: 0.3014 - val_loss: 0.3028\n",
            "Epoch 9/20\n",
            "1531/1531 [==============================] - 118s 77ms/step - loss: 0.3007 - val_loss: 0.3013\n",
            "Epoch 10/20\n",
            "1531/1531 [==============================] - 118s 77ms/step - loss: 0.3001 - val_loss: 0.3021\n",
            "Epoch 11/20\n",
            "1531/1531 [==============================] - 117s 77ms/step - loss: 0.2989 - val_loss: 0.3014\n",
            "Epoch 12/20\n",
            "1531/1531 [==============================] - 117s 76ms/step - loss: 0.2985 - val_loss: 0.2980\n",
            "Epoch 13/20\n",
            "1531/1531 [==============================] - 117s 77ms/step - loss: 0.2980 - val_loss: 0.2978\n",
            "Epoch 14/20\n",
            "1531/1531 [==============================] - 117s 77ms/step - loss: 0.2975 - val_loss: 0.2952\n",
            "Epoch 15/20\n",
            "1531/1531 [==============================] - 118s 77ms/step - loss: 0.2973 - val_loss: 0.2995\n",
            "Epoch 16/20\n",
            "1531/1531 [==============================] - 127s 83ms/step - loss: 0.2971 - val_loss: 0.2995\n",
            "Epoch 17/20\n",
            "1531/1531 [==============================] - 127s 83ms/step - loss: 0.2966 - val_loss: 0.2949\n",
            "Epoch 18/20\n",
            "1531/1531 [==============================] - 126s 82ms/step - loss: 0.2961 - val_loss: 0.2951\n",
            "Epoch 19/20\n",
            "1531/1531 [==============================] - 126s 82ms/step - loss: 0.2960 - val_loss: 0.2963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVBNBJHXyyt3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "760b06c7-900b-4d3a-ba40-bec5de64528b"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation losses for the regularized model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuWyAjDMxous"
      },
      "source": [
        "The learning curves show that the model no longer overfits. However, its validation performance is worse than the unregularized model. \n",
        "\n",
        "models that use dropout regularization typically take longer to coverge so Let's resume the training for a few more epochs. \n",
        "\n",
        "It is a common practice to stop the training once the learning slows down and decrease the learning rate by an order of magnitude before resuming the training again.\n",
        "\n",
        "To do this, you can recompile the model with the reduced learning rate and call mode.fit to resume the training. The parameter <code>initial_epoch=20</code> means to resume the training after the 20th epoch (the last epoch after which the training has been stopped)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv0D1chv_OVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a995db-8f5f-42ee-ac6f-b136f95b6f71"
      },
      "source": [
        "model_regularized.compile(optimizer=Adam(learning_rate=1e-4), loss='mae')\n",
        "\n",
        "history = model_regularized.fit(train_gen, steps_per_epoch=train_steps, epochs=40, initial_epoch=20,\n",
        "                                validation_data=val_gen, validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 21/40\n",
            "1531/1531 [==============================] - 123s 80ms/step - loss: 0.2882 - val_loss: 0.2869\n",
            "Epoch 22/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2877 - val_loss: 0.2863\n",
            "Epoch 23/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2867 - val_loss: 0.2865\n",
            "Epoch 24/40\n",
            "1531/1531 [==============================] - 124s 81ms/step - loss: 0.2870 - val_loss: 0.2861\n",
            "Epoch 25/40\n",
            "1531/1531 [==============================] - 122s 79ms/step - loss: 0.2866 - val_loss: 0.2860\n",
            "Epoch 26/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2870 - val_loss: 0.2853\n",
            "Epoch 27/40\n",
            "1531/1531 [==============================] - 121s 79ms/step - loss: 0.2862 - val_loss: 0.2859\n",
            "Epoch 28/40\n",
            "1531/1531 [==============================] - 121s 79ms/step - loss: 0.2864 - val_loss: 0.2856\n",
            "Epoch 29/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2858 - val_loss: 0.2857\n",
            "Epoch 30/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2856 - val_loss: 0.2853\n",
            "Epoch 31/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2852 - val_loss: 0.2846\n",
            "Epoch 32/40\n",
            "1531/1531 [==============================] - 123s 80ms/step - loss: 0.2853 - val_loss: 0.2848\n",
            "Epoch 33/40\n",
            "1531/1531 [==============================] - 122s 79ms/step - loss: 0.2850 - val_loss: 0.2854\n",
            "Epoch 34/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2853 - val_loss: 0.2844\n",
            "Epoch 35/40\n",
            "1531/1531 [==============================] - 122s 79ms/step - loss: 0.2846 - val_loss: 0.2838\n",
            "Epoch 36/40\n",
            "1531/1531 [==============================] - 122s 80ms/step - loss: 0.2847 - val_loss: 0.2843\n",
            "Epoch 37/40\n",
            "1531/1531 [==============================] - 118s 77ms/step - loss: 0.2841 - val_loss: 0.2846\n",
            "Epoch 38/40\n",
            "1531/1531 [==============================] - 117s 76ms/step - loss: 0.2848 - val_loss: 0.2845\n",
            "Epoch 39/40\n",
            "1531/1531 [==============================] - 116s 76ms/step - loss: 0.2845 - val_loss: 0.2842\n",
            "Epoch 40/40\n",
            "1531/1531 [==============================] - 116s 76ms/step - loss: 0.2841 - val_loss: 0.2841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7jrzf7UnVwL"
      },
      "source": [
        "That reduced the validaiton error but the final validation loss is still slightly higher than the unregularized model and mae is about $2.17^\\circ$ degree. Nevertheless, this model does not overfit and generalizes better than the unregularized model and the evaluation scores are more stable. You can further try to improve the model by tuning the hyperparameters such as the numeber of neurons in the recurrent layer, the learning rate and the drop out and weight decay rates. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhdlsC9_eCt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef13678e-a39d-4864-fe92-de26c9cceff6"
      },
      "source": [
        "mae_recurrent_regularized=model_regularized.evaluate(val_gen, steps=val_steps)\n",
        "val['T (degC)'].std(axis=0)*mae_recurrent_regularized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "436/436 [==============================] - 4s 8ms/step - loss: 0.2841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.173445837579321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M7bd9fSeCuD"
      },
      "source": [
        "## Stacking recurrent layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfJTu0lNeCuE"
      },
      "source": [
        "Because you’re no longer overfitting but seem to have hit a performance bottleneck, you should consider increasing the capacity of the network. Recall the description of\n",
        "the universal machine-learning workflow: it’s generally a good idea to increase the capacity of your network until overfitting becomes the primary obstacle (a\n",
        "you’re already taking basic steps to mitigate overfitting, such as using dropout). As long as you aren’t overfitting too badly, you’re likely under capacity.\n",
        "Increasing network capacity is typically done by increasing the number of units in the layers or adding more layers. Recurrent layer stacking is a classic way to build\n",
        "more-powerful recurrent networks: for instance, what currently powers the Google Translate algorithm is a stack of seven large LSTM layers—that’s huge.\n",
        "\n",
        "To stack recurrent layers on top of each other in Keras, all intermediate layers should return their full sequence of outputs (a 3D tensor) rather than their output at\n",
        "the last timestep. This is done by specifying <code>return_sequences=True</code> ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuYN9xWOeCuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44cb93e-2623-444e-e7b4-9551ff3b864c"
      },
      "source": [
        "\n",
        "train_steps= int((train_df.shape[0]-LOOKBACK-OFFSET)/BATCH_SIZE)\n",
        "val_steps= int((val_df.shape[0]-LOOKBACK-OFFSET)/BATCH_SIZE)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1e-3, restore_best_weights=True)\n",
        "\n",
        "model=keras.Sequential(\n",
        "    [\n",
        "        layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, kernel_regularizer=regularizers.l2(0.001), \n",
        "                   recurrent_regularizer=regularizers.l2(0.001), return_sequences=True),\n",
        "        layers.GRU(64, dropout=0.1, recurrent_dropout=0.5, kernel_regularizer=regularizers.l2(0.001), \n",
        "                   recurrent_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "model.compile(optimizer='Adam', loss='mae')\n",
        "\n",
        "\n",
        "history = model.fit(train_gen, steps_per_epoch=train_steps, epochs=30, validation_data=val_gen, validation_steps=val_steps, callbacks=[early_stopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_12 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/30\n",
            "1531/1531 [==============================] - 230s 150ms/step - loss: 0.3945 - val_loss: 0.3369\n",
            "Epoch 2/30\n",
            "1531/1531 [==============================] - 229s 149ms/step - loss: 0.3411 - val_loss: 0.3161\n",
            "Epoch 3/30\n",
            "1531/1531 [==============================] - 232s 152ms/step - loss: 0.3272 - val_loss: 0.3133\n",
            "Epoch 4/30\n",
            "1531/1531 [==============================] - 233s 152ms/step - loss: 0.3207 - val_loss: 0.3074\n",
            "Epoch 5/30\n",
            "1531/1531 [==============================] - 234s 153ms/step - loss: 0.3160 - val_loss: 0.3066\n",
            "Epoch 6/30\n",
            "1531/1531 [==============================] - 234s 153ms/step - loss: 0.3137 - val_loss: 0.3042\n",
            "Epoch 7/30\n",
            "1531/1531 [==============================] - 239s 156ms/step - loss: 0.3121 - val_loss: 0.3025\n",
            "Epoch 8/30\n",
            "1531/1531 [==============================] - 237s 155ms/step - loss: 0.3108 - val_loss: 0.3008\n",
            "Epoch 9/30\n",
            "1531/1531 [==============================] - 237s 155ms/step - loss: 0.3104 - val_loss: 0.2999\n",
            "Epoch 10/30\n",
            "1531/1531 [==============================] - 239s 156ms/step - loss: 0.3095 - val_loss: 0.2959\n",
            "Epoch 11/30\n",
            "1531/1531 [==============================] - 238s 156ms/step - loss: 0.3088 - val_loss: 0.2985\n",
            "Epoch 12/30\n",
            "1531/1531 [==============================] - 238s 155ms/step - loss: 0.3084 - val_loss: 0.2952\n",
            "Epoch 13/30\n",
            "1531/1531 [==============================] - 239s 156ms/step - loss: 0.3071 - val_loss: 0.2950\n",
            "Epoch 14/30\n",
            "1531/1531 [==============================] - 238s 155ms/step - loss: 0.3070 - val_loss: 0.2955\n",
            "Epoch 15/30\n",
            "1531/1531 [==============================] - 235s 153ms/step - loss: 0.3066 - val_loss: 0.2933\n",
            "Epoch 16/30\n",
            "1531/1531 [==============================] - 234s 153ms/step - loss: 0.3053 - val_loss: 0.2922\n",
            "Epoch 17/30\n",
            "1531/1531 [==============================] - 235s 153ms/step - loss: 0.3054 - val_loss: 0.2936\n",
            "Epoch 18/30\n",
            "1531/1531 [==============================] - 234s 153ms/step - loss: 0.3053 - val_loss: 0.2933\n",
            "Epoch 19/30\n",
            "1531/1531 [==============================] - 233s 152ms/step - loss: 0.3051 - val_loss: 0.2930\n",
            "Epoch 20/30\n",
            "1531/1531 [==============================] - 233s 152ms/step - loss: 0.3044 - val_loss: 0.2919\n",
            "Epoch 21/30\n",
            "1531/1531 [==============================] - 232s 152ms/step - loss: 0.3042 - val_loss: 0.2938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neoQceZveCuQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "55cbe390-b9f2-4416-fd40-3b594e93b92e"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVX2dLESeCuT"
      },
      "source": [
        " Overall it appears that stacking more recurrent layers ddid not help much on this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXSMGNBZlcG"
      },
      "source": [
        "## Evaluating the model on the test set\n",
        "Let's evaluate our best model ( the regularized recurrent model) on the test data. The mae of the model on the test data is $1.94^\\circ$ degree. This means on average the model was able to predict the temprature 24 hours in the future with $1.94^\\circ$ error.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYcAq44TZ9Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a00dbb-4767-4d32-c4d4-8ba4934f0fdc"
      },
      "source": [
        "test_steps= int((test_df.shape[0]-LOOKBACK-OFFSET)/BATCH_SIZE)\n",
        "mae_test=model_regularized.evaluate(test_gen, steps=test_steps)\n",
        "test['T (degC)'].std(axis=0)*mae_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "217/217 [==============================] - 2s 8ms/step - loss: 0.2477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9439480412958081"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    }
  ]
}