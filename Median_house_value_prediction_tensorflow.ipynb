{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Median_house_value_prediction_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamrika/Deep-Learning/blob/main/Median_house_value_prediction_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBrHWBgksE-J"
      },
      "source": [
        "# 1. Implementing the Learning Algorithm using tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEykaLkOdn2F"
      },
      "source": [
        "1.1 Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdkL2k3icDHe"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dyEVixAdtiO"
      },
      "source": [
        "Parameter Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-omkOmP_cras"
      },
      "source": [
        "\"\"\"\n",
        "nx is the number of neurons in the input layer (i.e., the number of features in the dataset)\n",
        "nh is the number of neurons in the hidden layer \n",
        "ny is the number of neurons in the output layer (For this example we are using one nueron in the output layer so ny=1)\n",
        "\"\"\"\n",
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set tensorflow global random seed\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer. Note that weights and biases are defined as tensorflow variables instead of numpy arrays\n",
        "    W1=tf.Variable(tf.random.uniform(shape=(nh,nx), minval=-0.01, maxval=0.01), name=\"W1\")\n",
        "    b1=tf.Variable(tf.zeros(shape=(nh,1),name=\"b1\" ))\n",
        "    W2=tf.Variable(tf.random.uniform(shape=(ny,nh), minval=-0.01, maxval=0.01), name=\"W2\")\n",
        "    b2=tf.Variable(tf.zeros(shape=(ny,1), name=\"b2\"))\n",
        "   \n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-61trzqhkBS"
      },
      "source": [
        "Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUpahRLTcsaz"
      },
      "source": [
        "\"\"\"\n",
        "In forward pass we do the computations in the computational graph. We cache the intermediate nodes we will later need in the backward pass\n",
        "\"\"\"\n",
        "def forward_pass(parameters,X):\n",
        "    #the input image is read as an integer, use tf.cast to cast it to float before using it in fowrard pass computation.\n",
        "    #X= tf.cast(X, tf.float32) \n",
        "    #print(\"debugging:::\",parameters) \n",
        "    Z1= tf.matmul(parameters[\"W1\"],X)+ parameters[\"b1\"] # b1 is broadcasted n times before it is added to \n",
        "    A1=tf.nn.relu(Z1)\n",
        "    Z2=tf.matmul(parameters[\"W2\"],A1)+parameters[\"b2\"] #b2 is broadcasted n times before it is added to np.dpt(W2,A1)\n",
        "    \n",
        "    Yhat=Z2\n",
        "       \n",
        "    return Yhat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p59w5zV7hoHQ"
      },
      "source": [
        "Compute Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGvrb555cwC5"
      },
      "source": [
        "\"\"\"\n",
        "n is the number of examples, y is a vector of actual/observed outputs and yhat is a vector of predicted outputs\n",
        "\"\"\" \n",
        "def compute_loss(Y,Yhat):\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "    loss = mse(Y, Yhat)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr_GH1a3iWxj"
      },
      "source": [
        "Backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTYA-NZCc0wt"
      },
      "source": [
        "def backward_pass(parameters, loss, tape):\n",
        "    gradients= tape.gradient(loss,parameters)\n",
        "    return gradients\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxLjREUNh_u5"
      },
      "source": [
        "Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQBhcnAUc7QH"
      },
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"].assign_sub(learning_rate*gradients[\"W1\"])\n",
        "    parameters[\"W2\"].assign_sub(learning_rate*gradients[\"W2\"])\n",
        "    parameters[\"b1\"].assign_sub(learning_rate*gradients[\"b1\"])\n",
        "    parameters[\"b2\"].assign_sub(learning_rate*gradients[\"b2\"])\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY2DYMV2jhkd"
      },
      "source": [
        "Creating the Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRrWKJokdFGH"
      },
      "source": [
        "\"\"\"\n",
        "Arguments: train_X: is the training dataset (features)\n",
        "           train_Y: is the vector of labels for training_X\n",
        "           val_X: is the vector of validation dataset (features)\n",
        "           val_y: is the vector of labels for val_X\n",
        "           nh: is the number of neurons in the hidden layer\n",
        "           num_iterations: The number of iterations of gradient descent\n",
        "\"\"\"\n",
        "def create_nn_model(train_X,train_Y,nh, val_X, val_Y, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Do some safety check on the data before proceeding. \n",
        "    train_X and val_X must have the same number of features (i.e., same number of rows)\n",
        "    train_X must have the same number of examples as train_Y (i.e., same number of columns )\n",
        "    val_X must have the same number of examples as Val_Y\n",
        "    \"\"\" \n",
        "    #tf.enable_eager_execution()\n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\" \n",
        "    \n",
        "    \n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "    \n",
        "    # We want to use this network for binary classification, so we have only one neuron in the output layer with a sigmoid activation\n",
        "    ny=1\n",
        "    \n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "    \n",
        "    \n",
        "    #initialize lists to store the training and valideation losses. \n",
        "    val_losses=[]\n",
        "    train_losses=[]\n",
        "    \n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "      \n",
        "      \"\"\"\n",
        "        run forward pass and compute the loss function on training and validation data. \n",
        "        Note that the forward pass and loss computations on the training data are enclosed inside the gradient tape context in order to build the computational graph.\n",
        "        The gradients are only computed on the training data and used to update the parameter. Validation data is not used for training and updating the parameters.\n",
        "        \"\"\"\n",
        "      \n",
        "      with tf.GradientTape() as tape:\n",
        "        #run the forward pass on train_X\n",
        "        train_Yhat=forward_pass(parameters,train_X)\n",
        "        #compute the train_loss\n",
        "        train_loss=compute_loss(train_Y,train_Yhat)\n",
        "\n",
        "\n",
        "       #compute validation loss\n",
        "      Yhat_val= forward_pass(parameters,val_X)\n",
        "      val_loss=compute_loss(val_Y,Yhat_val)\n",
        "        \n",
        "      \n",
        "      #print the trianing loss and validation loss for each iteration.\n",
        "      print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss,val_loss))\n",
        "\n",
        "       # append the train and validation loss for the current iteration to the train_losses and val_losses \n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "     \n",
        "      \"\"\"\n",
        "      Compute the gradients and update the parameters\n",
        "      \"\"\"    \n",
        "      #compute the gradients on the training data\n",
        "      gradients=backward_pass(parameters,train_loss,tape)\n",
        "\n",
        "      # update the parameters\n",
        "      parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "    \n",
        "    \n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_losses,\n",
        "             \"train_loss\": train_losses}\n",
        "        \n",
        "        \n",
        "    #return the parameters and the history\n",
        "    return parameters, history\n",
        "        \n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncLAS1CjoXM"
      },
      "source": [
        "predicting and evaluating the NN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxjJV_qpdJS1"
      },
      "source": [
        "def predict(parameters,X):\n",
        "    Yhat=forward_pass(parameters, X) \n",
        "    print(Yhat)\n",
        "    return Yhat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AUy_0a7jqok"
      },
      "source": [
        "Mean Absolute Percentage Error for accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU6vDzQDdTLt"
      },
      "source": [
        "def MAPE(observedY,predictedY):\n",
        "    mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
        "    mape_out = mape(observedY, predictedY)\n",
        "    return mape_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1k11HszgCRP"
      },
      "source": [
        "def plotLearningCurve():\n",
        "  plt.plot(range(0,iterations),history[\"train_loss\"],'b')\n",
        "  plt.plot(range(0,iterations),history[\"val_loss\"],'r')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('iterations')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNKEtl44ktE8"
      },
      "source": [
        "#2. Preparing California Housing Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1PKhU3_k9Nd",
        "outputId": "5647e016-5422-4800-f97d-b535dac0558c"
      },
      "source": [
        "# reading the input datasets train.csv and validation.csv \n",
        "cal_housing_train = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
        "cal_housing_test = pd.read_csv(\"sample_data/california_housing_test.csv\")\n",
        "print(cal_housing_train.shape)\n",
        "print(cal_housing_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17000, 9)\n",
            "(3000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "1py9pjbP3Hkw",
        "outputId": "49e1f0ed-e643-41c0-8f06-2ea4c712a2ab"
      },
      "source": [
        "cal_housing_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -114.31     34.19  ...         1.4936             66900.0\n",
              "1    -114.47     34.40  ...         1.8200             80100.0\n",
              "2    -114.56     33.69  ...         1.6509             85700.0\n",
              "3    -114.57     33.64  ...         3.1917             73400.0\n",
              "4    -114.57     33.57  ...         1.9250             65500.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnWtoxcvlutQ"
      },
      "source": [
        "Separated the features from labels for training and validation data and stored them in separate numpy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGodBGk6ldxr"
      },
      "source": [
        "df = pd.DataFrame(cal_housing_train) \n",
        "train = df.sample(frac = 0.8) \n",
        "val = df.drop(train.index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT-Oka6jl4D6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3twKUc8l9eA",
        "outputId": "80f46750-9f6f-4a4c-893c-a65a09b328d9"
      },
      "source": [
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(cal_housing_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13600, 9)\n",
            "(3400, 9)\n",
            "(3000, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZJYi60Z9iRY"
      },
      "source": [
        "train = train.to_numpy()\n",
        "val = val.to_numpy()\n",
        "test = cal_housing_test.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwGHglm0mpYg",
        "outputId": "67e554c9-64c3-4fed-a07c-5226b0662f38"
      },
      "source": [
        "\n",
        "#everything minus the last row is X\n",
        "train_X=train[:,:-1]\n",
        "#the last row (at index -1) is Y\n",
        "train_Y=train[:,-1]\n",
        "# the labels train_Y and val_Y have to be reshaped to a 2D array for the matrix operations to work in the forward and backward passes\n",
        "train_Y=np.reshape(train_Y, (1,train_Y.size))\n",
        "val_X=val[:,:-1]\n",
        "val_Y=val[:,-1]\n",
        "val_Y=np.reshape(val_Y, (1,val_Y.size))\n",
        "\n",
        "test_X=test[:,:-1]\n",
        "test_Y=test[:,-1]\n",
        "test_Y=np.reshape(test_Y, (1,test_Y.size))\n",
        "print(train_X.shape)\n",
        "print(val_X.shape)\n",
        "print(test_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(val_Y.shape)\n",
        "print(test_Y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13600, 8)\n",
            "(3400, 8)\n",
            "(3000, 8)\n",
            "(1, 13600)\n",
            "(1, 3400)\n",
            "(1, 3000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XOakEzwHzj2"
      },
      "source": [
        "val_X = (val_X - np.mean(train_X, axis=0)) / np.std(train_X, axis=0)\n",
        "test_X =(test_X - np.mean(train_X, axis=0)) / np.std(train_X, axis=0)\n",
        "train_X = (train_X - np.mean(train_X, axis=0)) / np.std(train_X, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slp3djUN-ZmA",
        "outputId": "7aaa425f-8283-41b1-e5a0-d0ec4b5ef7e2"
      },
      "source": [
        "train_X.mean(),val_X.mean(),test_X.mean()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.207008349477773e-15, 0.00633291250461858, -0.014270747769107452)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuS7f8lbn5jc"
      },
      "source": [
        "train_Y = train_Y/100000\n",
        "val_Y = val_Y/100000\n",
        "test_Y = test_Y/100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHaa6XfgEMXM",
        "outputId": "683868b0-f55e-438a-b7fb-1e5b8466f0f8"
      },
      "source": [
        "train_X.shape[1],val_X.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0YKH4GVoGaj",
        "outputId": "d20f70f3-f22c-4eff-b190-9af5ada1fa29"
      },
      "source": [
        "iterations = 1000\n",
        "parameters, history = create_nn_model(train_X.T, train_Y.T, 100, val_X.T, val_Y.T, iterations, 0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 :train_loss:5.644540309906006 val_loss5.630524635314941\n",
            "iteration 1 :train_loss:2.0245490074157715 val_loss2.0093231201171875\n",
            "iteration 2 :train_loss:1.452337384223938 val_loss1.4366779327392578\n",
            "iteration 3 :train_loss:1.3639476299285889 val_loss1.3481411933898926\n",
            "iteration 4 :train_loss:1.3507614135742188 val_loss1.3349045515060425\n",
            "iteration 5 :train_loss:1.348832130432129 val_loss1.3329558372497559\n",
            "iteration 6 :train_loss:1.348544955253601 val_loss1.3326603174209595\n",
            "iteration 7 :train_loss:1.3484941720962524 val_loss1.3326064348220825\n",
            "iteration 8 :train_loss:1.3484781980514526 val_loss1.33258855342865\n",
            "iteration 9 :train_loss:1.348468542098999 val_loss1.3325773477554321\n",
            "iteration 10 :train_loss:1.3484601974487305 val_loss1.3325680494308472\n",
            "iteration 11 :train_loss:1.348452091217041 val_loss1.3325597047805786\n",
            "iteration 12 :train_loss:1.348444938659668 val_loss1.3325520753860474\n",
            "iteration 13 :train_loss:1.3484385013580322 val_loss1.3325450420379639\n",
            "iteration 14 :train_loss:1.3484326601028442 val_loss1.3325387239456177\n",
            "iteration 15 :train_loss:1.348427414894104 val_loss1.3325327634811401\n",
            "iteration 16 :train_loss:1.3484222888946533 val_loss1.3325272798538208\n",
            "iteration 17 :train_loss:1.34841787815094 val_loss1.3325223922729492\n",
            "iteration 18 :train_loss:1.3484137058258057 val_loss1.3325178623199463\n",
            "iteration 19 :train_loss:1.3484095335006714 val_loss1.3325133323669434\n",
            "iteration 20 :train_loss:1.3484059572219849 val_loss1.332509160041809\n",
            "iteration 21 :train_loss:1.348402738571167 val_loss1.332505702972412\n",
            "iteration 22 :train_loss:1.34839928150177 val_loss1.3325021266937256\n",
            "iteration 23 :train_loss:1.3483965396881104 val_loss1.3324991464614868\n",
            "iteration 24 :train_loss:1.3483936786651611 val_loss1.332495927810669\n",
            "iteration 25 :train_loss:1.3483909368515015 val_loss1.3324929475784302\n",
            "iteration 26 :train_loss:1.3483880758285522 val_loss1.332490086555481\n",
            "iteration 27 :train_loss:1.3483860492706299 val_loss1.33248770236969\n",
            "iteration 28 :train_loss:1.348383903503418 val_loss1.3324856758117676\n",
            "iteration 29 :train_loss:1.3483818769454956 val_loss1.3324832916259766\n",
            "iteration 30 :train_loss:1.348380208015442 val_loss1.3324813842773438\n",
            "iteration 31 :train_loss:1.34837806224823 val_loss1.3324793577194214\n",
            "iteration 32 :train_loss:1.3483768701553345 val_loss1.3324774503707886\n",
            "iteration 33 :train_loss:1.348374843597412 val_loss1.3324755430221558\n",
            "iteration 34 :train_loss:1.3483736515045166 val_loss1.3324741125106812\n",
            "iteration 35 :train_loss:1.3483718633651733 val_loss1.3324724435806274\n",
            "iteration 36 :train_loss:1.348370909690857 val_loss1.3324710130691528\n",
            "iteration 37 :train_loss:1.3483695983886719 val_loss1.3324695825576782\n",
            "iteration 38 :train_loss:1.348368525505066 val_loss1.3324683904647827\n",
            "iteration 39 :train_loss:1.34836745262146 val_loss1.3324670791625977\n",
            "iteration 40 :train_loss:1.348366141319275 val_loss1.3324657678604126\n",
            "iteration 41 :train_loss:1.348365068435669 val_loss1.3324646949768066\n",
            "iteration 42 :train_loss:1.3483645915985107 val_loss1.3324635028839111\n",
            "iteration 43 :train_loss:1.3483636379241943 val_loss1.3324626684188843\n",
            "iteration 44 :train_loss:1.3483624458312988 val_loss1.3324618339538574\n",
            "iteration 45 :train_loss:1.3483617305755615 val_loss1.3324607610702515\n",
            "iteration 46 :train_loss:1.3483607769012451 val_loss1.332459807395935\n",
            "iteration 47 :train_loss:1.3483601808547974 val_loss1.3324590921401978\n",
            "iteration 48 :train_loss:1.3483591079711914 val_loss1.332458257675171\n",
            "iteration 49 :train_loss:1.3483585119247437 val_loss1.3324575424194336\n",
            "iteration 50 :train_loss:1.3483582735061646 val_loss1.3324565887451172\n",
            "iteration 51 :train_loss:1.3483575582504272 val_loss1.332456111907959\n",
            "iteration 52 :train_loss:1.348357081413269 val_loss1.3324552774429321\n",
            "iteration 53 :train_loss:1.3483558893203735 val_loss1.3324545621871948\n",
            "iteration 54 :train_loss:1.3483556509017944 val_loss1.3324540853500366\n",
            "iteration 55 :train_loss:1.3483550548553467 val_loss1.3324534893035889\n",
            "iteration 56 :train_loss:1.348354458808899 val_loss1.3324528932571411\n",
            "iteration 57 :train_loss:1.3483541011810303 val_loss1.3324522972106934\n",
            "iteration 58 :train_loss:1.348353624343872 val_loss1.3324519395828247\n",
            "iteration 59 :train_loss:1.348353385925293 val_loss1.3324514627456665\n",
            "iteration 60 :train_loss:1.348352313041687 val_loss1.3324508666992188\n",
            "iteration 61 :train_loss:1.3483521938323975 val_loss1.332450270652771\n",
            "iteration 62 :train_loss:1.3483517169952393 val_loss1.332450032234192\n",
            "iteration 63 :train_loss:1.3483514785766602 val_loss1.3324494361877441\n",
            "iteration 64 :train_loss:1.348351001739502 val_loss1.332449197769165\n",
            "iteration 65 :train_loss:1.3483508825302124 val_loss1.3324487209320068\n",
            "iteration 66 :train_loss:1.3483505249023438 val_loss1.3324482440948486\n",
            "iteration 67 :train_loss:1.348350167274475 val_loss1.3324480056762695\n",
            "iteration 68 :train_loss:1.348349928855896 val_loss1.3324474096298218\n",
            "iteration 69 :train_loss:1.3483492136001587 val_loss1.3324471712112427\n",
            "iteration 70 :train_loss:1.34834885597229 val_loss1.3324470520019531\n",
            "iteration 71 :train_loss:1.3483487367630005 val_loss1.332446575164795\n",
            "iteration 72 :train_loss:1.3483484983444214 val_loss1.3324463367462158\n",
            "iteration 73 :train_loss:1.3483481407165527 val_loss1.3324459791183472\n",
            "iteration 74 :train_loss:1.3483480215072632 val_loss1.332445740699768\n",
            "iteration 75 :train_loss:1.3483474254608154 val_loss1.3324453830718994\n",
            "iteration 76 :train_loss:1.3483473062515259 val_loss1.3324451446533203\n",
            "iteration 77 :train_loss:1.3483471870422363 val_loss1.3324449062347412\n",
            "iteration 78 :train_loss:1.3483470678329468 val_loss1.3324445486068726\n",
            "iteration 79 :train_loss:1.3483468294143677 val_loss1.3324443101882935\n",
            "iteration 80 :train_loss:1.3483465909957886 val_loss1.332444190979004\n",
            "iteration 81 :train_loss:1.348346471786499 val_loss1.3324438333511353\n",
            "iteration 82 :train_loss:1.3483457565307617 val_loss1.3324435949325562\n",
            "iteration 83 :train_loss:1.3483456373214722 val_loss1.3324434757232666\n",
            "iteration 84 :train_loss:1.3483456373214722 val_loss1.332443118095398\n",
            "iteration 85 :train_loss:1.3483452796936035 val_loss1.3324428796768188\n",
            "iteration 86 :train_loss:1.3483452796936035 val_loss1.3324428796768188\n",
            "iteration 87 :train_loss:1.3483450412750244 val_loss1.3324425220489502\n",
            "iteration 88 :train_loss:1.3483449220657349 val_loss1.332442283630371\n",
            "iteration 89 :train_loss:1.3483446836471558 val_loss1.332442283630371\n",
            "iteration 90 :train_loss:1.3483445644378662 val_loss1.3324419260025024\n",
            "iteration 91 :train_loss:1.3483444452285767 val_loss1.3324419260025024\n",
            "iteration 92 :train_loss:1.348344326019287 val_loss1.3324416875839233\n",
            "iteration 93 :train_loss:1.3483442068099976 val_loss1.3324414491653442\n",
            "iteration 94 :train_loss:1.3483442068099976 val_loss1.3324414491653442\n",
            "iteration 95 :train_loss:1.3483439683914185 val_loss1.3324410915374756\n",
            "iteration 96 :train_loss:1.348343849182129 val_loss1.332440972328186\n",
            "iteration 97 :train_loss:1.3483437299728394 val_loss1.3324408531188965\n",
            "iteration 98 :train_loss:1.3483434915542603 val_loss1.3324404954910278\n",
            "iteration 99 :train_loss:1.3483432531356812 val_loss1.3324404954910278\n",
            "iteration 100 :train_loss:1.3483431339263916 val_loss1.3324402570724487\n",
            "iteration 101 :train_loss:1.348343014717102 val_loss1.3324402570724487\n",
            "iteration 102 :train_loss:1.348343014717102 val_loss1.3324402570724487\n",
            "iteration 103 :train_loss:1.3483426570892334 val_loss1.3324400186538696\n",
            "iteration 104 :train_loss:1.3483425378799438 val_loss1.3324397802352905\n",
            "iteration 105 :train_loss:1.3483421802520752 val_loss1.332439661026001\n",
            "iteration 106 :train_loss:1.3483421802520752 val_loss1.332439661026001\n",
            "iteration 107 :train_loss:1.3483421802520752 val_loss1.3324394226074219\n",
            "iteration 108 :train_loss:1.348341941833496 val_loss1.3324394226074219\n",
            "iteration 109 :train_loss:1.3483418226242065 val_loss1.3324394226074219\n",
            "iteration 110 :train_loss:1.3483418226242065 val_loss1.3324393033981323\n",
            "iteration 111 :train_loss:1.3483418226242065 val_loss1.3324389457702637\n",
            "iteration 112 :train_loss:1.3483415842056274 val_loss1.3324389457702637\n",
            "iteration 113 :train_loss:1.3483415842056274 val_loss1.3324387073516846\n",
            "iteration 114 :train_loss:1.348341464996338 val_loss1.332438588142395\n",
            "iteration 115 :train_loss:1.3483412265777588 val_loss1.332438349723816\n",
            "iteration 116 :train_loss:1.3483411073684692 val_loss1.332438349723816\n",
            "iteration 117 :train_loss:1.3483411073684692 val_loss1.3324382305145264\n",
            "iteration 118 :train_loss:1.3483409881591797 val_loss1.3324381113052368\n",
            "iteration 119 :train_loss:1.3483409881591797 val_loss1.3324379920959473\n",
            "iteration 120 :train_loss:1.3483409881591797 val_loss1.3324378728866577\n",
            "iteration 121 :train_loss:1.3483407497406006 val_loss1.3324376344680786\n",
            "iteration 122 :train_loss:1.3483405113220215 val_loss1.3324378728866577\n",
            "iteration 123 :train_loss:1.348340392112732 val_loss1.3324376344680786\n",
            "iteration 124 :train_loss:1.348340392112732 val_loss1.3324373960494995\n",
            "iteration 125 :train_loss:1.348340392112732 val_loss1.3324373960494995\n",
            "iteration 126 :train_loss:1.3483402729034424 val_loss1.3324373960494995\n",
            "iteration 127 :train_loss:1.3483402729034424 val_loss1.3324370384216309\n",
            "iteration 128 :train_loss:1.3483402729034424 val_loss1.3324369192123413\n",
            "iteration 129 :train_loss:1.3483401536941528 val_loss1.3324369192123413\n",
            "iteration 130 :train_loss:1.3483400344848633 val_loss1.3324369192123413\n",
            "iteration 131 :train_loss:1.3483400344848633 val_loss1.3324368000030518\n",
            "iteration 132 :train_loss:1.3483397960662842 val_loss1.3324368000030518\n",
            "iteration 133 :train_loss:1.3483397960662842 val_loss1.3324365615844727\n",
            "iteration 134 :train_loss:1.3483396768569946 val_loss1.3324365615844727\n",
            "iteration 135 :train_loss:1.3483396768569946 val_loss1.3324365615844727\n",
            "iteration 136 :train_loss:1.3483396768569946 val_loss1.3324363231658936\n",
            "iteration 137 :train_loss:1.348339557647705 val_loss1.332436203956604\n",
            "iteration 138 :train_loss:1.348339557647705 val_loss1.332436203956604\n",
            "iteration 139 :train_loss:1.348339557647705 val_loss1.332435965538025\n",
            "iteration 140 :train_loss:1.3483388423919678 val_loss1.332435965538025\n",
            "iteration 141 :train_loss:1.3483388423919678 val_loss1.332435965538025\n",
            "iteration 142 :train_loss:1.3483387231826782 val_loss1.332435965538025\n",
            "iteration 143 :train_loss:1.3483386039733887 val_loss1.3324356079101562\n",
            "iteration 144 :train_loss:1.3483386039733887 val_loss1.3324356079101562\n",
            "iteration 145 :train_loss:1.3483386039733887 val_loss1.3324356079101562\n",
            "iteration 146 :train_loss:1.3483386039733887 val_loss1.3324354887008667\n",
            "iteration 147 :train_loss:1.3483383655548096 val_loss1.3324353694915771\n",
            "iteration 148 :train_loss:1.34833824634552 val_loss1.3324353694915771\n",
            "iteration 149 :train_loss:1.34833824634552 val_loss1.3324353694915771\n",
            "iteration 150 :train_loss:1.34833824634552 val_loss1.3324353694915771\n",
            "iteration 151 :train_loss:1.34833824634552 val_loss1.3324353694915771\n",
            "iteration 152 :train_loss:1.3483381271362305 val_loss1.332435131072998\n",
            "iteration 153 :train_loss:1.348338007926941 val_loss1.332435131072998\n",
            "iteration 154 :train_loss:1.348338007926941 val_loss1.332435131072998\n",
            "iteration 155 :train_loss:1.348338007926941 val_loss1.332435131072998\n",
            "iteration 156 :train_loss:1.3483376502990723 val_loss1.332435131072998\n",
            "iteration 157 :train_loss:1.3483376502990723 val_loss1.3324347734451294\n",
            "iteration 158 :train_loss:1.3483376502990723 val_loss1.3324347734451294\n",
            "iteration 159 :train_loss:1.3483376502990723 val_loss1.3324347734451294\n",
            "iteration 160 :train_loss:1.3483375310897827 val_loss1.3324346542358398\n",
            "iteration 161 :train_loss:1.3483375310897827 val_loss1.3324346542358398\n",
            "iteration 162 :train_loss:1.3483375310897827 val_loss1.3324345350265503\n",
            "iteration 163 :train_loss:1.3483374118804932 val_loss1.3324345350265503\n",
            "iteration 164 :train_loss:1.3483374118804932 val_loss1.3324345350265503\n",
            "iteration 165 :train_loss:1.3483374118804932 val_loss1.3324345350265503\n",
            "iteration 166 :train_loss:1.3483374118804932 val_loss1.3324345350265503\n",
            "iteration 167 :train_loss:1.3483374118804932 val_loss1.3324341773986816\n",
            "iteration 168 :train_loss:1.3483374118804932 val_loss1.3324341773986816\n",
            "iteration 169 :train_loss:1.3483372926712036 val_loss1.3324341773986816\n",
            "iteration 170 :train_loss:1.3483372926712036 val_loss1.3324341773986816\n",
            "iteration 171 :train_loss:1.3483372926712036 val_loss1.332434058189392\n",
            "iteration 172 :train_loss:1.348337173461914 val_loss1.3324339389801025\n",
            "iteration 173 :train_loss:1.348337173461914 val_loss1.3324339389801025\n",
            "iteration 174 :train_loss:1.348337173461914 val_loss1.3324339389801025\n",
            "iteration 175 :train_loss:1.348337173461914 val_loss1.3324339389801025\n",
            "iteration 176 :train_loss:1.348336935043335 val_loss1.3324339389801025\n",
            "iteration 177 :train_loss:1.348336935043335 val_loss1.332433819770813\n",
            "iteration 178 :train_loss:1.348337173461914 val_loss1.332433819770813\n",
            "iteration 179 :train_loss:1.3483368158340454 val_loss1.3324337005615234\n",
            "iteration 180 :train_loss:1.348336935043335 val_loss1.3324333429336548\n",
            "iteration 181 :train_loss:1.348336935043335 val_loss1.3324337005615234\n",
            "iteration 182 :train_loss:1.3483368158340454 val_loss1.3324333429336548\n",
            "iteration 183 :train_loss:1.3483368158340454 val_loss1.3324337005615234\n",
            "iteration 184 :train_loss:1.3483366966247559 val_loss1.3324334621429443\n",
            "iteration 185 :train_loss:1.3483368158340454 val_loss1.3324333429336548\n",
            "iteration 186 :train_loss:1.3483366966247559 val_loss1.3324333429336548\n",
            "iteration 187 :train_loss:1.3483365774154663 val_loss1.3324331045150757\n",
            "iteration 188 :train_loss:1.3483365774154663 val_loss1.3324331045150757\n",
            "iteration 189 :train_loss:1.3483365774154663 val_loss1.3324331045150757\n",
            "iteration 190 :train_loss:1.3483366966247559 val_loss1.3324331045150757\n",
            "iteration 191 :train_loss:1.3483365774154663 val_loss1.3324331045150757\n",
            "iteration 192 :train_loss:1.3483363389968872 val_loss1.3324331045150757\n",
            "iteration 193 :train_loss:1.3483363389968872 val_loss1.3324329853057861\n",
            "iteration 194 :train_loss:1.3483363389968872 val_loss1.332432746887207\n",
            "iteration 195 :train_loss:1.3483363389968872 val_loss1.332432746887207\n",
            "iteration 196 :train_loss:1.3483363389968872 val_loss1.332432746887207\n",
            "iteration 197 :train_loss:1.3483363389968872 val_loss1.332432746887207\n",
            "iteration 198 :train_loss:1.3483362197875977 val_loss1.332432746887207\n",
            "iteration 199 :train_loss:1.348336100578308 val_loss1.332432746887207\n",
            "iteration 200 :train_loss:1.348336100578308 val_loss1.332432508468628\n",
            "iteration 201 :train_loss:1.348336100578308 val_loss1.332432508468628\n",
            "iteration 202 :train_loss:1.348336100578308 val_loss1.332432508468628\n",
            "iteration 203 :train_loss:1.348335862159729 val_loss1.332432508468628\n",
            "iteration 204 :train_loss:1.348335862159729 val_loss1.332432508468628\n",
            "iteration 205 :train_loss:1.3483355045318604 val_loss1.332432508468628\n",
            "iteration 206 :train_loss:1.3483355045318604 val_loss1.3324323892593384\n",
            "iteration 207 :train_loss:1.3483352661132812 val_loss1.3324322700500488\n",
            "iteration 208 :train_loss:1.3483352661132812 val_loss1.3324322700500488\n",
            "iteration 209 :train_loss:1.3483352661132812 val_loss1.3324322700500488\n",
            "iteration 210 :train_loss:1.3483352661132812 val_loss1.3324322700500488\n",
            "iteration 211 :train_loss:1.3483352661132812 val_loss1.3324322700500488\n",
            "iteration 212 :train_loss:1.3483352661132812 val_loss1.3324322700500488\n",
            "iteration 213 :train_loss:1.3483352661132812 val_loss1.3324320316314697\n",
            "iteration 214 :train_loss:1.3483352661132812 val_loss1.3324319124221802\n",
            "iteration 215 :train_loss:1.3483351469039917 val_loss1.3324319124221802\n",
            "iteration 216 :train_loss:1.3483352661132812 val_loss1.3324319124221802\n",
            "iteration 217 :train_loss:1.3483351469039917 val_loss1.3324319124221802\n",
            "iteration 218 :train_loss:1.3483351469039917 val_loss1.3324319124221802\n",
            "iteration 219 :train_loss:1.3483351469039917 val_loss1.3324317932128906\n",
            "iteration 220 :train_loss:1.3483351469039917 val_loss1.3324319124221802\n",
            "iteration 221 :train_loss:1.3483351469039917 val_loss1.332431674003601\n",
            "iteration 222 :train_loss:1.3483351469039917 val_loss1.332431674003601\n",
            "iteration 223 :train_loss:1.3483351469039917 val_loss1.332431674003601\n",
            "iteration 224 :train_loss:1.3483349084854126 val_loss1.332431674003601\n",
            "iteration 225 :train_loss:1.3483351469039917 val_loss1.3324315547943115\n",
            "iteration 226 :train_loss:1.3483349084854126 val_loss1.332431674003601\n",
            "iteration 227 :train_loss:1.3483349084854126 val_loss1.3324315547943115\n",
            "iteration 228 :train_loss:1.3483349084854126 val_loss1.3324313163757324\n",
            "iteration 229 :train_loss:1.3483349084854126 val_loss1.3324313163757324\n",
            "iteration 230 :train_loss:1.348334789276123 val_loss1.3324313163757324\n",
            "iteration 231 :train_loss:1.348334789276123 val_loss1.3324313163757324\n",
            "iteration 232 :train_loss:1.348334789276123 val_loss1.3324313163757324\n",
            "iteration 233 :train_loss:1.3483349084854126 val_loss1.3324313163757324\n",
            "iteration 234 :train_loss:1.348334789276123 val_loss1.3324313163757324\n",
            "iteration 235 :train_loss:1.348334789276123 val_loss1.3324313163757324\n",
            "iteration 236 :train_loss:1.348334789276123 val_loss1.3324310779571533\n",
            "iteration 237 :train_loss:1.348334789276123 val_loss1.3324310779571533\n",
            "iteration 238 :train_loss:1.3483346700668335 val_loss1.3324310779571533\n",
            "iteration 239 :train_loss:1.3483346700668335 val_loss1.3324310779571533\n",
            "iteration 240 :train_loss:1.3483346700668335 val_loss1.3324310779571533\n",
            "iteration 241 :train_loss:1.3483346700668335 val_loss1.3324310779571533\n",
            "iteration 242 :train_loss:1.348334550857544 val_loss1.3324309587478638\n",
            "iteration 243 :train_loss:1.348334550857544 val_loss1.3324309587478638\n",
            "iteration 244 :train_loss:1.348334550857544 val_loss1.3324307203292847\n",
            "iteration 245 :train_loss:1.348334550857544 val_loss1.3324307203292847\n",
            "iteration 246 :train_loss:1.348334550857544 val_loss1.3324307203292847\n",
            "iteration 247 :train_loss:1.348334550857544 val_loss1.3324307203292847\n",
            "iteration 248 :train_loss:1.3483344316482544 val_loss1.3324307203292847\n",
            "iteration 249 :train_loss:1.3483340740203857 val_loss1.3324307203292847\n",
            "iteration 250 :train_loss:1.3483341932296753 val_loss1.3324306011199951\n",
            "iteration 251 :train_loss:1.3483339548110962 val_loss1.3324306011199951\n",
            "iteration 252 :train_loss:1.3483340740203857 val_loss1.3324306011199951\n",
            "iteration 253 :train_loss:1.3483339548110962 val_loss1.3324306011199951\n",
            "iteration 254 :train_loss:1.3483340740203857 val_loss1.3324306011199951\n",
            "iteration 255 :train_loss:1.3483339548110962 val_loss1.3324304819107056\n",
            "iteration 256 :train_loss:1.3483339548110962 val_loss1.3324304819107056\n",
            "iteration 257 :train_loss:1.3483338356018066 val_loss1.3324304819107056\n",
            "iteration 258 :train_loss:1.3483338356018066 val_loss1.3324304819107056\n",
            "iteration 259 :train_loss:1.3483338356018066 val_loss1.3324302434921265\n",
            "iteration 260 :train_loss:1.3483338356018066 val_loss1.3324302434921265\n",
            "iteration 261 :train_loss:1.3483339548110962 val_loss1.332430362701416\n",
            "iteration 262 :train_loss:1.3483339548110962 val_loss1.3324302434921265\n",
            "iteration 263 :train_loss:1.3483338356018066 val_loss1.3324302434921265\n",
            "iteration 264 :train_loss:1.3483338356018066 val_loss1.3324302434921265\n",
            "iteration 265 :train_loss:1.3483339548110962 val_loss1.3324302434921265\n",
            "iteration 266 :train_loss:1.3483338356018066 val_loss1.3324302434921265\n",
            "iteration 267 :train_loss:1.3483338356018066 val_loss1.3324302434921265\n",
            "iteration 268 :train_loss:1.348333716392517 val_loss1.3324302434921265\n",
            "iteration 269 :train_loss:1.3483338356018066 val_loss1.3324300050735474\n",
            "iteration 270 :train_loss:1.3483338356018066 val_loss1.3324300050735474\n",
            "iteration 271 :train_loss:1.348333716392517 val_loss1.3324298858642578\n",
            "iteration 272 :train_loss:1.3483338356018066 val_loss1.3324298858642578\n",
            "iteration 273 :train_loss:1.348333716392517 val_loss1.3324298858642578\n",
            "iteration 274 :train_loss:1.3483338356018066 val_loss1.3324298858642578\n",
            "iteration 275 :train_loss:1.348333716392517 val_loss1.3324298858642578\n",
            "iteration 276 :train_loss:1.348333716392517 val_loss1.3324298858642578\n",
            "iteration 277 :train_loss:1.348333716392517 val_loss1.3324298858642578\n",
            "iteration 278 :train_loss:1.348333477973938 val_loss1.3324298858642578\n",
            "iteration 279 :train_loss:1.348333716392517 val_loss1.3324296474456787\n",
            "iteration 280 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 281 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 282 :train_loss:1.348333477973938 val_loss1.3324296474456787\n",
            "iteration 283 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 284 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 285 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 286 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 287 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 288 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 289 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 290 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 291 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 292 :train_loss:1.3483333587646484 val_loss1.3324296474456787\n",
            "iteration 293 :train_loss:1.3483332395553589 val_loss1.33242928981781\n",
            "iteration 294 :train_loss:1.3483332395553589 val_loss1.33242928981781\n",
            "iteration 295 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 296 :train_loss:1.3483332395553589 val_loss1.33242928981781\n",
            "iteration 297 :train_loss:1.3483332395553589 val_loss1.33242928981781\n",
            "iteration 298 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 299 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 300 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 301 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 302 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 303 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 304 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 305 :train_loss:1.3483332395553589 val_loss1.33242928981781\n",
            "iteration 306 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 307 :train_loss:1.3483331203460693 val_loss1.33242928981781\n",
            "iteration 308 :train_loss:1.3483327627182007 val_loss1.33242928981781\n",
            "iteration 309 :train_loss:1.3483327627182007 val_loss1.332429051399231\n",
            "iteration 310 :train_loss:1.3483331203460693 val_loss1.332429051399231\n",
            "iteration 312 :train_loss:1.3483331203460693 val_loss1.332429051399231\n",
            "iteration 313 :train_loss:1.3483331203460693 val_loss1.332429051399231\n",
            "iteration 314 :train_loss:1.3483326435089111 val_loss1.332429051399231\n",
            "iteration 315 :train_loss:1.3483326435089111 val_loss1.332429051399231\n",
            "iteration 316 :train_loss:1.3483326435089111 val_loss1.332429051399231\n",
            "iteration 317 :train_loss:1.3483326435089111 val_loss1.332429051399231\n",
            "iteration 318 :train_loss:1.3483326435089111 val_loss1.332429051399231\n",
            "iteration 319 :train_loss:1.348332405090332 val_loss1.332429051399231\n",
            "iteration 320 :train_loss:1.348332405090332 val_loss1.3324289321899414\n",
            "iteration 321 :train_loss:1.348332405090332 val_loss1.3324289321899414\n",
            "iteration 322 :train_loss:1.3483322858810425 val_loss1.3324289321899414\n",
            "iteration 323 :train_loss:1.3483322858810425 val_loss1.3324288129806519\n",
            "iteration 324 :train_loss:1.3483322858810425 val_loss1.3324288129806519\n",
            "iteration 325 :train_loss:1.3483322858810425 val_loss1.3324288129806519\n",
            "iteration 326 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 327 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 328 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 329 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 330 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 331 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 332 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 333 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 334 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 335 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 336 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 337 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 338 :train_loss:1.3483320474624634 val_loss1.3324288129806519\n",
            "iteration 339 :train_loss:1.3483320474624634 val_loss1.3324285745620728\n",
            "iteration 340 :train_loss:1.3483320474624634 val_loss1.3324285745620728\n",
            "iteration 341 :train_loss:1.3483320474624634 val_loss1.3324285745620728\n",
            "iteration 342 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 343 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 344 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 345 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 346 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 347 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 348 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 349 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 350 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 351 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 352 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 353 :train_loss:1.3483319282531738 val_loss1.3324284553527832\n",
            "iteration 354 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 355 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 356 :train_loss:1.3483318090438843 val_loss1.3324284553527832\n",
            "iteration 357 :train_loss:1.3483318090438843 val_loss1.3324284553527832\n",
            "iteration 358 :train_loss:1.3483318090438843 val_loss1.3324284553527832\n",
            "iteration 359 :train_loss:1.3483320474624634 val_loss1.3324284553527832\n",
            "iteration 360 :train_loss:1.3483318090438843 val_loss1.3324283361434937\n",
            "iteration 361 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 362 :train_loss:1.3483318090438843 val_loss1.3324283361434937\n",
            "iteration 363 :train_loss:1.3483319282531738 val_loss1.332428216934204\n",
            "iteration 364 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 365 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 366 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 367 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 368 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 369 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 370 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 371 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 372 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 373 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 374 :train_loss:1.3483318090438843 val_loss1.332428216934204\n",
            "iteration 375 :train_loss:1.3483318090438843 val_loss1.3324280977249146\n",
            "iteration 376 :train_loss:1.3483318090438843 val_loss1.3324280977249146\n",
            "iteration 377 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 378 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 379 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 380 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 381 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 382 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 383 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 384 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 385 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 386 :train_loss:1.3483318090438843 val_loss1.3324278593063354\n",
            "iteration 387 :train_loss:1.3483316898345947 val_loss1.3324278593063354\n",
            "iteration 388 :train_loss:1.3483316898345947 val_loss1.3324278593063354\n",
            "iteration 389 :train_loss:1.3483316898345947 val_loss1.3324278593063354\n",
            "iteration 390 :train_loss:1.3483316898345947 val_loss1.3324278593063354\n",
            "iteration 391 :train_loss:1.3483315706253052 val_loss1.332427740097046\n",
            "iteration 392 :train_loss:1.3483316898345947 val_loss1.3324278593063354\n",
            "iteration 393 :train_loss:1.3483315706253052 val_loss1.3324278593063354\n",
            "iteration 394 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 395 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 396 :train_loss:1.3483316898345947 val_loss1.3324276208877563\n",
            "iteration 397 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 398 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 399 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 400 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 401 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 402 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 403 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 404 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 405 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 406 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 407 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 408 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 409 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 410 :train_loss:1.3483315706253052 val_loss1.3324275016784668\n",
            "iteration 411 :train_loss:1.3483315706253052 val_loss1.3324276208877563\n",
            "iteration 412 :train_loss:1.3483315706253052 val_loss1.3324273824691772\n",
            "iteration 413 :train_loss:1.348331332206726 val_loss1.3324276208877563\n",
            "iteration 414 :train_loss:1.3483315706253052 val_loss1.3324275016784668\n",
            "iteration 415 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 416 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 417 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 418 :train_loss:1.3483315706253052 val_loss1.3324273824691772\n",
            "iteration 419 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 420 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 421 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 422 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 423 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 424 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 425 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 426 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 427 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 428 :train_loss:1.3483315706253052 val_loss1.3324273824691772\n",
            "iteration 429 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 430 :train_loss:1.348331332206726 val_loss1.3324273824691772\n",
            "iteration 431 :train_loss:1.3483312129974365 val_loss1.3324273824691772\n",
            "iteration 432 :train_loss:1.3483312129974365 val_loss1.3324271440505981\n",
            "iteration 433 :train_loss:1.3483312129974365 val_loss1.3324271440505981\n",
            "iteration 434 :train_loss:1.3483312129974365 val_loss1.3324273824691772\n",
            "iteration 435 :train_loss:1.3483312129974365 val_loss1.3324271440505981\n",
            "iteration 436 :train_loss:1.3483312129974365 val_loss1.3324271440505981\n",
            "iteration 437 :train_loss:1.3483312129974365 val_loss1.3324271440505981\n",
            "iteration 438 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 439 :train_loss:1.3483312129974365 val_loss1.3324271440505981\n",
            "iteration 440 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 441 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 442 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 443 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 444 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 445 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 446 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 447 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 448 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 449 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 450 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 451 :train_loss:1.3483312129974365 val_loss1.3324270248413086\n",
            "iteration 452 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 453 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 454 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 455 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 456 :train_loss:1.348331093788147 val_loss1.3324270248413086\n",
            "iteration 457 :train_loss:1.348331093788147 val_loss1.332426905632019\n",
            "iteration 458 :train_loss:1.348331093788147 val_loss1.332426905632019\n",
            "iteration 459 :train_loss:1.348331093788147 val_loss1.332426905632019\n",
            "iteration 460 :train_loss:1.3483309745788574 val_loss1.332426905632019\n",
            "iteration 461 :train_loss:1.348331093788147 val_loss1.3324267864227295\n",
            "iteration 462 :train_loss:1.3483309745788574 val_loss1.332426905632019\n",
            "iteration 463 :train_loss:1.3483308553695679 val_loss1.332426905632019\n",
            "iteration 464 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 465 :train_loss:1.3483309745788574 val_loss1.332426905632019\n",
            "iteration 466 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 467 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 468 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 469 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 470 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 471 :train_loss:1.3483309745788574 val_loss1.3324267864227295\n",
            "iteration 472 :train_loss:1.3483306169509888 val_loss1.3324267864227295\n",
            "iteration 473 :train_loss:1.3483308553695679 val_loss1.3324267864227295\n",
            "iteration 474 :train_loss:1.3483306169509888 val_loss1.3324267864227295\n",
            "iteration 475 :train_loss:1.3483308553695679 val_loss1.3324267864227295\n",
            "iteration 476 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 477 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 478 :train_loss:1.3483306169509888 val_loss1.3324267864227295\n",
            "iteration 479 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 480 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 481 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 482 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 483 :train_loss:1.3483306169509888 val_loss1.3324264287948608\n",
            "iteration 484 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 485 :train_loss:1.3483306169509888 val_loss1.33242666721344\n",
            "iteration 486 :train_loss:1.3483306169509888 val_loss1.3324264287948608\n",
            "iteration 487 :train_loss:1.3483306169509888 val_loss1.3324264287948608\n",
            "iteration 488 :train_loss:1.3483306169509888 val_loss1.3324264287948608\n",
            "iteration 489 :train_loss:1.3483304977416992 val_loss1.33242666721344\n",
            "iteration 490 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 491 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 492 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 493 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 494 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 495 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 496 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 497 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 498 :train_loss:1.3483304977416992 val_loss1.3324264287948608\n",
            "iteration 499 :train_loss:1.3483306169509888 val_loss1.3324264287948608\n",
            "iteration 500 :train_loss:1.3483304977416992 val_loss1.3324263095855713\n",
            "iteration 501 :train_loss:1.3483304977416992 val_loss1.3324263095855713\n",
            "iteration 502 :train_loss:1.3483303785324097 val_loss1.3324263095855713\n",
            "iteration 503 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 504 :train_loss:1.3483304977416992 val_loss1.3324263095855713\n",
            "iteration 505 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 506 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 507 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 508 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 509 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 510 :train_loss:1.3483304977416992 val_loss1.3324261903762817\n",
            "iteration 511 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 512 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 513 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 514 :train_loss:1.3483302593231201 val_loss1.3324261903762817\n",
            "iteration 515 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 516 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 517 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 518 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 519 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 520 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 521 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 522 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 523 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 524 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 525 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 526 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 527 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 528 :train_loss:1.3483302593231201 val_loss1.3324261903762817\n",
            "iteration 529 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 530 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 531 :train_loss:1.3483302593231201 val_loss1.3324260711669922\n",
            "iteration 532 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 533 :train_loss:1.3483302593231201 val_loss1.3324260711669922\n",
            "iteration 534 :train_loss:1.3483303785324097 val_loss1.3324259519577026\n",
            "iteration 535 :train_loss:1.3483302593231201 val_loss1.3324260711669922\n",
            "iteration 536 :train_loss:1.348330020904541 val_loss1.3324260711669922\n",
            "iteration 537 :train_loss:1.3483302593231201 val_loss1.3324261903762817\n",
            "iteration 538 :train_loss:1.3483303785324097 val_loss1.3324261903762817\n",
            "iteration 539 :train_loss:1.3483302593231201 val_loss1.3324259519577026\n",
            "iteration 540 :train_loss:1.3483302593231201 val_loss1.3324259519577026\n",
            "iteration 541 :train_loss:1.3483302593231201 val_loss1.3324259519577026\n",
            "iteration 542 :train_loss:1.3483299016952515 val_loss1.3324259519577026\n",
            "iteration 543 :train_loss:1.3483302593231201 val_loss1.3324259519577026\n",
            "iteration 544 :train_loss:1.3483302593231201 val_loss1.3324260711669922\n",
            "iteration 545 :train_loss:1.3483302593231201 val_loss1.3324259519577026\n",
            "iteration 546 :train_loss:1.348329782485962 val_loss1.3324259519577026\n",
            "iteration 547 :train_loss:1.3483302593231201 val_loss1.3324259519577026\n",
            "iteration 548 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 549 :train_loss:1.3483299016952515 val_loss1.3324259519577026\n",
            "iteration 550 :train_loss:1.348329782485962 val_loss1.3324259519577026\n",
            "iteration 551 :train_loss:1.348329782485962 val_loss1.3324259519577026\n",
            "iteration 552 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 553 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 554 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 555 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 556 :train_loss:1.3483296632766724 val_loss1.3324259519577026\n",
            "iteration 557 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 558 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 559 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 560 :train_loss:1.348329782485962 val_loss1.3324259519577026\n",
            "iteration 561 :train_loss:1.3483295440673828 val_loss1.3324257135391235\n",
            "iteration 562 :train_loss:1.3483295440673828 val_loss1.3324257135391235\n",
            "iteration 563 :train_loss:1.3483295440673828 val_loss1.3324257135391235\n",
            "iteration 564 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 565 :train_loss:1.348329782485962 val_loss1.332425594329834\n",
            "iteration 566 :train_loss:1.3483295440673828 val_loss1.3324259519577026\n",
            "iteration 567 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 568 :train_loss:1.3483295440673828 val_loss1.3324257135391235\n",
            "iteration 569 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 570 :train_loss:1.3483295440673828 val_loss1.3324257135391235\n",
            "iteration 571 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 572 :train_loss:1.3483293056488037 val_loss1.3324257135391235\n",
            "iteration 573 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 574 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 575 :train_loss:1.3483295440673828 val_loss1.332425594329834\n",
            "iteration 576 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 577 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 578 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 579 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 580 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 581 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 582 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 583 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 584 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 585 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 586 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 587 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 588 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 589 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 590 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 591 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 592 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 593 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 594 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 595 :train_loss:1.3483293056488037 val_loss1.3324254751205444\n",
            "iteration 596 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 597 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 598 :train_loss:1.3483293056488037 val_loss1.3324254751205444\n",
            "iteration 599 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 600 :train_loss:1.3483293056488037 val_loss1.3324254751205444\n",
            "iteration 601 :train_loss:1.3483291864395142 val_loss1.332425594329834\n",
            "iteration 602 :train_loss:1.3483293056488037 val_loss1.3324254751205444\n",
            "iteration 603 :train_loss:1.3483293056488037 val_loss1.332425594329834\n",
            "iteration 604 :train_loss:1.3483293056488037 val_loss1.3324254751205444\n",
            "iteration 605 :train_loss:1.3483293056488037 val_loss1.3324253559112549\n",
            "iteration 606 :train_loss:1.3483291864395142 val_loss1.3324254751205444\n",
            "iteration 607 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 608 :train_loss:1.3483293056488037 val_loss1.3324253559112549\n",
            "iteration 609 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 610 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 611 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 612 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 613 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 614 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 615 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 616 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 617 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 618 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 619 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 620 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 621 :train_loss:1.3483291864395142 val_loss1.3324254751205444\n",
            "iteration 622 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 623 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 624 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 625 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 626 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 627 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 628 :train_loss:1.3483291864395142 val_loss1.3324252367019653\n",
            "iteration 629 :train_loss:1.3483291864395142 val_loss1.3324249982833862\n",
            "iteration 630 :train_loss:1.3483291864395142 val_loss1.3324252367019653\n",
            "iteration 631 :train_loss:1.3483291864395142 val_loss1.3324252367019653\n",
            "iteration 632 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 633 :train_loss:1.3483291864395142 val_loss1.3324249982833862\n",
            "iteration 634 :train_loss:1.3483290672302246 val_loss1.3324252367019653\n",
            "iteration 635 :train_loss:1.3483291864395142 val_loss1.3324249982833862\n",
            "iteration 636 :train_loss:1.3483291864395142 val_loss1.3324249982833862\n",
            "iteration 637 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 638 :train_loss:1.3483291864395142 val_loss1.3324249982833862\n",
            "iteration 639 :train_loss:1.3483291864395142 val_loss1.3324252367019653\n",
            "iteration 640 :train_loss:1.3483291864395142 val_loss1.3324249982833862\n",
            "iteration 641 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 642 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 643 :train_loss:1.3483291864395142 val_loss1.3324252367019653\n",
            "iteration 644 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 645 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 646 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 647 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 648 :train_loss:1.3483290672302246 val_loss1.3324249982833862\n",
            "iteration 649 :train_loss:1.3483291864395142 val_loss1.3324253559112549\n",
            "iteration 650 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 651 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 652 :train_loss:1.3483290672302246 val_loss1.3324249982833862\n",
            "iteration 653 :train_loss:1.3483290672302246 val_loss1.3324249982833862\n",
            "iteration 654 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 655 :train_loss:1.348328948020935 val_loss1.3324253559112549\n",
            "iteration 656 :train_loss:1.3483290672302246 val_loss1.3324252367019653\n",
            "iteration 657 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 658 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 659 :train_loss:1.3483290672302246 val_loss1.3324253559112549\n",
            "iteration 660 :train_loss:1.348328948020935 val_loss1.3324252367019653\n",
            "iteration 661 :train_loss:1.348328948020935 val_loss1.3324253559112549\n",
            "iteration 662 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 663 :train_loss:1.3483290672302246 val_loss1.3324249982833862\n",
            "iteration 664 :train_loss:1.3483290672302246 val_loss1.3324249982833862\n",
            "iteration 665 :train_loss:1.3483290672302246 val_loss1.3324249982833862\n",
            "iteration 666 :train_loss:1.348328948020935 val_loss1.3324248790740967\n",
            "iteration 667 :train_loss:1.348328948020935 val_loss1.3324248790740967\n",
            "iteration 668 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 669 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 670 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 671 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 672 :train_loss:1.348328948020935 val_loss1.3324252367019653\n",
            "iteration 673 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 674 :train_loss:1.348328948020935 val_loss1.3324252367019653\n",
            "iteration 675 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 676 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 677 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 678 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 679 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 680 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 681 :train_loss:1.348328948020935 val_loss1.3324247598648071\n",
            "iteration 682 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 683 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 684 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 685 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 686 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 687 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 688 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 689 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 690 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 691 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 692 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 693 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 694 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 695 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 696 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 697 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 698 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 699 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 700 :train_loss:1.348328948020935 val_loss1.3324249982833862\n",
            "iteration 701 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 702 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 703 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 704 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 705 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 706 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 707 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 708 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 709 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 710 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 711 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 712 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 713 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 714 :train_loss:1.3483288288116455 val_loss1.3324249982833862\n",
            "iteration 715 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 716 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 717 :train_loss:1.3483285903930664 val_loss1.3324248790740967\n",
            "iteration 718 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 719 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 720 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 721 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 722 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 723 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 724 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 725 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 726 :train_loss:1.3483288288116455 val_loss1.3324248790740967\n",
            "iteration 727 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 728 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 729 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 730 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 731 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 732 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 733 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 734 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 735 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 736 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 737 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 738 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 739 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 740 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 741 :train_loss:1.3483288288116455 val_loss1.3324247598648071\n",
            "iteration 742 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 743 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 744 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 745 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 746 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 747 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 748 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 749 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 750 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 751 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 752 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 753 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 754 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 755 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 756 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 757 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 758 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 759 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 760 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 761 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 762 :train_loss:1.3483285903930664 val_loss1.3324244022369385\n",
            "iteration 763 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 764 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 765 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 766 :train_loss:1.3483284711837769 val_loss1.3324247598648071\n",
            "iteration 767 :train_loss:1.3483284711837769 val_loss1.3324247598648071\n",
            "iteration 768 :train_loss:1.3483285903930664 val_loss1.3324246406555176\n",
            "iteration 769 :train_loss:1.3483284711837769 val_loss1.3324246406555176\n",
            "iteration 770 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 771 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 772 :train_loss:1.3483285903930664 val_loss1.3324244022369385\n",
            "iteration 773 :train_loss:1.3483285903930664 val_loss1.3324247598648071\n",
            "iteration 774 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 775 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 776 :train_loss:1.3483285903930664 val_loss1.3324246406555176\n",
            "iteration 777 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 778 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 779 :train_loss:1.3483285903930664 val_loss1.3324244022369385\n",
            "iteration 780 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 781 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 782 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 783 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 784 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 785 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 786 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 787 :train_loss:1.3483284711837769 val_loss1.332424283027649\n",
            "iteration 788 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 789 :train_loss:1.3483283519744873 val_loss1.3324244022369385\n",
            "iteration 790 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 791 :train_loss:1.3483282327651978 val_loss1.3324244022369385\n",
            "iteration 792 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 793 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 794 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 795 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 796 :train_loss:1.3483283519744873 val_loss1.3324244022369385\n",
            "iteration 797 :train_loss:1.3483283519744873 val_loss1.3324244022369385\n",
            "iteration 798 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 799 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 800 :train_loss:1.3483285903930664 val_loss1.3324244022369385\n",
            "iteration 801 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 802 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 803 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 804 :train_loss:1.3483285903930664 val_loss1.3324244022369385\n",
            "iteration 805 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 806 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 807 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 808 :train_loss:1.3483283519744873 val_loss1.3324244022369385\n",
            "iteration 809 :train_loss:1.3483284711837769 val_loss1.3324244022369385\n",
            "iteration 810 :train_loss:1.3483283519744873 val_loss1.3324244022369385\n",
            "iteration 811 :train_loss:1.3483283519744873 val_loss1.3324241638183594\n",
            "iteration 812 :train_loss:1.3483284711837769 val_loss1.332424283027649\n",
            "iteration 813 :train_loss:1.3483284711837769 val_loss1.332424283027649\n",
            "iteration 814 :train_loss:1.3483284711837769 val_loss1.3324241638183594\n",
            "iteration 815 :train_loss:1.3483284711837769 val_loss1.332424283027649\n",
            "iteration 816 :train_loss:1.3483283519744873 val_loss1.3324244022369385\n",
            "iteration 817 :train_loss:1.3483281135559082 val_loss1.3324244022369385\n",
            "iteration 818 :train_loss:1.3483283519744873 val_loss1.332424283027649\n",
            "iteration 819 :train_loss:1.3483283519744873 val_loss1.332424283027649\n",
            "iteration 820 :train_loss:1.3483281135559082 val_loss1.332424283027649\n",
            "iteration 821 :train_loss:1.3483281135559082 val_loss1.3324244022369385\n",
            "iteration 822 :train_loss:1.3483281135559082 val_loss1.3324244022369385\n",
            "iteration 823 :train_loss:1.3483281135559082 val_loss1.332424283027649\n",
            "iteration 824 :train_loss:1.3483281135559082 val_loss1.332424283027649\n",
            "iteration 825 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 826 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 827 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 828 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 829 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 830 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 831 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 832 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 833 :train_loss:1.3483281135559082 val_loss1.3324244022369385\n",
            "iteration 834 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 835 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 836 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 837 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 838 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 839 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 840 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 841 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 842 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 843 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 844 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 845 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 846 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 847 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 848 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 849 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 850 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 851 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 852 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 853 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 854 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 855 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 856 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 857 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 858 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 859 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 860 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 861 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 862 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 863 :train_loss:1.3483281135559082 val_loss1.3324240446090698\n",
            "iteration 864 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 865 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 866 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 867 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 868 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 869 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 870 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 871 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 872 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 873 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 874 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 875 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 876 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 877 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 878 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 879 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 880 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 881 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 882 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 883 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 884 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 885 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 886 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 887 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 888 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 889 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 890 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 891 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 892 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 893 :train_loss:1.3483281135559082 val_loss1.3324240446090698\n",
            "iteration 894 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 895 :train_loss:1.3483281135559082 val_loss1.3324239253997803\n",
            "iteration 896 :train_loss:1.3483281135559082 val_loss1.3324241638183594\n",
            "iteration 897 :train_loss:1.348327875137329 val_loss1.3324240446090698\n",
            "iteration 898 :train_loss:1.348327875137329 val_loss1.3324240446090698\n",
            "iteration 899 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 900 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 901 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 902 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 903 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 904 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 905 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 906 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 907 :train_loss:1.348327875137329 val_loss1.3324240446090698\n",
            "iteration 908 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 909 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 910 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 911 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 912 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 913 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 914 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 915 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 916 :train_loss:1.348327875137329 val_loss1.3324241638183594\n",
            "iteration 917 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 918 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 919 :train_loss:1.3483277559280396 val_loss1.3324240446090698\n",
            "iteration 920 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 921 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 922 :train_loss:1.348327875137329 val_loss1.3324240446090698\n",
            "iteration 923 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 924 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 925 :train_loss:1.3483277559280396 val_loss1.3324241638183594\n",
            "iteration 926 :train_loss:1.348327875137329 val_loss1.3324240446090698\n",
            "iteration 927 :train_loss:1.3483277559280396 val_loss1.3324240446090698\n",
            "iteration 928 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 929 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 930 :train_loss:1.3483277559280396 val_loss1.3324240446090698\n",
            "iteration 931 :train_loss:1.3483277559280396 val_loss1.3324240446090698\n",
            "iteration 932 :train_loss:1.3483277559280396 val_loss1.3324241638183594\n",
            "iteration 933 :train_loss:1.3483277559280396 val_loss1.3324240446090698\n",
            "iteration 934 :train_loss:1.3483277559280396 val_loss1.3324241638183594\n",
            "iteration 935 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 936 :train_loss:1.3483277559280396 val_loss1.3324241638183594\n",
            "iteration 937 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 938 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 939 :train_loss:1.3483277559280396 val_loss1.3324240446090698\n",
            "iteration 940 :train_loss:1.348327875137329 val_loss1.3324239253997803\n",
            "iteration 941 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 942 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 943 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 944 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 945 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 946 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 947 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 948 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 949 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 950 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 951 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 952 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 953 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 954 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 955 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 956 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 957 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 958 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 959 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 960 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 961 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 962 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 963 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 964 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 965 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 966 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 967 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 968 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 969 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 970 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 971 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 972 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 973 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 974 :train_loss:1.3483277559280396 val_loss1.3324236869812012\n",
            "iteration 975 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 976 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 977 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 978 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 979 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 980 :train_loss:1.3483277559280396 val_loss1.3324236869812012\n",
            "iteration 981 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 982 :train_loss:1.3483277559280396 val_loss1.3324236869812012\n",
            "iteration 983 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 984 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 985 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 986 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 987 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 988 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 989 :train_loss:1.3483277559280396 val_loss1.3324236869812012\n",
            "iteration 990 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 991 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 992 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 993 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 994 :train_loss:1.3483277559280396 val_loss1.3324235677719116\n",
            "iteration 995 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 996 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n",
            "iteration 997 :train_loss:1.3483277559280396 val_loss1.3324235677719116\n",
            "iteration 998 :train_loss:1.3483277559280396 val_loss1.3324235677719116\n",
            "iteration 999 :train_loss:1.3483277559280396 val_loss1.3324239253997803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdqCpgZ1digm",
        "outputId": "8a309ec6-cbed-414d-854e-6ccae514dbee"
      },
      "source": [
        "predY=predict(parameters, test_X.T)\n",
        "print(\"Pred_Y:\",predY)\n",
        "MAPE = MAPE(test_Y,predY)\n",
        "print(\"MAPE:\",MAPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2.0708938 2.0723407 2.0717793 ... 2.073682  2.0747945 2.0727477]], shape=(1, 3000), dtype=float32)\n",
            "Pred_Y: tf.Tensor([[2.0708938 2.0723407 2.0717793 ... 2.073682  2.0747945 2.0727477]], shape=(1, 3000), dtype=float32)\n",
            "MAPE: tf.Tensor(60.65116, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "42RdxZ9OgHj1",
        "outputId": "08162f05-cf35-4338-eba5-342c33174233"
      },
      "source": [
        "plotLearningCurve()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dmgJhJdgU8z",
        "outputId": "c2053267-3ce5-49db-c171-3d4775cee470"
      },
      "source": [
        "iterations = 500\n",
        "parameters, history = create_nn_model(train_X.T, train_Y.T, 100, val_X.T, val_Y.T, iterations, 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 :train_loss:5.644540309906006 val_loss5.630524635314941\n",
            "iteration 1 :train_loss:4.090591907501221 val_loss4.076151371002197\n",
            "iteration 2 :train_loss:3.0976579189300537 val_loss3.082883834838867\n",
            "iteration 3 :train_loss:2.462721824645996 val_loss2.447685480117798\n",
            "iteration 4 :train_loss:2.0569090843200684 val_loss2.0416698455810547\n",
            "iteration 5 :train_loss:1.7978824377059937 val_loss1.7824856042861938\n",
            "iteration 6 :train_loss:1.6328719854354858 val_loss1.6173566579818726\n",
            "iteration 7 :train_loss:1.5280131101608276 val_loss1.5124086141586304\n",
            "iteration 8 :train_loss:1.4615623950958252 val_loss1.4458918571472168\n",
            "iteration 9 :train_loss:1.419572353363037 val_loss1.4038522243499756\n",
            "iteration 10 :train_loss:1.393110752105713 val_loss1.3773548603057861\n",
            "iteration 11 :train_loss:1.376478672027588 val_loss1.3606953620910645\n",
            "iteration 12 :train_loss:1.3660470247268677 val_loss1.3502435684204102\n",
            "iteration 13 :train_loss:1.3595166206359863 val_loss1.343697428703308\n",
            "iteration 14 :train_loss:1.3554332256317139 val_loss1.3396024703979492\n",
            "iteration 15 :train_loss:1.3528826236724854 val_loss1.3370426893234253\n",
            "iteration 16 :train_loss:1.3512896299362183 val_loss1.335442066192627\n",
            "iteration 17 :train_loss:1.3502943515777588 val_loss1.334441065788269\n",
            "iteration 18 :train_loss:1.3496713638305664 val_loss1.3338134288787842\n",
            "iteration 19 :train_loss:1.3492804765701294 val_loss1.3334184885025024\n",
            "iteration 20 :train_loss:1.349034070968628 val_loss1.333168625831604\n",
            "iteration 21 :train_loss:1.3488777875900269 val_loss1.3330098390579224\n",
            "iteration 22 :train_loss:1.348777413368225 val_loss1.332906723022461\n",
            "iteration 23 :train_loss:1.348711609840393 val_loss1.3328396081924438\n",
            "iteration 24 :train_loss:1.3486679792404175 val_loss1.3327943086624146\n",
            "iteration 25 :train_loss:1.3486382961273193 val_loss1.3327629566192627\n",
            "iteration 26 :train_loss:1.3486164808273315 val_loss1.3327404260635376\n",
            "iteration 27 :train_loss:1.348601222038269 val_loss1.3327232599258423\n",
            "iteration 28 :train_loss:1.348588466644287 val_loss1.3327099084854126\n",
            "iteration 29 :train_loss:1.3485783338546753 val_loss1.3326987028121948\n",
            "iteration 30 :train_loss:1.3485695123672485 val_loss1.3326894044876099\n",
            "iteration 31 :train_loss:1.348562479019165 val_loss1.3326811790466309\n",
            "iteration 32 :train_loss:1.3485552072525024 val_loss1.3326737880706787\n",
            "iteration 33 :train_loss:1.3485491275787354 val_loss1.3326668739318848\n",
            "iteration 34 :train_loss:1.348543643951416 val_loss1.3326603174209595\n",
            "iteration 35 :train_loss:1.3485381603240967 val_loss1.3326542377471924\n",
            "iteration 36 :train_loss:1.348533034324646 val_loss1.3326483964920044\n",
            "iteration 37 :train_loss:1.3485270738601685 val_loss1.3326430320739746\n",
            "iteration 38 :train_loss:1.3485227823257446 val_loss1.3326373100280762\n",
            "iteration 39 :train_loss:1.3485183715820312 val_loss1.3326326608657837\n",
            "iteration 40 :train_loss:1.3485133647918701 val_loss1.332627773284912\n",
            "iteration 41 :train_loss:1.3485091924667358 val_loss1.3326232433319092\n",
            "iteration 42 :train_loss:1.3485058546066284 val_loss1.3326185941696167\n",
            "iteration 43 :train_loss:1.348501205444336 val_loss1.3326143026351929\n",
            "iteration 44 :train_loss:1.348497986793518 val_loss1.3326102495193481\n",
            "iteration 45 :train_loss:1.3484944105148315 val_loss1.3326060771942139\n",
            "iteration 46 :train_loss:1.3484904766082764 val_loss1.3326025009155273\n",
            "iteration 47 :train_loss:1.348487138748169 val_loss1.3325988054275513\n",
            "iteration 48 :train_loss:1.3484842777252197 val_loss1.3325953483581543\n",
            "iteration 49 :train_loss:1.3484808206558228 val_loss1.3325918912887573\n",
            "iteration 50 :train_loss:1.3484779596328735 val_loss1.3325884342193604\n",
            "iteration 51 :train_loss:1.3484750986099243 val_loss1.332585334777832\n",
            "iteration 52 :train_loss:1.348472237586975 val_loss1.3325821161270142\n",
            "iteration 53 :train_loss:1.3484692573547363 val_loss1.3325791358947754\n",
            "iteration 54 :train_loss:1.3484665155410767 val_loss1.3325762748718262\n",
            "iteration 55 :train_loss:1.348464012145996 val_loss1.332573413848877\n",
            "iteration 56 :train_loss:1.348461627960205 val_loss1.3325706720352173\n",
            "iteration 57 :train_loss:1.3484593629837036 val_loss1.3325679302215576\n",
            "iteration 58 :train_loss:1.3484567403793335 val_loss1.3325656652450562\n",
            "iteration 59 :train_loss:1.3484545946121216 val_loss1.3325629234313965\n",
            "iteration 60 :train_loss:1.3484524488449097 val_loss1.332560658454895\n",
            "iteration 61 :train_loss:1.3484504222869873 val_loss1.3325581550598145\n",
            "iteration 62 :train_loss:1.3484481573104858 val_loss1.3325560092926025\n",
            "iteration 63 :train_loss:1.3484461307525635 val_loss1.3325538635253906\n",
            "iteration 64 :train_loss:1.3484445810317993 val_loss1.3325518369674683\n",
            "iteration 65 :train_loss:1.3484421968460083 val_loss1.3325496912002563\n",
            "iteration 66 :train_loss:1.3484406471252441 val_loss1.332547664642334\n",
            "iteration 67 :train_loss:1.3484387397766113 val_loss1.3325457572937012\n",
            "iteration 68 :train_loss:1.348436951637268 val_loss1.332543969154358\n",
            "iteration 69 :train_loss:1.3484355211257935 val_loss1.3325421810150146\n",
            "iteration 70 :train_loss:1.348433494567871 val_loss1.3325401544570923\n",
            "iteration 71 :train_loss:1.3484323024749756 val_loss1.3325384855270386\n",
            "iteration 72 :train_loss:1.3484306335449219 val_loss1.332537055015564\n",
            "iteration 73 :train_loss:1.3484292030334473 val_loss1.3325351476669312\n",
            "iteration 74 :train_loss:1.3484278917312622 val_loss1.3325337171554565\n",
            "iteration 75 :train_loss:1.3484265804290771 val_loss1.3325321674346924\n",
            "iteration 76 :train_loss:1.3484253883361816 val_loss1.3325308561325073\n",
            "iteration 77 :train_loss:1.348423957824707 val_loss1.3325293064117432\n",
            "iteration 78 :train_loss:1.3484227657318115 val_loss1.332527995109558\n",
            "iteration 79 :train_loss:1.3484210968017578 val_loss1.3325265645980835\n",
            "iteration 80 :train_loss:1.348420262336731 val_loss1.3325251340866089\n",
            "iteration 81 :train_loss:1.3484190702438354 val_loss1.3325238227844238\n",
            "iteration 82 :train_loss:1.3484176397323608 val_loss1.3325225114822388\n",
            "iteration 83 :train_loss:1.3484165668487549 val_loss1.3325212001800537\n",
            "iteration 84 :train_loss:1.3484158515930176 val_loss1.3325202465057373\n",
            "iteration 85 :train_loss:1.348414421081543 val_loss1.3325189352035522\n",
            "iteration 86 :train_loss:1.3484137058258057 val_loss1.3325176239013672\n",
            "iteration 87 :train_loss:1.3484129905700684 val_loss1.3325166702270508\n",
            "iteration 88 :train_loss:1.3484116792678833 val_loss1.3325154781341553\n",
            "iteration 89 :train_loss:1.3484108448028564 val_loss1.3325146436691284\n",
            "iteration 90 :train_loss:1.3484097719192505 val_loss1.332513451576233\n",
            "iteration 91 :train_loss:1.3484083414077759 val_loss1.332512378692627\n",
            "iteration 92 :train_loss:1.3484078645706177 val_loss1.332511305809021\n",
            "iteration 93 :train_loss:1.3484070301055908 val_loss1.3325105905532837\n",
            "iteration 94 :train_loss:1.348406195640564 val_loss1.3325097560882568\n",
            "iteration 95 :train_loss:1.3484050035476685 val_loss1.3325088024139404\n",
            "iteration 96 :train_loss:1.3484042882919312 val_loss1.332507848739624\n",
            "iteration 97 :train_loss:1.3484035730361938 val_loss1.3325068950653076\n",
            "iteration 98 :train_loss:1.348402976989746 val_loss1.3325060606002808\n",
            "iteration 99 :train_loss:1.3484015464782715 val_loss1.332505226135254\n",
            "iteration 100 :train_loss:1.3484013080596924 val_loss1.3325045108795166\n",
            "iteration 101 :train_loss:1.3484007120132446 val_loss1.3325035572052002\n",
            "iteration 102 :train_loss:1.3484001159667969 val_loss1.332502841949463\n",
            "iteration 103 :train_loss:1.3483991622924805 val_loss1.332502007484436\n",
            "iteration 104 :train_loss:1.3483986854553223 val_loss1.3325011730194092\n",
            "iteration 105 :train_loss:1.348398208618164 val_loss1.3325005769729614\n",
            "iteration 106 :train_loss:1.3483972549438477 val_loss1.3324997425079346\n",
            "iteration 107 :train_loss:1.3483966588974 val_loss1.3324991464614868\n",
            "iteration 108 :train_loss:1.348395586013794 val_loss1.3324984312057495\n",
            "iteration 109 :train_loss:1.3483951091766357 val_loss1.3324978351593018\n",
            "iteration 110 :train_loss:1.348394751548767 val_loss1.3324971199035645\n",
            "iteration 111 :train_loss:1.3483943939208984 val_loss1.3324964046478271\n",
            "iteration 112 :train_loss:1.3483941555023193 val_loss1.3324958086013794\n",
            "iteration 113 :train_loss:1.348393440246582 val_loss1.3324952125549316\n",
            "iteration 114 :train_loss:1.348392367362976 val_loss1.3324947357177734\n",
            "iteration 115 :train_loss:1.348392128944397 val_loss1.3324940204620361\n",
            "iteration 116 :train_loss:1.3483915328979492 val_loss1.3324934244155884\n",
            "iteration 117 :train_loss:1.348391056060791 val_loss1.3324928283691406\n",
            "iteration 118 :train_loss:1.3483903408050537 val_loss1.3324923515319824\n",
            "iteration 119 :train_loss:1.3483901023864746 val_loss1.3324917554855347\n",
            "iteration 120 :train_loss:1.3483898639678955 val_loss1.3324912786483765\n",
            "iteration 121 :train_loss:1.3483887910842896 val_loss1.3324908018112183\n",
            "iteration 122 :train_loss:1.348388433456421 val_loss1.3324902057647705\n",
            "iteration 123 :train_loss:1.3483880758285522 val_loss1.3324898481369019\n",
            "iteration 124 :train_loss:1.3483878374099731 val_loss1.3324891328811646\n",
            "iteration 125 :train_loss:1.3483874797821045 val_loss1.3324886560440063\n",
            "iteration 126 :train_loss:1.3483871221542358 val_loss1.3324881792068481\n",
            "iteration 127 :train_loss:1.3483866453170776 val_loss1.33248770236969\n",
            "iteration 128 :train_loss:1.3483858108520508 val_loss1.3324873447418213\n",
            "iteration 129 :train_loss:1.3483854532241821 val_loss1.3324867486953735\n",
            "iteration 130 :train_loss:1.348385214805603 val_loss1.3324863910675049\n",
            "iteration 131 :train_loss:1.348384976387024 val_loss1.3324859142303467\n",
            "iteration 132 :train_loss:1.3483846187591553 val_loss1.3324854373931885\n",
            "iteration 133 :train_loss:1.348383903503418 val_loss1.3324849605560303\n",
            "iteration 134 :train_loss:1.3483835458755493 val_loss1.3324846029281616\n",
            "iteration 135 :train_loss:1.3483831882476807 val_loss1.332484245300293\n",
            "iteration 136 :train_loss:1.3483830690383911 val_loss1.3324840068817139\n",
            "iteration 137 :train_loss:1.3483821153640747 val_loss1.3324836492538452\n",
            "iteration 138 :train_loss:1.348381757736206 val_loss1.332483172416687\n",
            "iteration 139 :train_loss:1.3483816385269165 val_loss1.3324828147888184\n",
            "iteration 140 :train_loss:1.3483811616897583 val_loss1.3324824571609497\n",
            "iteration 141 :train_loss:1.3483811616897583 val_loss1.332481861114502\n",
            "iteration 142 :train_loss:1.3483809232711792 val_loss1.3324817419052124\n",
            "iteration 143 :train_loss:1.3483808040618896 val_loss1.3324815034866333\n",
            "iteration 144 :train_loss:1.348380446434021 val_loss1.3324809074401855\n",
            "iteration 145 :train_loss:1.3483803272247314 val_loss1.3324806690216064\n",
            "iteration 146 :train_loss:1.3483800888061523 val_loss1.3324801921844482\n",
            "iteration 147 :train_loss:1.3483796119689941 val_loss1.3324800729751587\n",
            "iteration 148 :train_loss:1.348379135131836 val_loss1.3324795961380005\n",
            "iteration 149 :train_loss:1.3483787775039673 val_loss1.3324793577194214\n",
            "iteration 150 :train_loss:1.3483786582946777 val_loss1.3324787616729736\n",
            "iteration 151 :train_loss:1.3483781814575195 val_loss1.332478404045105\n",
            "iteration 152 :train_loss:1.34837806224823 val_loss1.3324781656265259\n",
            "iteration 153 :train_loss:1.3483777046203613 val_loss1.3324779272079468\n",
            "iteration 154 :train_loss:1.3483773469924927 val_loss1.3324774503707886\n",
            "iteration 155 :train_loss:1.348376989364624 val_loss1.332477331161499\n",
            "iteration 156 :train_loss:1.3483768701553345 val_loss1.3324769735336304\n",
            "iteration 157 :train_loss:1.3483766317367554 val_loss1.3324767351150513\n",
            "iteration 158 :train_loss:1.3483765125274658 val_loss1.3324764966964722\n",
            "iteration 159 :train_loss:1.3483762741088867 val_loss1.3324761390686035\n",
            "iteration 160 :train_loss:1.3483754396438599 val_loss1.3324759006500244\n",
            "iteration 161 :train_loss:1.3483753204345703 val_loss1.3324755430221558\n",
            "iteration 162 :train_loss:1.3483752012252808 val_loss1.3324753046035767\n",
            "iteration 163 :train_loss:1.348374843597412 val_loss1.3324750661849976\n",
            "iteration 164 :train_loss:1.348374605178833 val_loss1.332474708557129\n",
            "iteration 165 :train_loss:1.348374605178833 val_loss1.3324744701385498\n",
            "iteration 166 :train_loss:1.348374366760254 val_loss1.3324741125106812\n",
            "iteration 167 :train_loss:1.3483740091323853 val_loss1.332473874092102\n",
            "iteration 168 :train_loss:1.3483740091323853 val_loss1.332473874092102\n",
            "iteration 169 :train_loss:1.3483738899230957 val_loss1.3324735164642334\n",
            "iteration 170 :train_loss:1.3483737707138062 val_loss1.3324732780456543\n",
            "iteration 171 :train_loss:1.3483734130859375 val_loss1.332472801208496\n",
            "iteration 172 :train_loss:1.348373293876648 val_loss1.3324726819992065\n",
            "iteration 173 :train_loss:1.3483731746673584 val_loss1.3324724435806274\n",
            "iteration 174 :train_loss:1.3483728170394897 val_loss1.3324720859527588\n",
            "iteration 175 :train_loss:1.3483721017837524 val_loss1.3324719667434692\n",
            "iteration 176 :train_loss:1.348371982574463 val_loss1.3324718475341797\n",
            "iteration 177 :train_loss:1.3483718633651733 val_loss1.3324716091156006\n",
            "iteration 178 :train_loss:1.3483717441558838 val_loss1.3324713706970215\n",
            "iteration 179 :train_loss:1.3483713865280151 val_loss1.3324710130691528\n",
            "iteration 180 :train_loss:1.3483713865280151 val_loss1.3324710130691528\n",
            "iteration 181 :train_loss:1.3483712673187256 val_loss1.3324706554412842\n",
            "iteration 182 :train_loss:1.3483710289001465 val_loss1.332470417022705\n",
            "iteration 183 :train_loss:1.3483710289001465 val_loss1.332470178604126\n",
            "iteration 184 :train_loss:1.3483704328536987 val_loss1.332470178604126\n",
            "iteration 185 :train_loss:1.3483704328536987 val_loss1.3324698209762573\n",
            "iteration 186 :train_loss:1.3483703136444092 val_loss1.3324698209762573\n",
            "iteration 187 :train_loss:1.3483701944351196 val_loss1.3324695825576782\n",
            "iteration 188 :train_loss:1.348369836807251 val_loss1.3324692249298096\n",
            "iteration 189 :train_loss:1.3483697175979614 val_loss1.33246910572052\n",
            "iteration 190 :train_loss:1.3483697175979614 val_loss1.3324689865112305\n",
            "iteration 191 :train_loss:1.3483695983886719 val_loss1.3324687480926514\n",
            "iteration 192 :train_loss:1.3483694791793823 val_loss1.3324687480926514\n",
            "iteration 193 :train_loss:1.3483691215515137 val_loss1.3324683904647827\n",
            "iteration 194 :train_loss:1.348368525505066 val_loss1.3324683904647827\n",
            "iteration 195 :train_loss:1.348368525505066 val_loss1.3324681520462036\n",
            "iteration 196 :train_loss:1.3483682870864868 val_loss1.332467794418335\n",
            "iteration 197 :train_loss:1.3483682870864868 val_loss1.3324676752090454\n",
            "iteration 198 :train_loss:1.3483680486679077 val_loss1.3324675559997559\n",
            "iteration 199 :train_loss:1.3483678102493286 val_loss1.3324671983718872\n",
            "iteration 200 :train_loss:1.3483678102493286 val_loss1.332466959953308\n",
            "iteration 201 :train_loss:1.348367691040039 val_loss1.332466721534729\n",
            "iteration 202 :train_loss:1.3483675718307495 val_loss1.332466721534729\n",
            "iteration 203 :train_loss:1.34836745262146 val_loss1.3324663639068604\n",
            "iteration 204 :train_loss:1.34836745262146 val_loss1.3324663639068604\n",
            "iteration 205 :train_loss:1.3483672142028809 val_loss1.3324663639068604\n",
            "iteration 206 :train_loss:1.3483670949935913 val_loss1.3324661254882812\n",
            "iteration 207 :train_loss:1.3483669757843018 val_loss1.3324657678604126\n",
            "iteration 208 :train_loss:1.3483668565750122 val_loss1.3324657678604126\n",
            "iteration 209 :train_loss:1.3483667373657227 val_loss1.3324655294418335\n",
            "iteration 210 :train_loss:1.3483667373657227 val_loss1.3324652910232544\n",
            "iteration 211 :train_loss:1.3483664989471436 val_loss1.3324652910232544\n",
            "iteration 212 :train_loss:1.348366379737854 val_loss1.3324650526046753\n",
            "iteration 213 :train_loss:1.3483662605285645 val_loss1.3324649333953857\n",
            "iteration 214 :train_loss:1.3483657836914062 val_loss1.3324646949768066\n",
            "iteration 215 :train_loss:1.3483654260635376 val_loss1.332464575767517\n",
            "iteration 216 :train_loss:1.348365306854248 val_loss1.332464337348938\n",
            "iteration 217 :train_loss:1.348365306854248 val_loss1.3324640989303589\n",
            "iteration 218 :train_loss:1.348365306854248 val_loss1.3324640989303589\n",
            "iteration 219 :train_loss:1.3483649492263794 val_loss1.3324638605117798\n",
            "iteration 220 :train_loss:1.3483649492263794 val_loss1.3324638605117798\n",
            "iteration 221 :train_loss:1.3483648300170898 val_loss1.3324635028839111\n",
            "iteration 222 :train_loss:1.3483647108078003 val_loss1.332463264465332\n",
            "iteration 223 :train_loss:1.3483645915985107 val_loss1.332463264465332\n",
            "iteration 224 :train_loss:1.3483647108078003 val_loss1.3324629068374634\n",
            "iteration 225 :train_loss:1.3483645915985107 val_loss1.3324627876281738\n",
            "iteration 226 :train_loss:1.3483645915985107 val_loss1.3324626684188843\n",
            "iteration 227 :train_loss:1.3483643531799316 val_loss1.3324626684188843\n",
            "iteration 228 :train_loss:1.3483638763427734 val_loss1.3324625492095947\n",
            "iteration 229 :train_loss:1.3483636379241943 val_loss1.3324624300003052\n",
            "iteration 230 :train_loss:1.3483638763427734 val_loss1.3324624300003052\n",
            "iteration 231 :train_loss:1.3483636379241943 val_loss1.3324620723724365\n",
            "iteration 232 :train_loss:1.3483635187149048 val_loss1.3324618339538574\n",
            "iteration 233 :train_loss:1.3483635187149048 val_loss1.3324618339538574\n",
            "iteration 234 :train_loss:1.3483633995056152 val_loss1.3324617147445679\n",
            "iteration 235 :train_loss:1.3483631610870361 val_loss1.3324614763259888\n",
            "iteration 236 :train_loss:1.3483631610870361 val_loss1.3324614763259888\n",
            "iteration 237 :train_loss:1.3483631610870361 val_loss1.3324612379074097\n",
            "iteration 238 :train_loss:1.3483628034591675 val_loss1.3324612379074097\n",
            "iteration 239 :train_loss:1.3483628034591675 val_loss1.3324612379074097\n",
            "iteration 240 :train_loss:1.3483628034591675 val_loss1.332460880279541\n",
            "iteration 241 :train_loss:1.3483624458312988 val_loss1.332460880279541\n",
            "iteration 242 :train_loss:1.3483619689941406 val_loss1.332460641860962\n",
            "iteration 243 :train_loss:1.3483619689941406 val_loss1.332460641860962\n",
            "iteration 244 :train_loss:1.348361849784851 val_loss1.332460641860962\n",
            "iteration 245 :train_loss:1.348361849784851 val_loss1.3324604034423828\n",
            "iteration 246 :train_loss:1.3483617305755615 val_loss1.3324604034423828\n",
            "iteration 247 :train_loss:1.3483617305755615 val_loss1.3324604034423828\n",
            "iteration 248 :train_loss:1.3483617305755615 val_loss1.3324600458145142\n",
            "iteration 249 :train_loss:1.3483613729476929 val_loss1.3324600458145142\n",
            "iteration 250 :train_loss:1.3483613729476929 val_loss1.332459807395935\n",
            "iteration 251 :train_loss:1.3483612537384033 val_loss1.332459807395935\n",
            "iteration 252 :train_loss:1.3483612537384033 val_loss1.332459807395935\n",
            "iteration 253 :train_loss:1.3483613729476929 val_loss1.3324596881866455\n",
            "iteration 254 :train_loss:1.3483611345291138 val_loss1.3324594497680664\n",
            "iteration 255 :train_loss:1.3483611345291138 val_loss1.3324594497680664\n",
            "iteration 256 :train_loss:1.3483608961105347 val_loss1.3324590921401978\n",
            "iteration 257 :train_loss:1.3483608961105347 val_loss1.3324590921401978\n",
            "iteration 258 :train_loss:1.3483607769012451 val_loss1.3324589729309082\n",
            "iteration 259 :train_loss:1.3483608961105347 val_loss1.332458734512329\n",
            "iteration 260 :train_loss:1.3483607769012451 val_loss1.3324586153030396\n",
            "iteration 261 :train_loss:1.3483607769012451 val_loss1.3324586153030396\n",
            "iteration 262 :train_loss:1.3483606576919556 val_loss1.3324583768844604\n",
            "iteration 263 :train_loss:1.348360538482666 val_loss1.3324583768844604\n",
            "iteration 264 :train_loss:1.348360538482666 val_loss1.3324580192565918\n",
            "iteration 265 :train_loss:1.348360538482666 val_loss1.3324580192565918\n",
            "iteration 266 :train_loss:1.3483604192733765 val_loss1.3324579000473022\n",
            "iteration 267 :train_loss:1.3483604192733765 val_loss1.3324577808380127\n",
            "iteration 268 :train_loss:1.3483601808547974 val_loss1.3324577808380127\n",
            "iteration 269 :train_loss:1.3483601808547974 val_loss1.3324576616287231\n",
            "iteration 270 :train_loss:1.3483601808547974 val_loss1.3324575424194336\n",
            "iteration 271 :train_loss:1.3483600616455078 val_loss1.3324575424194336\n",
            "iteration 272 :train_loss:1.3483599424362183 val_loss1.3324573040008545\n",
            "iteration 273 :train_loss:1.3483599424362183 val_loss1.3324573040008545\n",
            "iteration 274 :train_loss:1.348359227180481 val_loss1.3324570655822754\n",
            "iteration 275 :train_loss:1.3483591079711914 val_loss1.3324569463729858\n",
            "iteration 276 :train_loss:1.3483591079711914 val_loss1.3324569463729858\n",
            "iteration 277 :train_loss:1.3483587503433228 val_loss1.3324569463729858\n",
            "iteration 278 :train_loss:1.3483589887619019 val_loss1.3324565887451172\n",
            "iteration 279 :train_loss:1.3483586311340332 val_loss1.3324565887451172\n",
            "iteration 280 :train_loss:1.3483586311340332 val_loss1.3324565887451172\n",
            "iteration 281 :train_loss:1.3483585119247437 val_loss1.332456350326538\n",
            "iteration 282 :train_loss:1.3483585119247437 val_loss1.332456350326538\n",
            "iteration 283 :train_loss:1.3483582735061646 val_loss1.3324562311172485\n",
            "iteration 284 :train_loss:1.3483582735061646 val_loss1.332456111907959\n",
            "iteration 285 :train_loss:1.3483582735061646 val_loss1.332456111907959\n",
            "iteration 286 :train_loss:1.3483582735061646 val_loss1.332456111907959\n",
            "iteration 287 :train_loss:1.3483582735061646 val_loss1.332456111907959\n",
            "iteration 288 :train_loss:1.3483580350875854 val_loss1.3324558734893799\n",
            "iteration 289 :train_loss:1.348357915878296 val_loss1.3324557542800903\n",
            "iteration 290 :train_loss:1.348357915878296 val_loss1.3324557542800903\n",
            "iteration 291 :train_loss:1.3483576774597168 val_loss1.3324556350708008\n",
            "iteration 292 :train_loss:1.3483576774597168 val_loss1.3324555158615112\n",
            "iteration 293 :train_loss:1.3483573198318481 val_loss1.3324552774429321\n",
            "iteration 294 :train_loss:1.3483573198318481 val_loss1.3324551582336426\n",
            "iteration 295 :train_loss:1.3483573198318481 val_loss1.3324551582336426\n",
            "iteration 296 :train_loss:1.3483572006225586 val_loss1.3324551582336426\n",
            "iteration 297 :train_loss:1.348357081413269 val_loss1.3324549198150635\n",
            "iteration 298 :train_loss:1.348357081413269 val_loss1.3324549198150635\n",
            "iteration 299 :train_loss:1.348357081413269 val_loss1.3324549198150635\n",
            "iteration 300 :train_loss:1.3483569622039795 val_loss1.3324549198150635\n",
            "iteration 301 :train_loss:1.34835684299469 val_loss1.3324545621871948\n",
            "iteration 302 :train_loss:1.34835684299469 val_loss1.3324545621871948\n",
            "iteration 303 :train_loss:1.34835684299469 val_loss1.3324545621871948\n",
            "iteration 304 :train_loss:1.3483566045761108 val_loss1.3324545621871948\n",
            "iteration 305 :train_loss:1.3483566045761108 val_loss1.3324543237686157\n",
            "iteration 306 :train_loss:1.3483564853668213 val_loss1.3324543237686157\n",
            "iteration 307 :train_loss:1.3483563661575317 val_loss1.3324542045593262\n",
            "iteration 308 :train_loss:1.3483563661575317 val_loss1.3324542045593262\n",
            "iteration 309 :train_loss:1.3483563661575317 val_loss1.3324540853500366\n",
            "iteration 310 :train_loss:1.3483558893203735 val_loss1.3324538469314575\n",
            "iteration 311 :train_loss:1.3483555316925049 val_loss1.3324538469314575\n",
            "iteration 312 :train_loss:1.3483555316925049 val_loss1.332453727722168\n",
            "iteration 313 :train_loss:1.3483555316925049 val_loss1.332453727722168\n",
            "iteration 314 :train_loss:1.3483554124832153 val_loss1.332453727722168\n",
            "iteration 315 :train_loss:1.3483555316925049 val_loss1.3324536085128784\n",
            "iteration 316 :train_loss:1.3483554124832153 val_loss1.3324534893035889\n",
            "iteration 317 :train_loss:1.3483555316925049 val_loss1.3324534893035889\n",
            "iteration 318 :train_loss:1.3483554124832153 val_loss1.3324534893035889\n",
            "iteration 319 :train_loss:1.3483554124832153 val_loss1.3324533700942993\n",
            "iteration 320 :train_loss:1.3483551740646362 val_loss1.3324531316757202\n",
            "iteration 321 :train_loss:1.3483551740646362 val_loss1.3324531316757202\n",
            "iteration 322 :train_loss:1.3483550548553467 val_loss1.3324531316757202\n",
            "iteration 323 :train_loss:1.3483550548553467 val_loss1.3324531316757202\n",
            "iteration 324 :train_loss:1.3483550548553467 val_loss1.3324528932571411\n",
            "iteration 325 :train_loss:1.3483549356460571 val_loss1.3324528932571411\n",
            "iteration 326 :train_loss:1.3483549356460571 val_loss1.3324528932571411\n",
            "iteration 327 :train_loss:1.3483550548553467 val_loss1.332452654838562\n",
            "iteration 328 :train_loss:1.3483548164367676 val_loss1.332452654838562\n",
            "iteration 329 :train_loss:1.3483548164367676 val_loss1.332452654838562\n",
            "iteration 330 :train_loss:1.3483548164367676 val_loss1.332452416419983\n",
            "iteration 331 :train_loss:1.3483548164367676 val_loss1.3324522972106934\n",
            "iteration 332 :train_loss:1.3483548164367676 val_loss1.3324522972106934\n",
            "iteration 333 :train_loss:1.3483545780181885 val_loss1.3324522972106934\n",
            "iteration 334 :train_loss:1.348354458808899 val_loss1.3324520587921143\n",
            "iteration 335 :train_loss:1.348354458808899 val_loss1.3324520587921143\n",
            "iteration 336 :train_loss:1.348354458808899 val_loss1.3324517011642456\n",
            "iteration 337 :train_loss:1.348354458808899 val_loss1.3324517011642456\n",
            "iteration 338 :train_loss:1.348354458808899 val_loss1.3324517011642456\n",
            "iteration 339 :train_loss:1.3483543395996094 val_loss1.3324517011642456\n",
            "iteration 340 :train_loss:1.3483543395996094 val_loss1.3324514627456665\n",
            "iteration 341 :train_loss:1.3483542203903198 val_loss1.3324514627456665\n",
            "iteration 342 :train_loss:1.3483541011810303 val_loss1.3324514627456665\n",
            "iteration 343 :train_loss:1.3483541011810303 val_loss1.3324514627456665\n",
            "iteration 344 :train_loss:1.3483538627624512 val_loss1.3324514627456665\n",
            "iteration 345 :train_loss:1.3483537435531616 val_loss1.3324512243270874\n",
            "iteration 346 :train_loss:1.3483537435531616 val_loss1.3324512243270874\n",
            "iteration 347 :train_loss:1.3483537435531616 val_loss1.3324512243270874\n",
            "iteration 348 :train_loss:1.3483537435531616 val_loss1.3324512243270874\n",
            "iteration 349 :train_loss:1.348353624343872 val_loss1.3324512243270874\n",
            "iteration 350 :train_loss:1.3483537435531616 val_loss1.3324508666992188\n",
            "iteration 351 :train_loss:1.3483535051345825 val_loss1.3324508666992188\n",
            "iteration 352 :train_loss:1.3483535051345825 val_loss1.3324508666992188\n",
            "iteration 353 :train_loss:1.3483531475067139 val_loss1.3324508666992188\n",
            "iteration 354 :train_loss:1.3483530282974243 val_loss1.3324508666992188\n",
            "iteration 355 :train_loss:1.3483530282974243 val_loss1.3324507474899292\n",
            "iteration 356 :train_loss:1.3483529090881348 val_loss1.3324506282806396\n",
            "iteration 357 :train_loss:1.3483526706695557 val_loss1.3324506282806396\n",
            "iteration 358 :train_loss:1.3483526706695557 val_loss1.3324506282806396\n",
            "iteration 359 :train_loss:1.3483526706695557 val_loss1.332450270652771\n",
            "iteration 360 :train_loss:1.3483526706695557 val_loss1.3324501514434814\n",
            "iteration 361 :train_loss:1.3483526706695557 val_loss1.332450032234192\n",
            "iteration 362 :train_loss:1.3483524322509766 val_loss1.332450032234192\n",
            "iteration 363 :train_loss:1.3483526706695557 val_loss1.332450032234192\n",
            "iteration 364 :train_loss:1.3483524322509766 val_loss1.332450032234192\n",
            "iteration 365 :train_loss:1.348352313041687 val_loss1.332450032234192\n",
            "iteration 366 :train_loss:1.348352313041687 val_loss1.3324499130249023\n",
            "iteration 367 :train_loss:1.348352313041687 val_loss1.3324497938156128\n",
            "iteration 368 :train_loss:1.3483521938323975 val_loss1.3324497938156128\n",
            "iteration 369 :train_loss:1.3483521938323975 val_loss1.3324497938156128\n",
            "iteration 370 :train_loss:1.348352074623108 val_loss1.3324494361877441\n",
            "iteration 372 :train_loss:1.348352074623108 val_loss1.3324494361877441\n",
            "iteration 373 :train_loss:1.3483521938323975 val_loss1.3324494361877441\n",
            "iteration 374 :train_loss:1.348352074623108 val_loss1.3324494361877441\n",
            "iteration 375 :train_loss:1.348352074623108 val_loss1.332449197769165\n",
            "iteration 376 :train_loss:1.3483519554138184 val_loss1.332449197769165\n",
            "iteration 377 :train_loss:1.348352074623108 val_loss1.332449197769165\n",
            "iteration 378 :train_loss:1.3483515977859497 val_loss1.332449197769165\n",
            "iteration 379 :train_loss:1.3483514785766602 val_loss1.3324488401412964\n",
            "iteration 380 :train_loss:1.3483514785766602 val_loss1.3324488401412964\n",
            "iteration 381 :train_loss:1.3483514785766602 val_loss1.3324488401412964\n",
            "iteration 382 :train_loss:1.3483514785766602 val_loss1.3324486017227173\n",
            "iteration 383 :train_loss:1.3483514785766602 val_loss1.3324486017227173\n",
            "iteration 384 :train_loss:1.3483513593673706 val_loss1.3324486017227173\n",
            "iteration 385 :train_loss:1.3483513593673706 val_loss1.3324486017227173\n",
            "iteration 386 :train_loss:1.348351240158081 val_loss1.3324482440948486\n",
            "iteration 387 :train_loss:1.348351240158081 val_loss1.3324482440948486\n",
            "iteration 388 :train_loss:1.348351001739502 val_loss1.3324482440948486\n",
            "iteration 389 :train_loss:1.348351001739502 val_loss1.3324482440948486\n",
            "iteration 390 :train_loss:1.3483508825302124 val_loss1.3324482440948486\n",
            "iteration 391 :train_loss:1.3483508825302124 val_loss1.3324482440948486\n",
            "iteration 392 :train_loss:1.3483508825302124 val_loss1.3324480056762695\n",
            "iteration 393 :train_loss:1.3483507633209229 val_loss1.3324480056762695\n",
            "iteration 394 :train_loss:1.3483507633209229 val_loss1.3324480056762695\n",
            "iteration 395 :train_loss:1.3483507633209229 val_loss1.3324480056762695\n",
            "iteration 396 :train_loss:1.3483507633209229 val_loss1.3324480056762695\n",
            "iteration 397 :train_loss:1.3483506441116333 val_loss1.33244788646698\n",
            "iteration 398 :train_loss:1.3483507633209229 val_loss1.3324480056762695\n",
            "iteration 399 :train_loss:1.3483506441116333 val_loss1.3324477672576904\n",
            "iteration 400 :train_loss:1.3483507633209229 val_loss1.3324477672576904\n",
            "iteration 401 :train_loss:1.3483507633209229 val_loss1.3324475288391113\n",
            "iteration 402 :train_loss:1.348350167274475 val_loss1.3324474096298218\n",
            "iteration 403 :train_loss:1.348350167274475 val_loss1.3324474096298218\n",
            "iteration 404 :train_loss:1.3483505249023438 val_loss1.3324474096298218\n",
            "iteration 405 :train_loss:1.348349928855896 val_loss1.3324474096298218\n",
            "iteration 406 :train_loss:1.348349928855896 val_loss1.3324474096298218\n",
            "iteration 407 :train_loss:1.348349928855896 val_loss1.3324474096298218\n",
            "iteration 408 :train_loss:1.348349928855896 val_loss1.3324471712112427\n",
            "iteration 409 :train_loss:1.348349928855896 val_loss1.3324471712112427\n",
            "iteration 410 :train_loss:1.3483495712280273 val_loss1.3324471712112427\n",
            "iteration 411 :train_loss:1.3483495712280273 val_loss1.3324471712112427\n",
            "iteration 412 :train_loss:1.3483495712280273 val_loss1.3324470520019531\n",
            "iteration 413 :train_loss:1.3483494520187378 val_loss1.3324470520019531\n",
            "iteration 414 :train_loss:1.3483495712280273 val_loss1.332446813583374\n",
            "iteration 415 :train_loss:1.3483494520187378 val_loss1.332446813583374\n",
            "iteration 416 :train_loss:1.3483494520187378 val_loss1.332446813583374\n",
            "iteration 417 :train_loss:1.3483493328094482 val_loss1.332446813583374\n",
            "iteration 418 :train_loss:1.3483493328094482 val_loss1.3324466943740845\n",
            "iteration 419 :train_loss:1.3483493328094482 val_loss1.332446813583374\n",
            "iteration 420 :train_loss:1.3483492136001587 val_loss1.3324466943740845\n",
            "iteration 421 :train_loss:1.3483490943908691 val_loss1.332446575164795\n",
            "iteration 422 :train_loss:1.3483490943908691 val_loss1.332446575164795\n",
            "iteration 423 :train_loss:1.3483490943908691 val_loss1.332446575164795\n",
            "iteration 424 :train_loss:1.3483490943908691 val_loss1.332446575164795\n",
            "iteration 425 :train_loss:1.34834885597229 val_loss1.332446575164795\n",
            "iteration 426 :train_loss:1.34834885597229 val_loss1.3324464559555054\n",
            "iteration 427 :train_loss:1.34834885597229 val_loss1.3324464559555054\n",
            "iteration 428 :train_loss:1.34834885597229 val_loss1.3324463367462158\n",
            "iteration 429 :train_loss:1.3483487367630005 val_loss1.3324463367462158\n",
            "iteration 430 :train_loss:1.3483487367630005 val_loss1.3324460983276367\n",
            "iteration 431 :train_loss:1.3483487367630005 val_loss1.3324460983276367\n",
            "iteration 432 :train_loss:1.3483487367630005 val_loss1.3324459791183472\n",
            "iteration 433 :train_loss:1.3483487367630005 val_loss1.3324459791183472\n",
            "iteration 434 :train_loss:1.3483487367630005 val_loss1.3324459791183472\n",
            "iteration 435 :train_loss:1.3483487367630005 val_loss1.3324458599090576\n",
            "iteration 436 :train_loss:1.348348617553711 val_loss1.332445740699768\n",
            "iteration 437 :train_loss:1.348348617553711 val_loss1.332445740699768\n",
            "iteration 438 :train_loss:1.348348617553711 val_loss1.332445740699768\n",
            "iteration 439 :train_loss:1.3483484983444214 val_loss1.332445740699768\n",
            "iteration 440 :train_loss:1.3483484983444214 val_loss1.332445740699768\n",
            "iteration 441 :train_loss:1.3483484983444214 val_loss1.3324456214904785\n",
            "iteration 442 :train_loss:1.3483484983444214 val_loss1.3324456214904785\n",
            "iteration 443 :train_loss:1.3483484983444214 val_loss1.3324453830718994\n",
            "iteration 444 :train_loss:1.3483482599258423 val_loss1.3324453830718994\n",
            "iteration 445 :train_loss:1.3483482599258423 val_loss1.3324453830718994\n",
            "iteration 446 :train_loss:1.3483482599258423 val_loss1.3324453830718994\n",
            "iteration 447 :train_loss:1.3483482599258423 val_loss1.3324453830718994\n",
            "iteration 448 :train_loss:1.3483481407165527 val_loss1.3324451446533203\n",
            "iteration 449 :train_loss:1.3483481407165527 val_loss1.3324451446533203\n",
            "iteration 450 :train_loss:1.3483481407165527 val_loss1.3324451446533203\n",
            "iteration 451 :train_loss:1.3483480215072632 val_loss1.3324451446533203\n",
            "iteration 452 :train_loss:1.3483479022979736 val_loss1.3324450254440308\n",
            "iteration 453 :train_loss:1.3483479022979736 val_loss1.3324451446533203\n",
            "iteration 454 :train_loss:1.3483479022979736 val_loss1.3324451446533203\n",
            "iteration 455 :train_loss:1.3483479022979736 val_loss1.3324450254440308\n",
            "iteration 456 :train_loss:1.3483479022979736 val_loss1.3324449062347412\n",
            "iteration 457 :train_loss:1.3483479022979736 val_loss1.3324449062347412\n",
            "iteration 458 :train_loss:1.348347783088684 val_loss1.3324445486068726\n",
            "iteration 459 :train_loss:1.3483479022979736 val_loss1.3324445486068726\n",
            "iteration 460 :train_loss:1.3483479022979736 val_loss1.3324445486068726\n",
            "iteration 461 :train_loss:1.3483479022979736 val_loss1.3324445486068726\n",
            "iteration 462 :train_loss:1.348347783088684 val_loss1.3324445486068726\n",
            "iteration 463 :train_loss:1.348347783088684 val_loss1.3324445486068726\n",
            "iteration 464 :train_loss:1.3483479022979736 val_loss1.3324443101882935\n",
            "iteration 465 :train_loss:1.348347783088684 val_loss1.3324445486068726\n",
            "iteration 466 :train_loss:1.348347783088684 val_loss1.3324443101882935\n",
            "iteration 467 :train_loss:1.3483473062515259 val_loss1.3324443101882935\n",
            "iteration 468 :train_loss:1.3483473062515259 val_loss1.3324443101882935\n",
            "iteration 469 :train_loss:1.3483473062515259 val_loss1.332444190979004\n",
            "iteration 470 :train_loss:1.3483473062515259 val_loss1.3324443101882935\n",
            "iteration 471 :train_loss:1.3483473062515259 val_loss1.3324439525604248\n",
            "iteration 472 :train_loss:1.3483473062515259 val_loss1.3324439525604248\n",
            "iteration 473 :train_loss:1.3483473062515259 val_loss1.3324439525604248\n",
            "iteration 474 :train_loss:1.3483470678329468 val_loss1.3324439525604248\n",
            "iteration 475 :train_loss:1.3483474254608154 val_loss1.3324439525604248\n",
            "iteration 476 :train_loss:1.3483468294143677 val_loss1.3324438333511353\n",
            "iteration 477 :train_loss:1.3483470678329468 val_loss1.3324437141418457\n",
            "iteration 478 :train_loss:1.3483465909957886 val_loss1.3324437141418457\n",
            "iteration 479 :train_loss:1.3483467102050781 val_loss1.3324437141418457\n",
            "iteration 480 :train_loss:1.3483467102050781 val_loss1.3324437141418457\n",
            "iteration 481 :train_loss:1.3483465909957886 val_loss1.3324437141418457\n",
            "iteration 482 :train_loss:1.3483465909957886 val_loss1.3324437141418457\n",
            "iteration 483 :train_loss:1.3483465909957886 val_loss1.3324437141418457\n",
            "iteration 484 :train_loss:1.3483465909957886 val_loss1.3324437141418457\n",
            "iteration 485 :train_loss:1.3483467102050781 val_loss1.3324435949325562\n",
            "iteration 486 :train_loss:1.348346471786499 val_loss1.3324437141418457\n",
            "iteration 487 :train_loss:1.348346471786499 val_loss1.3324435949325562\n",
            "iteration 488 :train_loss:1.348346471786499 val_loss1.3324434757232666\n",
            "iteration 489 :train_loss:1.348346471786499 val_loss1.3324435949325562\n",
            "iteration 490 :train_loss:1.3483461141586304 val_loss1.3324434757232666\n",
            "iteration 491 :train_loss:1.3483461141586304 val_loss1.3324434757232666\n",
            "iteration 492 :train_loss:1.3483461141586304 val_loss1.3324434757232666\n",
            "iteration 493 :train_loss:1.3483461141586304 val_loss1.3324432373046875\n",
            "iteration 494 :train_loss:1.3483461141586304 val_loss1.3324432373046875\n",
            "iteration 495 :train_loss:1.3483461141586304 val_loss1.332443118095398\n",
            "iteration 496 :train_loss:1.3483461141586304 val_loss1.332443118095398\n",
            "iteration 497 :train_loss:1.3483461141586304 val_loss1.332443118095398\n",
            "iteration 498 :train_loss:1.3483459949493408 val_loss1.332443118095398\n",
            "iteration 499 :train_loss:1.3483459949493408 val_loss1.3324429988861084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INXNzR08kJj3",
        "outputId": "154d4769-c88b-4fb2-85b1-81e82fe42294"
      },
      "source": [
        "predY=predict(parameters, test_X.T)\n",
        "MAPE = MAPE(test_Y,predY)\n",
        "print(\"MAPE:\",MAPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2.0669248 2.072769  2.0689201 ... 2.0752263 2.0818286 2.072154 ]], shape=(1, 3000), dtype=float32)\n",
            "MAPE: tf.Tensor(60.68107, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "Hms6-_lIk5bm",
        "outputId": "072f1155-2cc8-4c3d-d10c-ac79722d97aa"
      },
      "source": [
        "plotLearningCurve()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5FabZClnarz",
        "outputId": "9f8ac3ae-b550-4a71-fcd0-6f7f736d69d5"
      },
      "source": [
        "iterations = 100\n",
        "parameters, history = create_nn_model(train_X.T, train_Y.T, 100, val_X.T, val_Y.T, iterations, 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 :train_loss:5.644540309906006 val_loss5.630524635314941\n",
            "iteration 1 :train_loss:5.473517417907715 val_loss5.459460735321045\n",
            "iteration 2 :train_loss:5.309300899505615 val_loss5.295202255249023\n",
            "iteration 3 :train_loss:5.151616096496582 val_loss5.137473106384277\n",
            "iteration 4 :train_loss:5.00019645690918 val_loss4.986014366149902\n",
            "iteration 5 :train_loss:4.854792594909668 val_loss4.84057092666626\n",
            "iteration 6 :train_loss:4.715163707733154 val_loss4.700902462005615\n",
            "iteration 7 :train_loss:4.581080436706543 val_loss4.566778659820557\n",
            "iteration 8 :train_loss:4.452314376831055 val_loss4.437978267669678\n",
            "iteration 9 :train_loss:4.32866096496582 val_loss4.314286708831787\n",
            "iteration 10 :train_loss:4.209910869598389 val_loss4.195502281188965\n",
            "iteration 11 :train_loss:4.095871448516846 val_loss4.081428527832031\n",
            "iteration 12 :train_loss:3.9863531589508057 val_loss3.971876382827759\n",
            "iteration 13 :train_loss:3.8811779022216797 val_loss3.866668701171875\n",
            "iteration 14 :train_loss:3.780172824859619 val_loss3.7656290531158447\n",
            "iteration 15 :train_loss:3.683171033859253 val_loss3.668595552444458\n",
            "iteration 16 :train_loss:3.5900144577026367 val_loss3.5754072666168213\n",
            "iteration 17 :train_loss:3.500549793243408 val_loss3.4859116077423096\n",
            "iteration 18 :train_loss:3.4146289825439453 val_loss3.399961471557617\n",
            "iteration 19 :train_loss:3.3321151733398438 val_loss3.317417860031128\n",
            "iteration 20 :train_loss:3.2528717517852783 val_loss3.238144874572754\n",
            "iteration 21 :train_loss:3.176767110824585 val_loss3.162012815475464\n",
            "iteration 22 :train_loss:3.1036787033081055 val_loss3.0888969898223877\n",
            "iteration 23 :train_loss:3.0334882736206055 val_loss3.01867938041687\n",
            "iteration 24 :train_loss:2.9660804271698 val_loss2.951244354248047\n",
            "iteration 25 :train_loss:2.9013442993164062 val_loss2.886482000350952\n",
            "iteration 26 :train_loss:2.839174509048462 val_loss2.824287176132202\n",
            "iteration 27 :train_loss:2.779468536376953 val_loss2.764557361602783\n",
            "iteration 28 :train_loss:2.7221319675445557 val_loss2.7071964740753174\n",
            "iteration 29 :train_loss:2.6670706272125244 val_loss2.6521105766296387\n",
            "iteration 30 :train_loss:2.614192008972168 val_loss2.599208354949951\n",
            "iteration 31 :train_loss:2.5634121894836426 val_loss2.5484068393707275\n",
            "iteration 32 :train_loss:2.5146472454071045 val_loss2.499619722366333\n",
            "iteration 33 :train_loss:2.4678187370300293 val_loss2.4527688026428223\n",
            "iteration 34 :train_loss:2.422849655151367 val_loss2.4077789783477783\n",
            "iteration 35 :train_loss:2.379666328430176 val_loss2.3645753860473633\n",
            "iteration 36 :train_loss:2.3381991386413574 val_loss2.323087453842163\n",
            "iteration 37 :train_loss:2.2983789443969727 val_loss2.28324818611145\n",
            "iteration 38 :train_loss:2.2601430416107178 val_loss2.2449920177459717\n",
            "iteration 39 :train_loss:2.223426342010498 val_loss2.2082571983337402\n",
            "iteration 40 :train_loss:2.188171625137329 val_loss2.172983407974243\n",
            "iteration 41 :train_loss:2.1543192863464355 val_loss2.139113664627075\n",
            "iteration 42 :train_loss:2.121814489364624 val_loss2.106590986251831\n",
            "iteration 43 :train_loss:2.090604305267334 val_loss2.075363874435425\n",
            "iteration 44 :train_loss:2.060638189315796 val_loss2.0453805923461914\n",
            "iteration 45 :train_loss:2.031865358352661 val_loss2.016592264175415\n",
            "iteration 46 :train_loss:2.0042405128479004 val_loss1.9889516830444336\n",
            "iteration 47 :train_loss:1.977718472480774 val_loss1.9624137878417969\n",
            "iteration 48 :train_loss:1.9522544145584106 val_loss1.9369347095489502\n",
            "iteration 49 :train_loss:1.9278072118759155 val_loss1.9124727249145508\n",
            "iteration 50 :train_loss:1.9043368101119995 val_loss1.8889881372451782\n",
            "iteration 51 :train_loss:1.8818045854568481 val_loss1.866442322731018\n",
            "iteration 52 :train_loss:1.8601746559143066 val_loss1.8447978496551514\n",
            "iteration 53 :train_loss:1.8394092321395874 val_loss1.8240193128585815\n",
            "iteration 54 :train_loss:1.819474697113037 val_loss1.8040724992752075\n",
            "iteration 55 :train_loss:1.8003398180007935 val_loss1.7849245071411133\n",
            "iteration 56 :train_loss:1.781970739364624 val_loss1.7665435075759888\n",
            "iteration 57 :train_loss:1.7643399238586426 val_loss1.7489001750946045\n",
            "iteration 58 :train_loss:1.7474149465560913 val_loss1.7319645881652832\n",
            "iteration 59 :train_loss:1.7311710119247437 val_loss1.7157084941864014\n",
            "iteration 60 :train_loss:1.7155787944793701 val_loss1.7001053094863892\n",
            "iteration 61 :train_loss:1.7006129026412964 val_loss1.6851294040679932\n",
            "iteration 62 :train_loss:1.6862503290176392 val_loss1.6707557439804077\n",
            "iteration 63 :train_loss:1.6724655628204346 val_loss1.6569607257843018\n",
            "iteration 64 :train_loss:1.6592353582382202 val_loss1.6437207460403442\n",
            "iteration 65 :train_loss:1.646539330482483 val_loss1.6310144662857056\n",
            "iteration 66 :train_loss:1.6343539953231812 val_loss1.6188204288482666\n",
            "iteration 67 :train_loss:1.6226611137390137 val_loss1.6071183681488037\n",
            "iteration 68 :train_loss:1.6114401817321777 val_loss1.595888614654541\n",
            "iteration 69 :train_loss:1.6006723642349243 val_loss1.5851125717163086\n",
            "iteration 70 :train_loss:1.590340495109558 val_loss1.5747722387313843\n",
            "iteration 71 :train_loss:1.5804263353347778 val_loss1.5648497343063354\n",
            "iteration 72 :train_loss:1.5709129571914673 val_loss1.5553293228149414\n",
            "iteration 73 :train_loss:1.5617859363555908 val_loss1.5461939573287964\n",
            "iteration 74 :train_loss:1.55302894115448 val_loss1.5374294519424438\n",
            "iteration 75 :train_loss:1.5446264743804932 val_loss1.5290203094482422\n",
            "iteration 76 :train_loss:1.5365655422210693 val_loss1.5209518671035767\n",
            "iteration 77 :train_loss:1.5288323163986206 val_loss1.513211727142334\n",
            "iteration 78 :train_loss:1.5214128494262695 val_loss1.5057858228683472\n",
            "iteration 79 :train_loss:1.5142956972122192 val_loss1.4986621141433716\n",
            "iteration 80 :train_loss:1.50746750831604 val_loss1.491828203201294\n",
            "iteration 81 :train_loss:1.5009177923202515 val_loss1.4852725267410278\n",
            "iteration 82 :train_loss:1.4946352243423462 val_loss1.4789841175079346\n",
            "iteration 83 :train_loss:1.4886090755462646 val_loss1.472951889038086\n",
            "iteration 84 :train_loss:1.4828282594680786 val_loss1.4671659469604492\n",
            "iteration 85 :train_loss:1.47728431224823 val_loss1.461616039276123\n",
            "iteration 86 :train_loss:1.4719665050506592 val_loss1.456292986869812\n",
            "iteration 87 :train_loss:1.466866135597229 val_loss1.4511874914169312\n",
            "iteration 88 :train_loss:1.4619742631912231 val_loss1.4462907314300537\n",
            "iteration 89 :train_loss:1.4572826623916626 val_loss1.4415950775146484\n",
            "iteration 90 :train_loss:1.4527838230133057 val_loss1.4370909929275513\n",
            "iteration 91 :train_loss:1.448468804359436 val_loss1.4327720403671265\n",
            "iteration 92 :train_loss:1.4443315267562866 val_loss1.4286302328109741\n",
            "iteration 93 :train_loss:1.4403632879257202 val_loss1.4246578216552734\n",
            "iteration 94 :train_loss:1.4365586042404175 val_loss1.420849084854126\n",
            "iteration 95 :train_loss:1.4329102039337158 val_loss1.4171963930130005\n",
            "iteration 96 :train_loss:1.429411768913269 val_loss1.413694143295288\n",
            "iteration 97 :train_loss:1.42605721950531 val_loss1.4103361368179321\n",
            "iteration 98 :train_loss:1.42284095287323 val_loss1.4071162939071655\n",
            "iteration 99 :train_loss:1.4197571277618408 val_loss1.4040288925170898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU7fyW74omzV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk2S-Wr5onN3",
        "outputId": "71becf4c-1a1b-442f-bf2c-76acc04b0c76"
      },
      "source": [
        "predY=predict(parameters, test_X.T)\n",
        "MAPE = MAPE(test_Y,predY)\n",
        "print(\"MAPE:\",MAPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[1.8103437 1.8021277 1.8000603 ... 1.8085947 1.8119953 1.8019692]], shape=(1, 3000), dtype=float32)\n",
            "MAPE: tf.Tensor(51.5707, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "wFcW_YGao32M",
        "outputId": "af665656-8e46-438f-cd99-8f987a92ec58"
      },
      "source": [
        "plotLearningCurve()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8T65VbspNZs",
        "outputId": "7c05238d-f363-4717-dae0-5e69e376d06b"
      },
      "source": [
        "iterations = 500\n",
        "parameters, history = create_nn_model(train_X.T, train_Y.T, 100, val_X.T, val_Y.T, iterations, 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 :train_loss:5.644540309906006 val_loss5.630524635314941\n",
            "iteration 1 :train_loss:5.473517417907715 val_loss5.459460735321045\n",
            "iteration 2 :train_loss:5.309300899505615 val_loss5.295202255249023\n",
            "iteration 3 :train_loss:5.151616096496582 val_loss5.137473106384277\n",
            "iteration 4 :train_loss:5.00019645690918 val_loss4.986014366149902\n",
            "iteration 5 :train_loss:4.854792594909668 val_loss4.84057092666626\n",
            "iteration 6 :train_loss:4.715163707733154 val_loss4.700902462005615\n",
            "iteration 7 :train_loss:4.581080436706543 val_loss4.566778659820557\n",
            "iteration 8 :train_loss:4.452314376831055 val_loss4.437978267669678\n",
            "iteration 9 :train_loss:4.32866096496582 val_loss4.314286708831787\n",
            "iteration 10 :train_loss:4.209910869598389 val_loss4.195502281188965\n",
            "iteration 11 :train_loss:4.095871448516846 val_loss4.081428527832031\n",
            "iteration 12 :train_loss:3.9863531589508057 val_loss3.971876382827759\n",
            "iteration 13 :train_loss:3.8811779022216797 val_loss3.866668701171875\n",
            "iteration 14 :train_loss:3.780172824859619 val_loss3.7656290531158447\n",
            "iteration 15 :train_loss:3.683171033859253 val_loss3.668595552444458\n",
            "iteration 16 :train_loss:3.5900144577026367 val_loss3.5754072666168213\n",
            "iteration 17 :train_loss:3.500549793243408 val_loss3.4859116077423096\n",
            "iteration 18 :train_loss:3.4146289825439453 val_loss3.399961471557617\n",
            "iteration 19 :train_loss:3.3321151733398438 val_loss3.317417860031128\n",
            "iteration 20 :train_loss:3.2528717517852783 val_loss3.238144874572754\n",
            "iteration 21 :train_loss:3.176767110824585 val_loss3.162012815475464\n",
            "iteration 22 :train_loss:3.1036787033081055 val_loss3.0888969898223877\n",
            "iteration 23 :train_loss:3.0334882736206055 val_loss3.01867938041687\n",
            "iteration 24 :train_loss:2.9660804271698 val_loss2.951244354248047\n",
            "iteration 25 :train_loss:2.9013442993164062 val_loss2.886482000350952\n",
            "iteration 26 :train_loss:2.839174509048462 val_loss2.824287176132202\n",
            "iteration 27 :train_loss:2.779468536376953 val_loss2.764557361602783\n",
            "iteration 28 :train_loss:2.7221319675445557 val_loss2.7071964740753174\n",
            "iteration 29 :train_loss:2.6670706272125244 val_loss2.6521105766296387\n",
            "iteration 30 :train_loss:2.614192008972168 val_loss2.599208354949951\n",
            "iteration 31 :train_loss:2.5634121894836426 val_loss2.5484068393707275\n",
            "iteration 32 :train_loss:2.5146472454071045 val_loss2.499619722366333\n",
            "iteration 33 :train_loss:2.4678187370300293 val_loss2.4527688026428223\n",
            "iteration 34 :train_loss:2.422849655151367 val_loss2.4077789783477783\n",
            "iteration 35 :train_loss:2.379666328430176 val_loss2.3645753860473633\n",
            "iteration 36 :train_loss:2.3381991386413574 val_loss2.323087453842163\n",
            "iteration 37 :train_loss:2.2983789443969727 val_loss2.28324818611145\n",
            "iteration 38 :train_loss:2.2601430416107178 val_loss2.2449920177459717\n",
            "iteration 39 :train_loss:2.223426342010498 val_loss2.2082571983337402\n",
            "iteration 40 :train_loss:2.188171625137329 val_loss2.172983407974243\n",
            "iteration 41 :train_loss:2.1543192863464355 val_loss2.139113664627075\n",
            "iteration 42 :train_loss:2.121814489364624 val_loss2.106590986251831\n",
            "iteration 43 :train_loss:2.090604305267334 val_loss2.075363874435425\n",
            "iteration 44 :train_loss:2.060638189315796 val_loss2.0453805923461914\n",
            "iteration 45 :train_loss:2.031865358352661 val_loss2.016592264175415\n",
            "iteration 46 :train_loss:2.0042405128479004 val_loss1.9889516830444336\n",
            "iteration 47 :train_loss:1.977718472480774 val_loss1.9624137878417969\n",
            "iteration 48 :train_loss:1.9522544145584106 val_loss1.9369347095489502\n",
            "iteration 49 :train_loss:1.9278072118759155 val_loss1.9124727249145508\n",
            "iteration 50 :train_loss:1.9043368101119995 val_loss1.8889881372451782\n",
            "iteration 51 :train_loss:1.8818045854568481 val_loss1.866442322731018\n",
            "iteration 52 :train_loss:1.8601746559143066 val_loss1.8447978496551514\n",
            "iteration 53 :train_loss:1.8394092321395874 val_loss1.8240193128585815\n",
            "iteration 54 :train_loss:1.819474697113037 val_loss1.8040724992752075\n",
            "iteration 55 :train_loss:1.8003398180007935 val_loss1.7849245071411133\n",
            "iteration 56 :train_loss:1.781970739364624 val_loss1.7665435075759888\n",
            "iteration 57 :train_loss:1.7643399238586426 val_loss1.7489001750946045\n",
            "iteration 58 :train_loss:1.7474149465560913 val_loss1.7319645881652832\n",
            "iteration 59 :train_loss:1.7311710119247437 val_loss1.7157084941864014\n",
            "iteration 60 :train_loss:1.7155787944793701 val_loss1.7001053094863892\n",
            "iteration 61 :train_loss:1.7006129026412964 val_loss1.6851294040679932\n",
            "iteration 62 :train_loss:1.6862503290176392 val_loss1.6707557439804077\n",
            "iteration 63 :train_loss:1.6724655628204346 val_loss1.6569607257843018\n",
            "iteration 64 :train_loss:1.6592353582382202 val_loss1.6437207460403442\n",
            "iteration 65 :train_loss:1.646539330482483 val_loss1.6310144662857056\n",
            "iteration 66 :train_loss:1.6343539953231812 val_loss1.6188204288482666\n",
            "iteration 67 :train_loss:1.6226611137390137 val_loss1.6071183681488037\n",
            "iteration 68 :train_loss:1.6114401817321777 val_loss1.595888614654541\n",
            "iteration 69 :train_loss:1.6006723642349243 val_loss1.5851125717163086\n",
            "iteration 70 :train_loss:1.590340495109558 val_loss1.5747722387313843\n",
            "iteration 71 :train_loss:1.5804263353347778 val_loss1.5648497343063354\n",
            "iteration 72 :train_loss:1.5709129571914673 val_loss1.5553293228149414\n",
            "iteration 73 :train_loss:1.5617859363555908 val_loss1.5461939573287964\n",
            "iteration 74 :train_loss:1.55302894115448 val_loss1.5374294519424438\n",
            "iteration 75 :train_loss:1.5446264743804932 val_loss1.5290203094482422\n",
            "iteration 76 :train_loss:1.5365655422210693 val_loss1.5209518671035767\n",
            "iteration 77 :train_loss:1.5288323163986206 val_loss1.513211727142334\n",
            "iteration 78 :train_loss:1.5214128494262695 val_loss1.5057858228683472\n",
            "iteration 79 :train_loss:1.5142956972122192 val_loss1.4986621141433716\n",
            "iteration 80 :train_loss:1.50746750831604 val_loss1.491828203201294\n",
            "iteration 81 :train_loss:1.5009177923202515 val_loss1.4852725267410278\n",
            "iteration 82 :train_loss:1.4946352243423462 val_loss1.4789841175079346\n",
            "iteration 83 :train_loss:1.4886090755462646 val_loss1.472951889038086\n",
            "iteration 84 :train_loss:1.4828282594680786 val_loss1.4671659469604492\n",
            "iteration 85 :train_loss:1.47728431224823 val_loss1.461616039276123\n",
            "iteration 86 :train_loss:1.4719665050506592 val_loss1.456292986869812\n",
            "iteration 87 :train_loss:1.466866135597229 val_loss1.4511874914169312\n",
            "iteration 88 :train_loss:1.4619742631912231 val_loss1.4462907314300537\n",
            "iteration 89 :train_loss:1.4572826623916626 val_loss1.4415950775146484\n",
            "iteration 90 :train_loss:1.4527838230133057 val_loss1.4370909929275513\n",
            "iteration 91 :train_loss:1.448468804359436 val_loss1.4327720403671265\n",
            "iteration 92 :train_loss:1.4443315267562866 val_loss1.4286302328109741\n",
            "iteration 93 :train_loss:1.4403632879257202 val_loss1.4246578216552734\n",
            "iteration 94 :train_loss:1.4365586042404175 val_loss1.420849084854126\n",
            "iteration 95 :train_loss:1.4329102039337158 val_loss1.4171963930130005\n",
            "iteration 96 :train_loss:1.429411768913269 val_loss1.413694143295288\n",
            "iteration 97 :train_loss:1.42605721950531 val_loss1.4103361368179321\n",
            "iteration 98 :train_loss:1.42284095287323 val_loss1.4071162939071655\n",
            "iteration 99 :train_loss:1.4197571277618408 val_loss1.4040288925170898\n",
            "iteration 100 :train_loss:1.4168007373809814 val_loss1.4010688066482544\n",
            "iteration 101 :train_loss:1.4139657020568848 val_loss1.3982306718826294\n",
            "iteration 102 :train_loss:1.411247730255127 val_loss1.3955094814300537\n",
            "iteration 103 :train_loss:1.408642292022705 val_loss1.3929013013839722\n",
            "iteration 104 :train_loss:1.4061448574066162 val_loss1.3904004096984863\n",
            "iteration 105 :train_loss:1.4037505388259888 val_loss1.388002872467041\n",
            "iteration 106 :train_loss:1.4014548063278198 val_loss1.385704517364502\n",
            "iteration 107 :train_loss:1.399254322052002 val_loss1.3835011720657349\n",
            "iteration 108 :train_loss:1.3971452713012695 val_loss1.3813892602920532\n",
            "iteration 109 :train_loss:1.3951232433319092 val_loss1.3793646097183228\n",
            "iteration 110 :train_loss:1.3931852579116821 val_loss1.377423882484436\n",
            "iteration 111 :train_loss:1.3913270235061646 val_loss1.375563621520996\n",
            "iteration 112 :train_loss:1.3895467519760132 val_loss1.3737807273864746\n",
            "iteration 113 :train_loss:1.3878397941589355 val_loss1.3720717430114746\n",
            "iteration 114 :train_loss:1.3862040042877197 val_loss1.3704336881637573\n",
            "iteration 115 :train_loss:1.384636402130127 val_loss1.3688634634017944\n",
            "iteration 116 :train_loss:1.3831336498260498 val_loss1.367358684539795\n",
            "iteration 117 :train_loss:1.3816933631896973 val_loss1.36591637134552\n",
            "iteration 118 :train_loss:1.3803133964538574 val_loss1.36453378200531\n",
            "iteration 119 :train_loss:1.3789902925491333 val_loss1.3632094860076904\n",
            "iteration 120 :train_loss:1.377723217010498 val_loss1.3619396686553955\n",
            "iteration 121 :train_loss:1.3765078783035278 val_loss1.3607228994369507\n",
            "iteration 122 :train_loss:1.375343680381775 val_loss1.359556794166565\n",
            "iteration 123 :train_loss:1.3742281198501587 val_loss1.358439564704895\n",
            "iteration 124 :train_loss:1.3731586933135986 val_loss1.3573687076568604\n",
            "iteration 125 :train_loss:1.3721346855163574 val_loss1.3563424348831177\n",
            "iteration 126 :train_loss:1.3711528778076172 val_loss1.3553593158721924\n",
            "iteration 127 :train_loss:1.3702123165130615 val_loss1.3544169664382935\n",
            "iteration 128 :train_loss:1.369310975074768 val_loss1.3535139560699463\n",
            "iteration 129 :train_loss:1.3684473037719727 val_loss1.3526487350463867\n",
            "iteration 130 :train_loss:1.3676196336746216 val_loss1.3518195152282715\n",
            "iteration 131 :train_loss:1.3668266534805298 val_loss1.351025104522705\n",
            "iteration 132 :train_loss:1.3660666942596436 val_loss1.3502637147903442\n",
            "iteration 133 :train_loss:1.365338683128357 val_loss1.349534511566162\n",
            "iteration 134 :train_loss:1.364640712738037 val_loss1.3488353490829468\n",
            "iteration 135 :train_loss:1.3639723062515259 val_loss1.34816575050354\n",
            "iteration 136 :train_loss:1.3633326292037964 val_loss1.3475241661071777\n",
            "iteration 137 :train_loss:1.362719178199768 val_loss1.346909523010254\n",
            "iteration 138 :train_loss:1.3621307611465454 val_loss1.3463205099105835\n",
            "iteration 139 :train_loss:1.3615676164627075 val_loss1.345755934715271\n",
            "iteration 140 :train_loss:1.3610283136367798 val_loss1.3452154397964478\n",
            "iteration 141 :train_loss:1.360511064529419 val_loss1.3446972370147705\n",
            "iteration 142 :train_loss:1.360016107559204 val_loss1.3442009687423706\n",
            "iteration 143 :train_loss:1.3595415353775024 val_loss1.343725323677063\n",
            "iteration 144 :train_loss:1.3590872287750244 val_loss1.3432697057724\n",
            "iteration 145 :train_loss:1.3586512804031372 val_loss1.342833161354065\n",
            "iteration 146 :train_loss:1.3582347631454468 val_loss1.3424149751663208\n",
            "iteration 147 :train_loss:1.3578344583511353 val_loss1.3420143127441406\n",
            "iteration 148 :train_loss:1.3574517965316772 val_loss1.3416303396224976\n",
            "iteration 149 :train_loss:1.3570852279663086 val_loss1.3412624597549438\n",
            "iteration 150 :train_loss:1.356733798980713 val_loss1.3409103155136108\n",
            "iteration 151 :train_loss:1.356397032737732 val_loss1.340572476387024\n",
            "iteration 152 :train_loss:1.3560740947723389 val_loss1.3402493000030518\n",
            "iteration 153 :train_loss:1.3557652235031128 val_loss1.3399395942687988\n",
            "iteration 154 :train_loss:1.3554694652557373 val_loss1.3396426439285278\n",
            "iteration 155 :train_loss:1.355185866355896 val_loss1.3393583297729492\n",
            "iteration 156 :train_loss:1.3549137115478516 val_loss1.3390859365463257\n",
            "iteration 157 :train_loss:1.3546538352966309 val_loss1.3388246297836304\n",
            "iteration 158 :train_loss:1.3544045686721802 val_loss1.338574767112732\n",
            "iteration 159 :train_loss:1.3541661500930786 val_loss1.3383351564407349\n",
            "iteration 160 :train_loss:1.3539372682571411 val_loss1.3381054401397705\n",
            "iteration 161 :train_loss:1.3537179231643677 val_loss1.337885856628418\n",
            "iteration 162 :train_loss:1.3535079956054688 val_loss1.3376752138137817\n",
            "iteration 163 :train_loss:1.3533071279525757 val_loss1.3374732732772827\n",
            "iteration 164 :train_loss:1.3531142473220825 val_loss1.3372796773910522\n",
            "iteration 165 :train_loss:1.3529293537139893 val_loss1.3370944261550903\n",
            "iteration 166 :train_loss:1.352752447128296 val_loss1.3369170427322388\n",
            "iteration 167 :train_loss:1.352583885192871 val_loss1.3367469310760498\n",
            "iteration 168 :train_loss:1.3524210453033447 val_loss1.3365840911865234\n",
            "iteration 169 :train_loss:1.3522655963897705 val_loss1.3364278078079224\n",
            "iteration 170 :train_loss:1.3521162271499634 val_loss1.3362782001495361\n",
            "iteration 171 :train_loss:1.3519737720489502 val_loss1.3361347913742065\n",
            "iteration 172 :train_loss:1.3518368005752563 val_loss1.3359975814819336\n",
            "iteration 173 :train_loss:1.3517061471939087 val_loss1.3358659744262695\n",
            "iteration 174 :train_loss:1.3515801429748535 val_loss1.3357398509979248\n",
            "iteration 175 :train_loss:1.351460576057434 val_loss1.3356189727783203\n",
            "iteration 176 :train_loss:1.3513448238372803 val_loss1.335503339767456\n",
            "iteration 177 :train_loss:1.3512346744537354 val_loss1.3353923559188843\n",
            "iteration 178 :train_loss:1.3511290550231934 val_loss1.3352859020233154\n",
            "iteration 179 :train_loss:1.3510273694992065 val_loss1.335184097290039\n",
            "iteration 180 :train_loss:1.3509303331375122 val_loss1.335086703300476\n",
            "iteration 181 :train_loss:1.3508375883102417 val_loss1.3349931240081787\n",
            "iteration 182 :train_loss:1.3507487773895264 val_loss1.3349034786224365\n",
            "iteration 183 :train_loss:1.3506630659103394 val_loss1.33481764793396\n",
            "iteration 184 :train_loss:1.3505810499191284 val_loss1.3347350358963013\n",
            "iteration 185 :train_loss:1.3505020141601562 val_loss1.3346564769744873\n",
            "iteration 186 :train_loss:1.3504273891448975 val_loss1.334580659866333\n",
            "iteration 187 :train_loss:1.350355863571167 val_loss1.3345082998275757\n",
            "iteration 188 :train_loss:1.3502863645553589 val_loss1.3344388008117676\n",
            "iteration 189 :train_loss:1.3502205610275269 val_loss1.3343724012374878\n",
            "iteration 190 :train_loss:1.3501567840576172 val_loss1.334308385848999\n",
            "iteration 191 :train_loss:1.350096344947815 val_loss1.3342472314834595\n",
            "iteration 192 :train_loss:1.350037932395935 val_loss1.3341885805130005\n",
            "iteration 193 :train_loss:1.3499821424484253 val_loss1.3341323137283325\n",
            "iteration 194 :train_loss:1.3499284982681274 val_loss1.334078311920166\n",
            "iteration 195 :train_loss:1.349877119064331 val_loss1.3340266942977905\n",
            "iteration 196 :train_loss:1.349827766418457 val_loss1.333977222442627\n",
            "iteration 197 :train_loss:1.349780797958374 val_loss1.3339297771453857\n",
            "iteration 198 :train_loss:1.3497354984283447 val_loss1.3338840007781982\n",
            "iteration 199 :train_loss:1.3496923446655273 val_loss1.3338404893875122\n",
            "iteration 200 :train_loss:1.3496507406234741 val_loss1.3337984085083008\n",
            "iteration 201 :train_loss:1.3496112823486328 val_loss1.3337582349777222\n",
            "iteration 202 :train_loss:1.349573016166687 val_loss1.3337197303771973\n",
            "iteration 203 :train_loss:1.349536418914795 val_loss1.3336827754974365\n",
            "iteration 204 :train_loss:1.3495007753372192 val_loss1.3336471319198608\n",
            "iteration 205 :train_loss:1.3494677543640137 val_loss1.3336132764816284\n",
            "iteration 206 :train_loss:1.349435567855835 val_loss1.3335808515548706\n",
            "iteration 207 :train_loss:1.3494045734405518 val_loss1.3335494995117188\n",
            "iteration 208 :train_loss:1.349374771118164 val_loss1.3335193395614624\n",
            "iteration 209 :train_loss:1.3493460416793823 val_loss1.3334906101226807\n",
            "iteration 210 :train_loss:1.3493189811706543 val_loss1.3334629535675049\n",
            "iteration 211 :train_loss:1.349292278289795 val_loss1.333436369895935\n",
            "iteration 212 :train_loss:1.349267601966858 val_loss1.3334107398986816\n",
            "iteration 213 :train_loss:1.3492432832717896 val_loss1.3333864212036133\n",
            "iteration 214 :train_loss:1.3492202758789062 val_loss1.3333629369735718\n",
            "iteration 215 :train_loss:1.349197506904602 val_loss1.3333404064178467\n",
            "iteration 216 :train_loss:1.3491764068603516 val_loss1.333318829536438\n",
            "iteration 217 :train_loss:1.349156379699707 val_loss1.3332982063293457\n",
            "iteration 218 :train_loss:1.349136471748352 val_loss1.3332780599594116\n",
            "iteration 219 :train_loss:1.349117636680603 val_loss1.333258867263794\n",
            "iteration 220 :train_loss:1.34909987449646 val_loss1.3332406282424927\n",
            "iteration 221 :train_loss:1.3490819931030273 val_loss1.3332229852676392\n",
            "iteration 222 :train_loss:1.3490650653839111 val_loss1.3332061767578125\n",
            "iteration 223 :train_loss:1.34904944896698 val_loss1.333189606666565\n",
            "iteration 224 :train_loss:1.349034070968628 val_loss1.3331739902496338\n",
            "iteration 225 :train_loss:1.349019169807434 val_loss1.3331587314605713\n",
            "iteration 226 :train_loss:1.349005103111267 val_loss1.3331444263458252\n",
            "iteration 227 :train_loss:1.3489910364151 val_loss1.3331305980682373\n",
            "iteration 228 :train_loss:1.34897780418396 val_loss1.3331172466278076\n",
            "iteration 229 :train_loss:1.348964810371399 val_loss1.3331043720245361\n",
            "iteration 230 :train_loss:1.348953366279602 val_loss1.3330920934677124\n",
            "iteration 231 :train_loss:1.3489415645599365 val_loss1.3330799341201782\n",
            "iteration 232 :train_loss:1.3489307165145874 val_loss1.3330683708190918\n",
            "iteration 233 :train_loss:1.3489195108413696 val_loss1.3330577611923218\n",
            "iteration 234 :train_loss:1.3489094972610474 val_loss1.3330469131469727\n",
            "iteration 235 :train_loss:1.3488991260528564 val_loss1.3330366611480713\n",
            "iteration 236 :train_loss:1.3488898277282715 val_loss1.3330268859863281\n",
            "iteration 237 :train_loss:1.3488799333572388 val_loss1.3330175876617432\n",
            "iteration 238 :train_loss:1.3488712310791016 val_loss1.3330084085464478\n",
            "iteration 239 :train_loss:1.348862886428833 val_loss1.3329994678497314\n",
            "iteration 240 :train_loss:1.3488545417785645 val_loss1.332991123199463\n",
            "iteration 241 :train_loss:1.3488470315933228 val_loss1.3329827785491943\n",
            "iteration 242 :train_loss:1.348839282989502 val_loss1.332974910736084\n",
            "iteration 243 :train_loss:1.3488315343856812 val_loss1.3329674005508423\n",
            "iteration 244 :train_loss:1.3488246202468872 val_loss1.3329602479934692\n",
            "iteration 245 :train_loss:1.3488174676895142 val_loss1.3329529762268066\n",
            "iteration 246 :train_loss:1.348811149597168 val_loss1.3329463005065918\n",
            "iteration 247 :train_loss:1.3488045930862427 val_loss1.3329397439956665\n",
            "iteration 248 :train_loss:1.3487985134124756 val_loss1.3329335451126099\n",
            "iteration 249 :train_loss:1.3487927913665771 val_loss1.3329273462295532\n",
            "iteration 250 :train_loss:1.3487870693206787 val_loss1.3329213857650757\n",
            "iteration 251 :train_loss:1.3487813472747803 val_loss1.3329157829284668\n",
            "iteration 252 :train_loss:1.34877610206604 val_loss1.332910180091858\n",
            "iteration 253 :train_loss:1.3487712144851685 val_loss1.3329049348831177\n",
            "iteration 254 :train_loss:1.3487660884857178 val_loss1.3328996896743774\n",
            "iteration 255 :train_loss:1.3487612009048462 val_loss1.3328949213027954\n",
            "iteration 256 :train_loss:1.3487569093704224 val_loss1.3328900337219238\n",
            "iteration 257 :train_loss:1.3487526178359985 val_loss1.332885503768921\n",
            "iteration 258 :train_loss:1.3487480878829956 val_loss1.3328808546066284\n",
            "iteration 259 :train_loss:1.3487439155578613 val_loss1.332876443862915\n",
            "iteration 260 :train_loss:1.3487396240234375 val_loss1.3328725099563599\n",
            "iteration 261 :train_loss:1.348736047744751 val_loss1.3328683376312256\n",
            "iteration 262 :train_loss:1.3487317562103271 val_loss1.3328641653060913\n",
            "iteration 263 :train_loss:1.3487286567687988 val_loss1.3328605890274048\n",
            "iteration 264 :train_loss:1.3487248420715332 val_loss1.3328567743301392\n",
            "iteration 265 :train_loss:1.3487217426300049 val_loss1.3328531980514526\n",
            "iteration 266 :train_loss:1.348718523979187 val_loss1.3328497409820557\n",
            "iteration 267 :train_loss:1.3487149477005005 val_loss1.3328464031219482\n",
            "iteration 268 :train_loss:1.3487120866775513 val_loss1.3328428268432617\n",
            "iteration 269 :train_loss:1.3487086296081543 val_loss1.3328399658203125\n",
            "iteration 270 :train_loss:1.348705530166626 val_loss1.332836627960205\n",
            "iteration 271 :train_loss:1.3487029075622559 val_loss1.3328337669372559\n",
            "iteration 272 :train_loss:1.3487000465393066 val_loss1.332830548286438\n",
            "iteration 273 :train_loss:1.348697304725647 val_loss1.3328280448913574\n",
            "iteration 274 :train_loss:1.3486945629119873 val_loss1.3328251838684082\n",
            "iteration 275 :train_loss:1.348691701889038 val_loss1.332822561264038\n",
            "iteration 276 :train_loss:1.3486896753311157 val_loss1.3328197002410889\n",
            "iteration 277 :train_loss:1.3486872911453247 val_loss1.3328170776367188\n",
            "iteration 278 :train_loss:1.3486851453781128 val_loss1.3328149318695068\n",
            "iteration 279 :train_loss:1.3486831188201904 val_loss1.3328121900558472\n",
            "iteration 280 :train_loss:1.3486807346343994 val_loss1.3328099250793457\n",
            "iteration 281 :train_loss:1.3486783504486084 val_loss1.3328076601028442\n",
            "iteration 282 :train_loss:1.3486764430999756 val_loss1.3328052759170532\n",
            "iteration 283 :train_loss:1.3486735820770264 val_loss1.3328031301498413\n",
            "iteration 284 :train_loss:1.3486721515655518 val_loss1.3328009843826294\n",
            "iteration 285 :train_loss:1.3486701250076294 val_loss1.3327988386154175\n",
            "iteration 286 :train_loss:1.3486683368682861 val_loss1.3327966928482056\n",
            "iteration 287 :train_loss:1.3486664295196533 val_loss1.3327946662902832\n",
            "iteration 288 :train_loss:1.348664402961731 val_loss1.3327929973602295\n",
            "iteration 289 :train_loss:1.3486627340316772 val_loss1.3327909708023071\n",
            "iteration 290 :train_loss:1.348660945892334 val_loss1.3327891826629639\n",
            "iteration 291 :train_loss:1.3486589193344116 val_loss1.332787275314331\n",
            "iteration 292 :train_loss:1.3486576080322266 val_loss1.3327853679656982\n",
            "iteration 293 :train_loss:1.3486560583114624 val_loss1.332783818244934\n",
            "iteration 294 :train_loss:1.34865403175354 val_loss1.3327817916870117\n",
            "iteration 295 :train_loss:1.3486524820327759 val_loss1.3327800035476685\n",
            "iteration 296 :train_loss:1.3486509323120117 val_loss1.3327785730361938\n",
            "iteration 297 :train_loss:1.3486496210098267 val_loss1.3327771425247192\n",
            "iteration 298 :train_loss:1.3486478328704834 val_loss1.332775592803955\n",
            "iteration 299 :train_loss:1.3486467599868774 val_loss1.332774043083191\n",
            "iteration 300 :train_loss:1.3486453294754028 val_loss1.3327722549438477\n",
            "iteration 301 :train_loss:1.3486440181732178 val_loss1.332770824432373\n",
            "iteration 302 :train_loss:1.3486425876617432 val_loss1.3327692747116089\n",
            "iteration 303 :train_loss:1.3486411571502686 val_loss1.3327679634094238\n",
            "iteration 304 :train_loss:1.3486398458480835 val_loss1.3327665328979492\n",
            "iteration 305 :train_loss:1.348638892173767 val_loss1.332764983177185\n",
            "iteration 306 :train_loss:1.3486371040344238 val_loss1.332763671875\n",
            "iteration 307 :train_loss:1.3486357927322388 val_loss1.3327624797821045\n",
            "iteration 308 :train_loss:1.3486348390579224 val_loss1.3327609300613403\n",
            "iteration 309 :train_loss:1.3486336469650269 val_loss1.332759976387024\n",
            "iteration 310 :train_loss:1.348632574081421 val_loss1.3327585458755493\n",
            "iteration 311 :train_loss:1.3486313819885254 val_loss1.3327571153640747\n",
            "iteration 312 :train_loss:1.3486298322677612 val_loss1.3327556848526\n",
            "iteration 313 :train_loss:1.3486287593841553 val_loss1.3327544927597046\n",
            "iteration 314 :train_loss:1.3486276865005493 val_loss1.332753300666809\n",
            "iteration 315 :train_loss:1.348626732826233 val_loss1.3327522277832031\n",
            "iteration 316 :train_loss:1.3486253023147583 val_loss1.3327507972717285\n",
            "iteration 317 :train_loss:1.3486242294311523 val_loss1.332749843597412\n",
            "iteration 318 :train_loss:1.3486231565475464 val_loss1.3327484130859375\n",
            "iteration 319 :train_loss:1.348622441291809 val_loss1.3327475786209106\n",
            "iteration 320 :train_loss:1.348621129989624 val_loss1.3327462673187256\n",
            "iteration 321 :train_loss:1.3486204147338867 val_loss1.3327449560165405\n",
            "iteration 322 :train_loss:1.3486193418502808 val_loss1.3327438831329346\n",
            "iteration 323 :train_loss:1.3486181497573853 val_loss1.3327428102493286\n",
            "iteration 324 :train_loss:1.348617434501648 val_loss1.3327417373657227\n",
            "iteration 325 :train_loss:1.3486164808273315 val_loss1.3327407836914062\n",
            "iteration 326 :train_loss:1.348615050315857 val_loss1.3327398300170898\n",
            "iteration 327 :train_loss:1.3486143350601196 val_loss1.332738995552063\n",
            "iteration 328 :train_loss:1.3486133813858032 val_loss1.3327378034591675\n",
            "iteration 329 :train_loss:1.3486123085021973 val_loss1.3327367305755615\n",
            "iteration 330 :train_loss:1.3486112356185913 val_loss1.332735538482666\n",
            "iteration 331 :train_loss:1.3486104011535645 val_loss1.33273446559906\n",
            "iteration 332 :train_loss:1.3486098051071167 val_loss1.3327335119247437\n",
            "iteration 333 :train_loss:1.348608374595642 val_loss1.3327325582504272\n",
            "iteration 334 :train_loss:1.3486077785491943 val_loss1.3327314853668213\n",
            "iteration 335 :train_loss:1.3486067056655884 val_loss1.3327306509017944\n",
            "iteration 336 :train_loss:1.3486067056655884 val_loss1.3327298164367676\n",
            "iteration 337 :train_loss:1.3486050367355347 val_loss1.3327288627624512\n",
            "iteration 338 :train_loss:1.3486040830612183 val_loss1.3327277898788452\n",
            "iteration 339 :train_loss:1.3486032485961914 val_loss1.3327269554138184\n",
            "iteration 340 :train_loss:1.3486028909683228 val_loss1.332726001739502\n",
            "iteration 341 :train_loss:1.3486021757125854 val_loss1.332724928855896\n",
            "iteration 342 :train_loss:1.3486013412475586 val_loss1.3327243328094482\n",
            "iteration 343 :train_loss:1.3486000299453735 val_loss1.3327230215072632\n",
            "iteration 344 :train_loss:1.3485991954803467 val_loss1.3327223062515259\n",
            "iteration 345 :train_loss:1.348598599433899 val_loss1.332721471786499\n",
            "iteration 346 :train_loss:1.348597764968872 val_loss1.3327206373214722\n",
            "iteration 347 :train_loss:1.3485972881317139 val_loss1.3327195644378662\n",
            "iteration 348 :train_loss:1.3485963344573975 val_loss1.332718849182129\n",
            "iteration 349 :train_loss:1.3485954999923706 val_loss1.332718014717102\n",
            "iteration 350 :train_loss:1.348594069480896 val_loss1.3327171802520752\n",
            "iteration 351 :train_loss:1.3485937118530273 val_loss1.3327162265777588\n",
            "iteration 352 :train_loss:1.3485934734344482 val_loss1.3327152729034424\n",
            "iteration 353 :train_loss:1.3485926389694214 val_loss1.332714557647705\n",
            "iteration 354 :train_loss:1.348591685295105 val_loss1.3327137231826782\n",
            "iteration 355 :train_loss:1.3485908508300781 val_loss1.3327128887176514\n",
            "iteration 356 :train_loss:1.3485898971557617 val_loss1.332711935043335\n",
            "iteration 357 :train_loss:1.348589301109314 val_loss1.3327113389968872\n",
            "iteration 358 :train_loss:1.3485888242721558 val_loss1.3327105045318604\n",
            "iteration 359 :train_loss:1.3485883474349976 val_loss1.3327096700668335\n",
            "iteration 360 :train_loss:1.348587155342102 val_loss1.3327088356018066\n",
            "iteration 361 :train_loss:1.3485863208770752 val_loss1.3327080011367798\n",
            "iteration 362 :train_loss:1.3485863208770752 val_loss1.3327072858810425\n",
            "iteration 363 :train_loss:1.3485853672027588 val_loss1.3327065706253052\n",
            "iteration 364 :train_loss:1.3485842943191528 val_loss1.3327056169509888\n",
            "iteration 365 :train_loss:1.3485835790634155 val_loss1.3327049016952515\n",
            "iteration 366 :train_loss:1.3485829830169678 val_loss1.3327041864395142\n",
            "iteration 367 :train_loss:1.3485822677612305 val_loss1.3327035903930664\n",
            "iteration 368 :train_loss:1.3485816717147827 val_loss1.33270263671875\n",
            "iteration 369 :train_loss:1.3485804796218872 val_loss1.3327019214630127\n",
            "iteration 370 :train_loss:1.348580241203308 val_loss1.3327012062072754\n",
            "iteration 371 :train_loss:1.348580002784729 val_loss1.332700490951538\n",
            "iteration 372 :train_loss:1.3485788106918335 val_loss1.3326996564865112\n",
            "iteration 373 :train_loss:1.3485782146453857 val_loss1.3326990604400635\n",
            "iteration 374 :train_loss:1.3485773801803589 val_loss1.3326982259750366\n",
            "iteration 375 :train_loss:1.3485767841339111 val_loss1.3326976299285889\n",
            "iteration 376 :train_loss:1.3485764265060425 val_loss1.3326969146728516\n",
            "iteration 377 :train_loss:1.3485759496688843 val_loss1.3326960802078247\n",
            "iteration 378 :train_loss:1.3485747575759888 val_loss1.3326953649520874\n",
            "iteration 379 :train_loss:1.3485745191574097 val_loss1.33269464969635\n",
            "iteration 380 :train_loss:1.3485736846923828 val_loss1.3326940536499023\n",
            "iteration 381 :train_loss:1.3485729694366455 val_loss1.332693338394165\n",
            "iteration 382 :train_loss:1.3485722541809082 val_loss1.3326923847198486\n",
            "iteration 383 :train_loss:1.3485722541809082 val_loss1.3326919078826904\n",
            "iteration 384 :train_loss:1.3485711812973022 val_loss1.3326913118362427\n",
            "iteration 385 :train_loss:1.3485705852508545 val_loss1.3326904773712158\n",
            "iteration 386 :train_loss:1.3485697507858276 val_loss1.3326895236968994\n",
            "iteration 387 :train_loss:1.3485695123672485 val_loss1.3326892852783203\n",
            "iteration 388 :train_loss:1.3485687971115112 val_loss1.3326884508132935\n",
            "iteration 389 :train_loss:1.3485678434371948 val_loss1.3326877355575562\n",
            "iteration 390 :train_loss:1.3485674858093262 val_loss1.3326871395111084\n",
            "iteration 391 :train_loss:1.3485668897628784 val_loss1.3326866626739502\n",
            "iteration 392 :train_loss:1.3485661745071411 val_loss1.3326858282089233\n",
            "iteration 393 :train_loss:1.3485655784606934 val_loss1.3326852321624756\n",
            "iteration 394 :train_loss:1.348564863204956 val_loss1.3326843976974487\n",
            "iteration 395 :train_loss:1.3485642671585083 val_loss1.332683801651001\n",
            "iteration 396 :train_loss:1.348563551902771 val_loss1.3326829671859741\n",
            "iteration 397 :train_loss:1.3485631942749023 val_loss1.3326823711395264\n",
            "iteration 398 :train_loss:1.3485628366470337 val_loss1.3326817750930786\n",
            "iteration 400 :train_loss:1.34856116771698 val_loss1.3326808214187622\n",
            "iteration 401 :train_loss:1.3485615253448486 val_loss1.3326797485351562\n",
            "iteration 402 :train_loss:1.3485605716705322 val_loss1.332679271697998\n",
            "iteration 403 :train_loss:1.3485596179962158 val_loss1.3326786756515503\n",
            "iteration 404 :train_loss:1.3485591411590576 val_loss1.3326780796051025\n",
            "iteration 405 :train_loss:1.3485586643218994 val_loss1.3326774835586548\n",
            "iteration 406 :train_loss:1.3485581874847412 val_loss1.332676887512207\n",
            "iteration 407 :train_loss:1.3485575914382935 val_loss1.3326764106750488\n",
            "iteration 408 :train_loss:1.3485568761825562 val_loss1.3326756954193115\n",
            "iteration 409 :train_loss:1.3485565185546875 val_loss1.3326748609542847\n",
            "iteration 410 :train_loss:1.3485558032989502 val_loss1.3326743841171265\n",
            "iteration 411 :train_loss:1.348555564880371 val_loss1.3326739072799683\n",
            "iteration 412 :train_loss:1.3485543727874756 val_loss1.3326730728149414\n",
            "iteration 413 :train_loss:1.3485537767410278 val_loss1.3326725959777832\n",
            "iteration 414 :train_loss:1.3485537767410278 val_loss1.3326719999313354\n",
            "iteration 415 :train_loss:1.348552942276001 val_loss1.3326712846755981\n",
            "iteration 416 :train_loss:1.3485523462295532 val_loss1.3326709270477295\n",
            "iteration 417 :train_loss:1.3485523462295532 val_loss1.3326702117919922\n",
            "iteration 418 :train_loss:1.3485512733459473 val_loss1.3326696157455444\n",
            "iteration 419 :train_loss:1.3485509157180786 val_loss1.3326690196990967\n",
            "iteration 420 :train_loss:1.3485503196716309 val_loss1.332668423652649\n",
            "iteration 421 :train_loss:1.3485500812530518 val_loss1.3326680660247803\n",
            "iteration 422 :train_loss:1.3485498428344727 val_loss1.3326674699783325\n",
            "iteration 423 :train_loss:1.3485486507415771 val_loss1.3326668739318848\n",
            "iteration 424 :train_loss:1.348548173904419 val_loss1.3326661586761475\n",
            "iteration 425 :train_loss:1.3485476970672607 val_loss1.3326654434204102\n",
            "iteration 426 :train_loss:1.3485474586486816 val_loss1.332665205001831\n",
            "iteration 427 :train_loss:1.3485466241836548 val_loss1.3326646089553833\n",
            "iteration 428 :train_loss:1.3485463857650757 val_loss1.3326640129089355\n",
            "iteration 429 :train_loss:1.3485459089279175 val_loss1.3326632976531982\n",
            "iteration 430 :train_loss:1.3485451936721802 val_loss1.33266282081604\n",
            "iteration 431 :train_loss:1.3485448360443115 val_loss1.3326624631881714\n",
            "iteration 432 :train_loss:1.3485443592071533 val_loss1.3326618671417236\n",
            "iteration 433 :train_loss:1.348543643951416 val_loss1.3326611518859863\n",
            "iteration 434 :train_loss:1.3485432863235474 val_loss1.3326607942581177\n",
            "iteration 435 :train_loss:1.3485424518585205 val_loss1.3326599597930908\n",
            "iteration 436 :train_loss:1.3485420942306519 val_loss1.3326597213745117\n",
            "iteration 437 :train_loss:1.3485413789749146 val_loss1.3326588869094849\n",
            "iteration 438 :train_loss:1.348541498184204 val_loss1.3326584100723267\n",
            "iteration 439 :train_loss:1.3485407829284668 val_loss1.3326579332351685\n",
            "iteration 440 :train_loss:1.3485406637191772 val_loss1.3326574563980103\n",
            "iteration 441 :train_loss:1.3485400676727295 val_loss1.3326568603515625\n",
            "iteration 442 :train_loss:1.3485392332077026 val_loss1.3326562643051147\n",
            "iteration 443 :train_loss:1.3485386371612549 val_loss1.332655668258667\n",
            "iteration 444 :train_loss:1.3485385179519653 val_loss1.332655429840088\n",
            "iteration 445 :train_loss:1.3485380411148071 val_loss1.3326548337936401\n",
            "iteration 446 :train_loss:1.3485373258590698 val_loss1.3326542377471924\n",
            "iteration 447 :train_loss:1.3485366106033325 val_loss1.3326536417007446\n",
            "iteration 448 :train_loss:1.3485368490219116 val_loss1.3326534032821655\n",
            "iteration 449 :train_loss:1.3485362529754639 val_loss1.3326528072357178\n",
            "iteration 450 :train_loss:1.348535418510437 val_loss1.33265221118927\n",
            "iteration 451 :train_loss:1.348535418510437 val_loss1.3326518535614014\n",
            "iteration 452 :train_loss:1.3485345840454102 val_loss1.332651138305664\n",
            "iteration 453 :train_loss:1.348534107208252 val_loss1.3326506614685059\n",
            "iteration 454 :train_loss:1.3485339879989624 val_loss1.3326501846313477\n",
            "iteration 455 :train_loss:1.3485333919525146 val_loss1.3326495885849\n",
            "iteration 456 :train_loss:1.3485325574874878 val_loss1.3326491117477417\n",
            "iteration 457 :train_loss:1.3485325574874878 val_loss1.3326486349105835\n",
            "iteration 458 :train_loss:1.34853196144104 val_loss1.3326483964920044\n",
            "iteration 459 :train_loss:1.348531723022461 val_loss1.332647681236267\n",
            "iteration 460 :train_loss:1.3485311269760132 val_loss1.3326473236083984\n",
            "iteration 461 :train_loss:1.3485305309295654 val_loss1.3326467275619507\n",
            "iteration 462 :train_loss:1.3485302925109863 val_loss1.3326462507247925\n",
            "iteration 463 :train_loss:1.3485296964645386 val_loss1.3326458930969238\n",
            "iteration 464 :train_loss:1.3485294580459595 val_loss1.3326451778411865\n",
            "iteration 465 :train_loss:1.3485288619995117 val_loss1.3326449394226074\n",
            "iteration 466 :train_loss:1.348528504371643 val_loss1.3326442241668701\n",
            "iteration 467 :train_loss:1.3485276699066162 val_loss1.3326438665390015\n",
            "iteration 468 :train_loss:1.3485276699066162 val_loss1.3326435089111328\n",
            "iteration 469 :train_loss:1.348526954650879 val_loss1.332642912864685\n",
            "iteration 470 :train_loss:1.3485267162322998 val_loss1.3326423168182373\n",
            "iteration 471 :train_loss:1.348526120185852 val_loss1.332641839981079\n",
            "iteration 472 :train_loss:1.3485260009765625 val_loss1.3326414823532104\n",
            "iteration 473 :train_loss:1.3485256433486938 val_loss1.3326410055160522\n",
            "iteration 474 :train_loss:1.348525047302246 val_loss1.3326404094696045\n",
            "iteration 475 :train_loss:1.3485242128372192 val_loss1.3326404094696045\n",
            "iteration 476 :train_loss:1.3485243320465088 val_loss1.3326399326324463\n",
            "iteration 477 :train_loss:1.3485236167907715 val_loss1.332639217376709\n",
            "iteration 478 :train_loss:1.3485236167907715 val_loss1.3326387405395508\n",
            "iteration 479 :train_loss:1.3485231399536133 val_loss1.332638144493103\n",
            "iteration 480 :train_loss:1.348522424697876 val_loss1.3326376676559448\n",
            "iteration 481 :train_loss:1.3485221862792969 val_loss1.3326375484466553\n",
            "iteration 482 :train_loss:1.3485217094421387 val_loss1.3326369524002075\n",
            "iteration 483 :train_loss:1.34852135181427 val_loss1.3326365947723389\n",
            "iteration 484 :train_loss:1.3485209941864014 val_loss1.3326361179351807\n",
            "iteration 485 :train_loss:1.348520278930664 val_loss1.332635521888733\n",
            "iteration 486 :train_loss:1.348520278930664 val_loss1.3326350450515747\n",
            "iteration 487 :train_loss:1.3485194444656372 val_loss1.3326350450515747\n",
            "iteration 488 :train_loss:1.3485193252563477 val_loss1.3326340913772583\n",
            "iteration 489 :train_loss:1.3485187292099 val_loss1.3326338529586792\n",
            "iteration 490 :train_loss:1.3485182523727417 val_loss1.3326336145401\n",
            "iteration 491 :train_loss:1.3485180139541626 val_loss1.332633137702942\n",
            "iteration 492 :train_loss:1.3485177755355835 val_loss1.3326326608657837\n",
            "iteration 493 :train_loss:1.3485170602798462 val_loss1.332632064819336\n",
            "iteration 494 :train_loss:1.3485170602798462 val_loss1.3326317071914673\n",
            "iteration 495 :train_loss:1.348516821861267 val_loss1.332631230354309\n",
            "iteration 496 :train_loss:1.3485158681869507 val_loss1.33263099193573\n",
            "iteration 497 :train_loss:1.3485158681869507 val_loss1.3326303958892822\n",
            "iteration 498 :train_loss:1.3485156297683716 val_loss1.3326301574707031\n",
            "iteration 499 :train_loss:1.3485147953033447 val_loss1.3326295614242554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "__E1DN-puvfI",
        "outputId": "e6ac5d2f-3138-4a52-d086-17a0b0bf5d45"
      },
      "source": [
        "plotLearningCurve()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayFA0CNzu2XU",
        "outputId": "4af5be3d-5ec7-4f8b-b90a-6dfa670151c6"
      },
      "source": [
        "predY=predict(parameters, test_X.T)\n",
        "MAPE = MAPE(test_Y,predY)\n",
        "print(\"MAPE:\",MAPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2.0654123 2.0676706 2.059299  ... 2.0749414 2.0839307 2.0655203]], shape=(1, 3000), dtype=float32)\n",
            "MAPE: tf.Tensor(60.697754, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udGxB14DvUiz",
        "outputId": "72c3eb6f-6cb8-404f-f633-4b4312c8123b"
      },
      "source": [
        "iterations = 500\n",
        "parameters, history = create_nn_model(train_X.T, train_Y.T, 50, val_X.T, val_Y.T, iterations, 0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 :train_loss:5.644177436828613 val_loss5.630143165588379\n",
            "iteration 1 :train_loss:5.473613262176514 val_loss5.459537029266357\n",
            "iteration 2 :train_loss:5.309814929962158 val_loss5.2957000732421875\n",
            "iteration 3 :train_loss:5.152515888214111 val_loss5.138360977172852\n",
            "iteration 4 :train_loss:5.001450538635254 val_loss4.987259864807129\n",
            "iteration 5 :train_loss:4.8563761711120605 val_loss4.842148780822754\n",
            "iteration 6 :train_loss:4.71705436706543 val_loss4.702790260314941\n",
            "iteration 7 :train_loss:4.583254337310791 val_loss4.568953990936279\n",
            "iteration 8 :train_loss:4.454756736755371 val_loss4.4404215812683105\n",
            "iteration 9 :train_loss:4.3313517570495605 val_loss4.316983222961426\n",
            "iteration 10 :train_loss:4.212837219238281 val_loss4.1984333992004395\n",
            "iteration 11 :train_loss:4.099016189575195 val_loss4.0845818519592285\n",
            "iteration 12 :train_loss:3.9897048473358154 val_loss3.975238084793091\n",
            "iteration 13 :train_loss:3.8847265243530273 val_loss3.870224952697754\n",
            "iteration 14 :train_loss:3.783902883529663 val_loss3.7693727016448975\n",
            "iteration 15 :train_loss:3.687073230743408 val_loss3.672513008117676\n",
            "iteration 16 :train_loss:3.594081401824951 val_loss3.5794904232025146\n",
            "iteration 17 :train_loss:3.504770040512085 val_loss3.4901504516601562\n",
            "iteration 18 :train_loss:3.4189977645874023 val_loss3.4043490886688232\n",
            "iteration 19 :train_loss:3.3366217613220215 val_loss3.3219447135925293\n",
            "iteration 20 :train_loss:3.257509231567383 val_loss3.242804527282715\n",
            "iteration 21 :train_loss:3.181530237197876 val_loss3.1667981147766113\n",
            "iteration 22 :train_loss:3.1085610389709473 val_loss3.0938022136688232\n",
            "iteration 23 :train_loss:3.038480520248413 val_loss3.023696184158325\n",
            "iteration 24 :train_loss:2.9711782932281494 val_loss2.9563677310943604\n",
            "iteration 25 :train_loss:2.906540632247925 val_loss2.8917057514190674\n",
            "iteration 26 :train_loss:2.84446382522583 val_loss2.8296051025390625\n",
            "iteration 27 :train_loss:2.784846544265747 val_loss2.7699646949768066\n",
            "iteration 28 :train_loss:2.727592945098877 val_loss2.712686777114868\n",
            "iteration 29 :train_loss:2.6726083755493164 val_loss2.6576783657073975\n",
            "iteration 30 :train_loss:2.6198017597198486 val_loss2.6048500537872314\n",
            "iteration 31 :train_loss:2.5690886974334717 val_loss2.554114818572998\n",
            "iteration 32 :train_loss:2.5203866958618164 val_loss2.5053906440734863\n",
            "iteration 33 :train_loss:2.473614454269409 val_loss2.4585983753204346\n",
            "iteration 34 :train_loss:2.428697347640991 val_loss2.413660764694214\n",
            "iteration 35 :train_loss:2.3855621814727783 val_loss2.370504856109619\n",
            "iteration 36 :train_loss:2.3441359996795654 val_loss2.3290600776672363\n",
            "iteration 37 :train_loss:2.3043549060821533 val_loss2.289259910583496\n",
            "iteration 38 :train_loss:2.2661526203155518 val_loss2.251037836074829\n",
            "iteration 39 :train_loss:2.229466438293457 val_loss2.2143328189849854\n",
            "iteration 40 :train_loss:2.194234848022461 val_loss2.179084539413452\n",
            "iteration 41 :train_loss:2.1604042053222656 val_loss2.145235300064087\n",
            "iteration 42 :train_loss:2.127915620803833 val_loss2.112729787826538\n",
            "iteration 43 :train_loss:2.0967180728912354 val_loss2.081514835357666\n",
            "iteration 44 :train_loss:2.0667593479156494 val_loss2.0515401363372803\n",
            "iteration 45 :train_loss:2.0379910469055176 val_loss2.0227558612823486\n",
            "iteration 46 :train_loss:2.010366678237915 val_loss1.9951163530349731\n",
            "iteration 47 :train_loss:1.9838407039642334 val_loss1.9685745239257812\n",
            "iteration 48 :train_loss:1.9583700895309448 val_loss1.9430880546569824\n",
            "iteration 49 :train_loss:1.9339120388031006 val_loss1.9186151027679443\n",
            "iteration 50 :train_loss:1.9104269742965698 val_loss1.8951163291931152\n",
            "iteration 51 :train_loss:1.8878765106201172 val_loss1.8725522756576538\n",
            "iteration 52 :train_loss:1.8662241697311401 val_loss1.8508861064910889\n",
            "iteration 53 :train_loss:1.8454334735870361 val_loss1.8300822973251343\n",
            "iteration 54 :train_loss:1.8254711627960205 val_loss1.8101072311401367\n",
            "iteration 55 :train_loss:1.8063054084777832 val_loss1.7909283638000488\n",
            "iteration 56 :train_loss:1.787902593612671 val_loss1.7725132703781128\n",
            "iteration 57 :train_loss:1.7702335119247437 val_loss1.7548322677612305\n",
            "iteration 58 :train_loss:1.7532687187194824 val_loss1.7378562688827515\n",
            "iteration 59 :train_loss:1.7369815111160278 val_loss1.721557378768921\n",
            "iteration 60 :train_loss:1.7213449478149414 val_loss1.705909013748169\n",
            "iteration 61 :train_loss:1.7063318490982056 val_loss1.6908849477767944\n",
            "iteration 62 :train_loss:1.6919188499450684 val_loss1.676461100578308\n",
            "iteration 63 :train_loss:1.6780809164047241 val_loss1.6626132726669312\n",
            "iteration 64 :train_loss:1.6647963523864746 val_loss1.6493184566497803\n",
            "iteration 65 :train_loss:1.6520429849624634 val_loss1.6365553140640259\n",
            "iteration 66 :train_loss:1.63979971408844 val_loss1.6243025064468384\n",
            "iteration 67 :train_loss:1.6280467510223389 val_loss1.612539529800415\n",
            "iteration 68 :train_loss:1.6167634725570679 val_loss1.6012474298477173\n",
            "iteration 69 :train_loss:1.6059322357177734 val_loss1.5904072523117065\n",
            "iteration 70 :train_loss:1.5955350399017334 val_loss1.5800009965896606\n",
            "iteration 71 :train_loss:1.5855543613433838 val_loss1.5700116157531738\n",
            "iteration 72 :train_loss:1.575973391532898 val_loss1.5604225397109985\n",
            "iteration 73 :train_loss:1.566777229309082 val_loss1.5512181520462036\n",
            "iteration 74 :train_loss:1.5579495429992676 val_loss1.5423824787139893\n",
            "iteration 75 :train_loss:1.5494757890701294 val_loss1.5339016914367676\n",
            "iteration 76 :train_loss:1.541343331336975 val_loss1.5257610082626343\n",
            "iteration 77 :train_loss:1.5335367918014526 val_loss1.5179469585418701\n",
            "iteration 78 :train_loss:1.5260440111160278 val_loss1.5104469060897827\n",
            "iteration 79 :train_loss:1.5188519954681396 val_loss1.5032483339309692\n",
            "iteration 80 :train_loss:1.511949062347412 val_loss1.4963390827178955\n",
            "iteration 81 :train_loss:1.5053247213363647 val_loss1.4897072315216064\n",
            "iteration 82 :train_loss:1.4989659786224365 val_loss1.4833424091339111\n",
            "iteration 83 :train_loss:1.4928630590438843 val_loss1.47723388671875\n",
            "iteration 84 :train_loss:1.4870067834854126 val_loss1.471371054649353\n",
            "iteration 85 :train_loss:1.4813858270645142 val_loss1.4657444953918457\n",
            "iteration 86 :train_loss:1.4759917259216309 val_loss1.4603440761566162\n",
            "iteration 87 :train_loss:1.4708154201507568 val_loss1.455161690711975\n",
            "iteration 88 :train_loss:1.4658472537994385 val_loss1.4501885175704956\n",
            "iteration 89 :train_loss:1.4610791206359863 val_loss1.4454154968261719\n",
            "iteration 90 :train_loss:1.456504464149475 val_loss1.4408352375030518\n",
            "iteration 91 :train_loss:1.4521139860153198 val_loss1.4364395141601562\n",
            "iteration 92 :train_loss:1.4479007720947266 val_loss1.4322214126586914\n",
            "iteration 93 :train_loss:1.443858027458191 val_loss1.428173542022705\n",
            "iteration 94 :train_loss:1.4399785995483398 val_loss1.4242894649505615\n",
            "iteration 95 :train_loss:1.4362560510635376 val_loss1.4205623865127563\n",
            "iteration 96 :train_loss:1.4326839447021484 val_loss1.4169859886169434\n",
            "iteration 97 :train_loss:1.429256558418274 val_loss1.413554072380066\n",
            "iteration 98 :train_loss:1.4259679317474365 val_loss1.4102610349655151\n",
            "iteration 99 :train_loss:1.4228124618530273 val_loss1.4071015119552612\n",
            "iteration 100 :train_loss:1.4197845458984375 val_loss1.4040696620941162\n",
            "iteration 101 :train_loss:1.4168798923492432 val_loss1.401160717010498\n",
            "iteration 102 :train_loss:1.4140925407409668 val_loss1.3983696699142456\n",
            "iteration 103 :train_loss:1.411418080329895 val_loss1.3956917524337769\n",
            "iteration 104 :train_loss:1.4088521003723145 val_loss1.3931224346160889\n",
            "iteration 105 :train_loss:1.4063905477523804 val_loss1.3906570672988892\n",
            "iteration 106 :train_loss:1.404029130935669 val_loss1.388291835784912\n",
            "iteration 107 :train_loss:1.4017633199691772 val_loss1.3860225677490234\n",
            "iteration 108 :train_loss:1.3995893001556396 val_loss1.3838456869125366\n",
            "iteration 109 :train_loss:1.3975038528442383 val_loss1.3817569017410278\n",
            "iteration 110 :train_loss:1.395503044128418 val_loss1.3797528743743896\n",
            "iteration 111 :train_loss:1.3935840129852295 val_loss1.3778305053710938\n",
            "iteration 112 :train_loss:1.3917423486709595 val_loss1.375986099243164\n",
            "iteration 113 :train_loss:1.3899757862091064 val_loss1.374216914176941\n",
            "iteration 114 :train_loss:1.3882817029953003 val_loss1.3725194931030273\n",
            "iteration 115 :train_loss:1.3866558074951172 val_loss1.3708912134170532\n",
            "iteration 116 :train_loss:1.385096788406372 val_loss1.3693290948867798\n",
            "iteration 117 :train_loss:1.3836003541946411 val_loss1.3678306341171265\n",
            "iteration 118 :train_loss:1.3821654319763184 val_loss1.3663933277130127\n",
            "iteration 119 :train_loss:1.380789875984192 val_loss1.3650141954421997\n",
            "iteration 120 :train_loss:1.3794691562652588 val_loss1.3636916875839233\n",
            "iteration 121 :train_loss:1.3782023191452026 val_loss1.3624221086502075\n",
            "iteration 122 :train_loss:1.376987338066101 val_loss1.3612054586410522\n",
            "iteration 123 :train_loss:1.3758227825164795 val_loss1.3600378036499023\n",
            "iteration 124 :train_loss:1.3747047185897827 val_loss1.3589178323745728\n",
            "iteration 125 :train_loss:1.3736321926116943 val_loss1.3578435182571411\n",
            "iteration 126 :train_loss:1.372604250907898 val_loss1.3568131923675537\n",
            "iteration 127 :train_loss:1.3716176748275757 val_loss1.355824589729309\n",
            "iteration 128 :train_loss:1.3706721067428589 val_loss1.3548765182495117\n",
            "iteration 129 :train_loss:1.3697644472122192 val_loss1.3539674282073975\n",
            "iteration 130 :train_loss:1.3688938617706299 val_loss1.3530949354171753\n",
            "iteration 131 :train_loss:1.3680589199066162 val_loss1.3522584438323975\n",
            "iteration 132 :train_loss:1.3672587871551514 val_loss1.3514559268951416\n",
            "iteration 133 :train_loss:1.3664907217025757 val_loss1.3506861925125122\n",
            "iteration 134 :train_loss:1.3657540082931519 val_loss1.3499480485916138\n",
            "iteration 135 :train_loss:1.3650479316711426 val_loss1.3492399454116821\n",
            "iteration 136 :train_loss:1.364370584487915 val_loss1.3485606908798218\n",
            "iteration 137 :train_loss:1.3637206554412842 val_loss1.3479094505310059\n",
            "iteration 138 :train_loss:1.3630974292755127 val_loss1.3472845554351807\n",
            "iteration 139 :train_loss:1.3624999523162842 val_loss1.3466854095458984\n",
            "iteration 140 :train_loss:1.3619266748428345 val_loss1.3461107015609741\n",
            "iteration 141 :train_loss:1.3613766431808472 val_loss1.3455595970153809\n",
            "iteration 142 :train_loss:1.3608498573303223 val_loss1.3450310230255127\n",
            "iteration 143 :train_loss:1.3603442907333374 val_loss1.3445243835449219\n",
            "iteration 144 :train_loss:1.3598593473434448 val_loss1.3440377712249756\n",
            "iteration 145 :train_loss:1.3593941926956177 val_loss1.3435713052749634\n",
            "iteration 146 :train_loss:1.3589485883712769 val_loss1.3431246280670166\n",
            "iteration 147 :train_loss:1.3585205078125 val_loss1.3426953554153442\n",
            "iteration 148 :train_loss:1.3581109046936035 val_loss1.3422839641571045\n",
            "iteration 149 :train_loss:1.3577171564102173 val_loss1.3418893814086914\n",
            "iteration 150 :train_loss:1.3573402166366577 val_loss1.3415111303329468\n",
            "iteration 151 :train_loss:1.356978416442871 val_loss1.341148018836975\n",
            "iteration 152 :train_loss:1.3566317558288574 val_loss1.3408000469207764\n",
            "iteration 153 :train_loss:1.3562986850738525 val_loss1.3404664993286133\n",
            "iteration 154 :train_loss:1.3559796810150146 val_loss1.3401461839675903\n",
            "iteration 155 :train_loss:1.3556740283966064 val_loss1.3398391008377075\n",
            "iteration 156 :train_loss:1.3553805351257324 val_loss1.3395448923110962\n",
            "iteration 157 :train_loss:1.3550993204116821 val_loss1.3392623662948608\n",
            "iteration 158 :train_loss:1.35482919216156 val_loss1.338991641998291\n",
            "iteration 159 :train_loss:1.354570984840393 val_loss1.3387318849563599\n",
            "iteration 160 :train_loss:1.3543226718902588 val_loss1.3384829759597778\n",
            "iteration 161 :train_loss:1.3540847301483154 val_loss1.338244080543518\n",
            "iteration 162 :train_loss:1.3538564443588257 val_loss1.3380149602890015\n",
            "iteration 163 :train_loss:1.3536375761032104 val_loss1.3377952575683594\n",
            "iteration 164 :train_loss:1.3534280061721802 val_loss1.3375844955444336\n",
            "iteration 165 :train_loss:1.353226661682129 val_loss1.3373825550079346\n",
            "iteration 166 :train_loss:1.3530339002609253 val_loss1.337188959121704\n",
            "iteration 167 :train_loss:1.352849006652832 val_loss1.3370029926300049\n",
            "iteration 168 :train_loss:1.352671504020691 val_loss1.3368250131607056\n",
            "iteration 169 :train_loss:1.3525012731552124 val_loss1.3366539478302002\n",
            "iteration 170 :train_loss:1.352338433265686 val_loss1.3364897966384888\n",
            "iteration 171 :train_loss:1.352182388305664 val_loss1.3363324403762817\n",
            "iteration 172 :train_loss:1.3520318269729614 val_loss1.336181879043579\n",
            "iteration 173 :train_loss:1.3518873453140259 val_loss1.336037278175354\n",
            "iteration 174 :train_loss:1.3517497777938843 val_loss1.3358988761901855\n",
            "iteration 175 :train_loss:1.351617693901062 val_loss1.3357657194137573\n",
            "iteration 176 :train_loss:1.3514904975891113 val_loss1.3356380462646484\n",
            "iteration 177 :train_loss:1.3513689041137695 val_loss1.3355158567428589\n",
            "iteration 178 :train_loss:1.3512523174285889 val_loss1.335398554801941\n",
            "iteration 179 :train_loss:1.3511406183242798 val_loss1.3352859020233154\n",
            "iteration 180 :train_loss:1.351033329963684 val_loss1.3351781368255615\n",
            "iteration 181 :train_loss:1.3509303331375122 val_loss1.335074543952942\n",
            "iteration 182 :train_loss:1.3508315086364746 val_loss1.334975004196167\n",
            "iteration 183 :train_loss:1.3507369756698608 val_loss1.334879994392395\n",
            "iteration 184 :train_loss:1.350646734237671 val_loss1.3347885608673096\n",
            "iteration 185 :train_loss:1.3505594730377197 val_loss1.3347009420394897\n",
            "iteration 186 :train_loss:1.3504756689071655 val_loss1.3346171379089355\n",
            "iteration 187 :train_loss:1.3503957986831665 val_loss1.3345364332199097\n",
            "iteration 188 :train_loss:1.3503191471099854 val_loss1.3344593048095703\n",
            "iteration 189 :train_loss:1.350245714187622 val_loss1.3343850374221802\n",
            "iteration 190 :train_loss:1.3501747846603394 val_loss1.3343138694763184\n",
            "iteration 191 :train_loss:1.350107192993164 val_loss1.3342458009719849\n",
            "iteration 192 :train_loss:1.350042462348938 val_loss1.3341805934906006\n",
            "iteration 193 :train_loss:1.3499798774719238 val_loss1.3341176509857178\n",
            "iteration 194 :train_loss:1.3499201536178589 val_loss1.334057331085205\n",
            "iteration 195 :train_loss:1.349863052368164 val_loss1.3339996337890625\n",
            "iteration 196 :train_loss:1.3498079776763916 val_loss1.3339442014694214\n",
            "iteration 197 :train_loss:1.3497546911239624 val_loss1.3338910341262817\n",
            "iteration 198 :train_loss:1.3497047424316406 val_loss1.333840012550354\n",
            "iteration 199 :train_loss:1.3496558666229248 val_loss1.3337912559509277\n",
            "iteration 200 :train_loss:1.349609613418579 val_loss1.3337442874908447\n",
            "iteration 201 :train_loss:1.349565029144287 val_loss1.333699345588684\n",
            "iteration 202 :train_loss:1.349522352218628 val_loss1.3336561918258667\n",
            "iteration 203 :train_loss:1.349481225013733 val_loss1.333614706993103\n",
            "iteration 204 :train_loss:1.3494422435760498 val_loss1.3335747718811035\n",
            "iteration 205 :train_loss:1.3494043350219727 val_loss1.3335367441177368\n",
            "iteration 206 :train_loss:1.3493680953979492 val_loss1.3335001468658447\n",
            "iteration 207 :train_loss:1.34933340549469 val_loss1.3334650993347168\n",
            "iteration 208 :train_loss:1.3492995500564575 val_loss1.333431363105774\n",
            "iteration 209 :train_loss:1.349267840385437 val_loss1.3333991765975952\n",
            "iteration 210 :train_loss:1.349237322807312 val_loss1.333367943763733\n",
            "iteration 211 :train_loss:1.349207878112793 val_loss1.3333381414413452\n",
            "iteration 212 :train_loss:1.3491793870925903 val_loss1.3333096504211426\n",
            "iteration 213 :train_loss:1.34915292263031 val_loss1.3332823514938354\n",
            "iteration 214 :train_loss:1.3491265773773193 val_loss1.3332560062408447\n",
            "iteration 215 :train_loss:1.3491015434265137 val_loss1.33323073387146\n",
            "iteration 216 :train_loss:1.3490777015686035 val_loss1.3332064151763916\n",
            "iteration 217 :train_loss:1.3490549325942993 val_loss1.3331831693649292\n",
            "iteration 218 :train_loss:1.3490327596664429 val_loss1.3331608772277832\n",
            "iteration 219 :train_loss:1.3490114212036133 val_loss1.3331395387649536\n",
            "iteration 220 :train_loss:1.3489912748336792 val_loss1.3331189155578613\n",
            "iteration 221 :train_loss:1.348972201347351 val_loss1.3330990076065063\n",
            "iteration 222 :train_loss:1.3489532470703125 val_loss1.3330801725387573\n",
            "iteration 223 :train_loss:1.3489352464675903 val_loss1.3330618143081665\n",
            "iteration 224 :train_loss:1.3489177227020264 val_loss1.3330447673797607\n",
            "iteration 225 :train_loss:1.3489015102386475 val_loss1.3330276012420654\n",
            "iteration 226 :train_loss:1.3488858938217163 val_loss1.3330117464065552\n",
            "iteration 227 :train_loss:1.348870873451233 val_loss1.332996129989624\n",
            "iteration 228 :train_loss:1.348855972290039 val_loss1.3329812288284302\n",
            "iteration 229 :train_loss:1.3488417863845825 val_loss1.3329670429229736\n",
            "iteration 230 :train_loss:1.3488284349441528 val_loss1.3329535722732544\n",
            "iteration 231 :train_loss:1.3488154411315918 val_loss1.3329402208328247\n",
            "iteration 232 :train_loss:1.3488028049468994 val_loss1.3329273462295532\n",
            "iteration 233 :train_loss:1.3487913608551025 val_loss1.3329153060913086\n",
            "iteration 234 :train_loss:1.3487799167633057 val_loss1.3329038619995117\n",
            "iteration 235 :train_loss:1.348768711090088 val_loss1.3328922986984253\n",
            "iteration 236 :train_loss:1.3487577438354492 val_loss1.3328814506530762\n",
            "iteration 237 :train_loss:1.3487478494644165 val_loss1.3328711986541748\n",
            "iteration 238 :train_loss:1.3487380743026733 val_loss1.332861304283142\n",
            "iteration 239 :train_loss:1.348728895187378 val_loss1.3328518867492676\n",
            "iteration 240 :train_loss:1.3487200736999512 val_loss1.3328428268432617\n",
            "iteration 241 :train_loss:1.3487114906311035 val_loss1.3328338861465454\n",
            "iteration 242 :train_loss:1.3487035036087036 val_loss1.3328254222869873\n",
            "iteration 243 :train_loss:1.3486948013305664 val_loss1.3328169584274292\n",
            "iteration 244 :train_loss:1.3486870527267456 val_loss1.332809329032898\n",
            "iteration 245 :train_loss:1.3486799001693726 val_loss1.3328015804290771\n",
            "iteration 246 :train_loss:1.3486727476119995 val_loss1.332794427871704\n",
            "iteration 247 :train_loss:1.3486655950546265 val_loss1.3327875137329102\n",
            "iteration 248 :train_loss:1.3486593961715698 val_loss1.3327805995941162\n",
            "iteration 249 :train_loss:1.348652958869934 val_loss1.332774043083191\n",
            "iteration 250 :train_loss:1.348646879196167 val_loss1.3327679634094238\n",
            "iteration 251 :train_loss:1.3486411571502686 val_loss1.3327620029449463\n",
            "iteration 252 :train_loss:1.3486360311508179 val_loss1.3327564001083374\n",
            "iteration 253 :train_loss:1.3486307859420776 val_loss1.3327507972717285\n",
            "iteration 254 :train_loss:1.3486253023147583 val_loss1.3327454328536987\n",
            "iteration 255 :train_loss:1.3486202955245972 val_loss1.332740306854248\n",
            "iteration 256 :train_loss:1.3486154079437256 val_loss1.3327354192733765\n",
            "iteration 257 :train_loss:1.348610520362854 val_loss1.3327305316925049\n",
            "iteration 258 :train_loss:1.348606824874878 val_loss1.332726001739502\n",
            "iteration 259 :train_loss:1.3486021757125854 val_loss1.332721471786499\n",
            "iteration 260 :train_loss:1.348597764968872 val_loss1.3327171802520752\n",
            "iteration 261 :train_loss:1.348594069480896 val_loss1.3327131271362305\n",
            "iteration 262 :train_loss:1.3485908508300781 val_loss1.3327089548110962\n",
            "iteration 263 :train_loss:1.3485864400863647 val_loss1.3327052593231201\n",
            "iteration 264 :train_loss:1.3485829830169678 val_loss1.3327019214630127\n",
            "iteration 265 :train_loss:1.3485796451568604 val_loss1.3326982259750366\n",
            "iteration 266 :train_loss:1.348576545715332 val_loss1.3326947689056396\n",
            "iteration 267 :train_loss:1.348573088645935 val_loss1.3326913118362427\n",
            "iteration 268 :train_loss:1.3485702276229858 val_loss1.3326884508132935\n",
            "iteration 269 :train_loss:1.3485671281814575 val_loss1.332685112953186\n",
            "iteration 270 :train_loss:1.3485642671585083 val_loss1.3326821327209473\n",
            "iteration 271 :train_loss:1.3485617637634277 val_loss1.332679271697998\n",
            "iteration 272 :train_loss:1.348559021949768 val_loss1.3326765298843384\n",
            "iteration 273 :train_loss:1.348556399345398 val_loss1.3326737880706787\n",
            "iteration 274 :train_loss:1.3485537767410278 val_loss1.3326711654663086\n",
            "iteration 275 :train_loss:1.3485515117645264 val_loss1.3326687812805176\n",
            "iteration 276 :train_loss:1.3485491275787354 val_loss1.332666277885437\n",
            "iteration 277 :train_loss:1.348547101020813 val_loss1.3326640129089355\n",
            "iteration 278 :train_loss:1.348544955253601 val_loss1.332661509513855\n",
            "iteration 279 :train_loss:1.3485429286956787 val_loss1.3326592445373535\n",
            "iteration 280 :train_loss:1.3485409021377563 val_loss1.3326574563980103\n",
            "iteration 281 :train_loss:1.3485386371612549 val_loss1.332655429840088\n",
            "iteration 282 :train_loss:1.3485372066497803 val_loss1.332653522491455\n",
            "iteration 283 :train_loss:1.3485348224639893 val_loss1.3326513767242432\n",
            "iteration 284 :train_loss:1.3485338687896729 val_loss1.3326494693756104\n",
            "iteration 285 :train_loss:1.3485318422317505 val_loss1.3326479196548462\n",
            "iteration 286 :train_loss:1.3485298156738281 val_loss1.3326460123062134\n",
            "iteration 287 :train_loss:1.348528504371643 val_loss1.3326444625854492\n",
            "iteration 288 :train_loss:1.3485268354415894 val_loss1.3326427936553955\n",
            "iteration 289 :train_loss:1.3485249280929565 val_loss1.3326411247253418\n",
            "iteration 290 :train_loss:1.3485238552093506 val_loss1.3326395750045776\n",
            "iteration 291 :train_loss:1.3485225439071655 val_loss1.3326380252838135\n",
            "iteration 292 :train_loss:1.348521113395691 val_loss1.3326367139816284\n",
            "iteration 293 :train_loss:1.3485193252563477 val_loss1.3326352834701538\n",
            "iteration 294 :train_loss:1.3485184907913208 val_loss1.3326338529586792\n",
            "iteration 295 :train_loss:1.3485172986984253 val_loss1.3326325416564941\n",
            "iteration 296 :train_loss:1.3485161066055298 val_loss1.3326311111450195\n",
            "iteration 297 :train_loss:1.3485149145126343 val_loss1.3326297998428345\n",
            "iteration 298 :train_loss:1.3485140800476074 val_loss1.3326287269592285\n",
            "iteration 299 :train_loss:1.3485124111175537 val_loss1.332627534866333\n",
            "iteration 300 :train_loss:1.3485113382339478 val_loss1.3326261043548584\n",
            "iteration 301 :train_loss:1.348510503768921 val_loss1.332625389099121\n",
            "iteration 302 :train_loss:1.348509430885315 val_loss1.3326239585876465\n",
            "iteration 303 :train_loss:1.3485084772109985 val_loss1.3326228857040405\n",
            "iteration 304 :train_loss:1.3485074043273926 val_loss1.3326220512390137\n",
            "iteration 305 :train_loss:1.3485060930252075 val_loss1.332620620727539\n",
            "iteration 306 :train_loss:1.3485056161880493 val_loss1.3326197862625122\n",
            "iteration 307 :train_loss:1.348504662513733 val_loss1.3326189517974854\n",
            "iteration 308 :train_loss:1.348503589630127 val_loss1.332617998123169\n",
            "iteration 309 :train_loss:1.3485028743743896 val_loss1.3326170444488525\n",
            "iteration 310 :train_loss:1.3485023975372314 val_loss1.3326160907745361\n",
            "iteration 311 :train_loss:1.348501443862915 val_loss1.3326151371002197\n",
            "iteration 312 :train_loss:1.3485004901885986 val_loss1.3326143026351929\n",
            "iteration 313 :train_loss:1.3485002517700195 val_loss1.332613468170166\n",
            "iteration 314 :train_loss:1.3484985828399658 val_loss1.3326127529144287\n",
            "iteration 315 :train_loss:1.3484985828399658 val_loss1.3326119184494019\n",
            "iteration 316 :train_loss:1.3484975099563599 val_loss1.3326112031936646\n",
            "iteration 317 :train_loss:1.348496913909912 val_loss1.3326102495193481\n",
            "iteration 318 :train_loss:1.3484961986541748 val_loss1.33260977268219\n",
            "iteration 319 :train_loss:1.3484948873519897 val_loss1.3326088190078735\n",
            "iteration 320 :train_loss:1.3484948873519897 val_loss1.3326079845428467\n",
            "iteration 321 :train_loss:1.3484939336776733 val_loss1.3326071500778198\n",
            "iteration 322 :train_loss:1.348493218421936 val_loss1.332606554031372\n",
            "iteration 323 :train_loss:1.3484925031661987 val_loss1.3326058387756348\n",
            "iteration 324 :train_loss:1.3484923839569092 val_loss1.3326051235198975\n",
            "iteration 325 :train_loss:1.3484911918640137 val_loss1.3326044082641602\n",
            "iteration 326 :train_loss:1.3484909534454346 val_loss1.3326036930084229\n",
            "iteration 327 :train_loss:1.3484903573989868 val_loss1.332603096961975\n",
            "iteration 328 :train_loss:1.3484901189804077 val_loss1.3326023817062378\n",
            "iteration 329 :train_loss:1.3484891653060913 val_loss1.3326020240783691\n",
            "iteration 330 :train_loss:1.3484883308410645 val_loss1.3326014280319214\n",
            "iteration 331 :train_loss:1.3484883308410645 val_loss1.3326008319854736\n",
            "iteration 332 :train_loss:1.3484877347946167 val_loss1.3326002359390259\n",
            "iteration 333 :train_loss:1.3484870195388794 val_loss1.332599401473999\n",
            "iteration 334 :train_loss:1.3484866619110107 val_loss1.3325988054275513\n",
            "iteration 335 :train_loss:1.3484857082366943 val_loss1.3325982093811035\n",
            "iteration 336 :train_loss:1.348486065864563 val_loss1.3325979709625244\n",
            "iteration 337 :train_loss:1.3484848737716675 val_loss1.3325973749160767\n",
            "iteration 338 :train_loss:1.3484842777252197 val_loss1.332596778869629\n",
            "iteration 339 :train_loss:1.3484842777252197 val_loss1.3325961828231812\n",
            "iteration 340 :train_loss:1.3484834432601929 val_loss1.3325955867767334\n",
            "iteration 341 :train_loss:1.3484834432601929 val_loss1.3325953483581543\n",
            "iteration 342 :train_loss:1.3484827280044556 val_loss1.3325945138931274\n",
            "iteration 343 :train_loss:1.3484820127487183 val_loss1.332594394683838\n",
            "iteration 344 :train_loss:1.3484817743301392 val_loss1.3325936794281006\n",
            "iteration 345 :train_loss:1.3484811782836914 val_loss1.3325930833816528\n",
            "iteration 346 :train_loss:1.3484807014465332 val_loss1.3325926065444946\n",
            "iteration 347 :train_loss:1.3484803438186646 val_loss1.332592248916626\n",
            "iteration 348 :train_loss:1.3484798669815063 val_loss1.3325916528701782\n",
            "iteration 349 :train_loss:1.3484793901443481 val_loss1.3325910568237305\n",
            "iteration 350 :train_loss:1.348479151725769 val_loss1.3325908184051514\n",
            "iteration 351 :train_loss:1.3484785556793213 val_loss1.3325902223587036\n",
            "iteration 352 :train_loss:1.3484781980514526 val_loss1.3325897455215454\n",
            "iteration 353 :train_loss:1.3484776020050049 val_loss1.3325892686843872\n",
            "iteration 354 :train_loss:1.3484777212142944 val_loss1.332588791847229\n",
            "iteration 355 :train_loss:1.3484768867492676 val_loss1.3325884342193604\n",
            "iteration 356 :train_loss:1.3484770059585571 val_loss1.3325878381729126\n",
            "iteration 357 :train_loss:1.3484759330749512 val_loss1.332587480545044\n",
            "iteration 358 :train_loss:1.3484762907028198 val_loss1.3325870037078857\n",
            "iteration 359 :train_loss:1.348475456237793 val_loss1.332586407661438\n",
            "iteration 360 :train_loss:1.3484745025634766 val_loss1.3325861692428589\n",
            "iteration 361 :train_loss:1.3484745025634766 val_loss1.3325856924057007\n",
            "iteration 362 :train_loss:1.348474144935608 val_loss1.332585334777832\n",
            "iteration 363 :train_loss:1.3484737873077393 val_loss1.3325852155685425\n",
            "iteration 364 :train_loss:1.3484735488891602 val_loss1.3325845003128052\n",
            "iteration 365 :train_loss:1.3484734296798706 val_loss1.332584023475647\n",
            "iteration 366 :train_loss:1.348472237586975 val_loss1.3325835466384888\n",
            "iteration 367 :train_loss:1.348471999168396 val_loss1.3325833082199097\n",
            "iteration 368 :train_loss:1.3484723567962646 val_loss1.3325828313827515\n",
            "iteration 369 :train_loss:1.348471999168396 val_loss1.3325824737548828\n",
            "iteration 370 :train_loss:1.3484712839126587 val_loss1.3325821161270142\n",
            "iteration 371 :train_loss:1.3484708070755005 val_loss1.3325812816619873\n",
            "iteration 372 :train_loss:1.3484708070755005 val_loss1.3325812816619873\n",
            "iteration 373 :train_loss:1.3484702110290527 val_loss1.3325806856155396\n",
            "iteration 374 :train_loss:1.3484700918197632 val_loss1.3325804471969604\n",
            "iteration 375 :train_loss:1.3484694957733154 val_loss1.3325800895690918\n",
            "iteration 376 :train_loss:1.3484693765640259 val_loss1.3325798511505127\n",
            "iteration 377 :train_loss:1.3484687805175781 val_loss1.332579255104065\n",
            "iteration 378 :train_loss:1.348468542098999 val_loss1.3325790166854858\n",
            "iteration 379 :train_loss:1.3484679460525513 val_loss1.3325785398483276\n",
            "iteration 380 :train_loss:1.3484679460525513 val_loss1.3325783014297485\n",
            "iteration 381 :train_loss:1.3484673500061035 val_loss1.3325777053833008\n",
            "iteration 382 :train_loss:1.348467230796814 val_loss1.3325775861740112\n",
            "iteration 383 :train_loss:1.3484666347503662 val_loss1.3325769901275635\n",
            "iteration 384 :train_loss:1.3484665155410767 val_loss1.3325766324996948\n",
            "iteration 385 :train_loss:1.3484662771224976 val_loss1.3325762748718262\n",
            "iteration 386 :train_loss:1.348465919494629 val_loss1.3325761556625366\n",
            "iteration 387 :train_loss:1.3484656810760498 val_loss1.332575798034668\n",
            "iteration 388 :train_loss:1.348465085029602 val_loss1.3325752019882202\n",
            "iteration 389 :train_loss:1.3484649658203125 val_loss1.3325749635696411\n",
            "iteration 390 :train_loss:1.3484647274017334 val_loss1.332574486732483\n",
            "iteration 391 :train_loss:1.3484644889831543 val_loss1.3325743675231934\n",
            "iteration 392 :train_loss:1.348463773727417 val_loss1.3325737714767456\n",
            "iteration 393 :train_loss:1.3484632968902588 val_loss1.332573413848877\n",
            "iteration 394 :train_loss:1.3484632968902588 val_loss1.3325729370117188\n",
            "iteration 395 :train_loss:1.3484632968902588 val_loss1.3325729370117188\n",
            "iteration 396 :train_loss:1.3484630584716797 val_loss1.33257257938385\n",
            "iteration 397 :train_loss:1.3484623432159424 val_loss1.332572102546692\n",
            "iteration 398 :train_loss:1.3484618663787842 val_loss1.3325718641281128\n",
            "iteration 399 :train_loss:1.3484617471694946 val_loss1.3325715065002441\n",
            "iteration 400 :train_loss:1.3484615087509155 val_loss1.332571268081665\n",
            "iteration 401 :train_loss:1.3484610319137573 val_loss1.3325707912445068\n",
            "iteration 402 :train_loss:1.3484609127044678 val_loss1.3325706720352173\n",
            "iteration 403 :train_loss:1.3484607934951782 val_loss1.3325700759887695\n",
            "iteration 404 :train_loss:1.3484601974487305 val_loss1.3325700759887695\n",
            "iteration 405 :train_loss:1.3484601974487305 val_loss1.3325693607330322\n",
            "iteration 406 :train_loss:1.3484597206115723 val_loss1.3325692415237427\n",
            "iteration 407 :train_loss:1.3484596014022827 val_loss1.332568883895874\n",
            "iteration 408 :train_loss:1.3484587669372559 val_loss1.332568645477295\n",
            "iteration 409 :train_loss:1.3484587669372559 val_loss1.3325681686401367\n",
            "iteration 410 :train_loss:1.3484584093093872 val_loss1.332567811012268\n",
            "iteration 411 :train_loss:1.348458170890808 val_loss1.3325674533843994\n",
            "iteration 412 :train_loss:1.3484584093093872 val_loss1.3325674533843994\n",
            "iteration 413 :train_loss:1.3484575748443604 val_loss1.3325672149658203\n",
            "iteration 414 :train_loss:1.3484574556350708 val_loss1.3325669765472412\n",
            "iteration 415 :train_loss:1.3484572172164917 val_loss1.3325666189193726\n",
            "iteration 416 :train_loss:1.3484569787979126 val_loss1.3325660228729248\n",
            "iteration 417 :train_loss:1.3484565019607544 val_loss1.3325656652450562\n",
            "iteration 418 :train_loss:1.3484565019607544 val_loss1.3325653076171875\n",
            "iteration 419 :train_loss:1.3484560251235962 val_loss1.332565188407898\n",
            "iteration 420 :train_loss:1.3484559059143066 val_loss1.3325647115707397\n",
            "iteration 421 :train_loss:1.348455786705017 val_loss1.3325645923614502\n",
            "iteration 422 :train_loss:1.3484551906585693 val_loss1.332564353942871\n",
            "iteration 423 :train_loss:1.3484548330307007 val_loss1.332563877105713\n",
            "iteration 424 :train_loss:1.348454475402832 val_loss1.3325635194778442\n",
            "iteration 425 :train_loss:1.3484539985656738 val_loss1.3325632810592651\n",
            "iteration 426 :train_loss:1.3484538793563843 val_loss1.3325629234313965\n",
            "iteration 427 :train_loss:1.3484538793563843 val_loss1.3325625658035278\n",
            "iteration 428 :train_loss:1.3484538793563843 val_loss1.3325623273849487\n",
            "iteration 429 :train_loss:1.3484532833099365 val_loss1.3325620889663696\n",
            "iteration 430 :train_loss:1.3484532833099365 val_loss1.332561731338501\n",
            "iteration 431 :train_loss:1.3484526872634888 val_loss1.3325614929199219\n",
            "iteration 432 :train_loss:1.348452091217041 val_loss1.3325613737106323\n",
            "iteration 433 :train_loss:1.3484516143798828 val_loss1.332560658454895\n",
            "iteration 434 :train_loss:1.348451852798462 val_loss1.3325603008270264\n",
            "iteration 435 :train_loss:1.3484512567520142 val_loss1.3325603008270264\n",
            "iteration 436 :train_loss:1.3484512567520142 val_loss1.3325600624084473\n",
            "iteration 437 :train_loss:1.3484504222869873 val_loss1.3325597047805786\n",
            "iteration 438 :train_loss:1.3484506607055664 val_loss1.3325594663619995\n",
            "iteration 439 :train_loss:1.3484506607055664 val_loss1.3325591087341309\n",
            "iteration 440 :train_loss:1.3484506607055664 val_loss1.3325588703155518\n",
            "iteration 441 :train_loss:1.3484501838684082 val_loss1.3325586318969727\n",
            "iteration 442 :train_loss:1.3484501838684082 val_loss1.3325583934783936\n",
            "iteration 443 :train_loss:1.348449468612671 val_loss1.332558035850525\n",
            "iteration 444 :train_loss:1.3484491109848022 val_loss1.3325575590133667\n",
            "iteration 445 :train_loss:1.3484488725662231 val_loss1.3325574398040771\n",
            "iteration 446 :train_loss:1.3484487533569336 val_loss1.3325574398040771\n",
            "iteration 447 :train_loss:1.3484488725662231 val_loss1.332556962966919\n",
            "iteration 448 :train_loss:1.348448395729065 val_loss1.3325567245483398\n",
            "iteration 449 :train_loss:1.3484481573104858 val_loss1.3325562477111816\n",
            "iteration 450 :train_loss:1.3484477996826172 val_loss1.332556128501892\n",
            "iteration 451 :train_loss:1.3484477996826172 val_loss1.3325560092926025\n",
            "iteration 452 :train_loss:1.348447322845459 val_loss1.3325554132461548\n",
            "iteration 453 :train_loss:1.3484469652175903 val_loss1.3325551748275757\n",
            "iteration 454 :train_loss:1.3484467267990112 val_loss1.3325551748275757\n",
            "iteration 455 :train_loss:1.3484468460083008 val_loss1.3325546979904175\n",
            "iteration 456 :train_loss:1.3484463691711426 val_loss1.3325543403625488\n",
            "iteration 457 :train_loss:1.3484456539154053 val_loss1.3325543403625488\n",
            "iteration 458 :train_loss:1.3484461307525635 val_loss1.3325539827346802\n",
            "iteration 459 :train_loss:1.3484456539154053 val_loss1.332553744316101\n",
            "iteration 460 :train_loss:1.3484456539154053 val_loss1.3325533866882324\n",
            "iteration 461 :train_loss:1.3484450578689575 val_loss1.3325531482696533\n",
            "iteration 462 :train_loss:1.3484448194503784 val_loss1.3325527906417847\n",
            "iteration 463 :train_loss:1.3484447002410889 val_loss1.3325525522232056\n",
            "iteration 464 :train_loss:1.3484443426132202 val_loss1.3325525522232056\n",
            "iteration 465 :train_loss:1.3484447002410889 val_loss1.3325519561767578\n",
            "iteration 466 :train_loss:1.3484442234039307 val_loss1.3325517177581787\n",
            "iteration 467 :train_loss:1.3484435081481934 val_loss1.3325515985488892\n",
            "iteration 468 :train_loss:1.3484435081481934 val_loss1.33255136013031\n",
            "iteration 469 :train_loss:1.3484432697296143 val_loss1.3325510025024414\n",
            "iteration 470 :train_loss:1.3484429121017456 val_loss1.3325508832931519\n",
            "iteration 471 :train_loss:1.348442554473877 val_loss1.3325505256652832\n",
            "iteration 472 :train_loss:1.3484426736831665 val_loss1.332550287246704\n",
            "iteration 473 :train_loss:1.348442554473877 val_loss1.3325501680374146\n",
            "iteration 474 :train_loss:1.3484420776367188 val_loss1.3325496912002563\n",
            "iteration 475 :train_loss:1.348441481590271 val_loss1.3325494527816772\n",
            "iteration 476 :train_loss:1.348441481590271 val_loss1.3325492143630981\n",
            "iteration 477 :train_loss:1.348441481590271 val_loss1.3325490951538086\n",
            "iteration 478 :train_loss:1.3484413623809814 val_loss1.3325488567352295\n",
            "iteration 479 :train_loss:1.348441243171692 val_loss1.3325484991073608\n",
            "iteration 480 :train_loss:1.3484406471252441 val_loss1.3325482606887817\n",
            "iteration 481 :train_loss:1.3484407663345337 val_loss1.3325480222702026\n",
            "iteration 482 :train_loss:1.3484400510787964 val_loss1.3325480222702026\n",
            "iteration 483 :train_loss:1.3484399318695068 val_loss1.332547664642334\n",
            "iteration 484 :train_loss:1.3484398126602173 val_loss1.3325473070144653\n",
            "iteration 485 :train_loss:1.3484394550323486 val_loss1.3325470685958862\n",
            "iteration 486 :train_loss:1.3484396934509277 val_loss1.3325470685958862\n",
            "iteration 487 :train_loss:1.3484389781951904 val_loss1.3325464725494385\n",
            "iteration 488 :train_loss:1.3484386205673218 val_loss1.3325462341308594\n",
            "iteration 489 :train_loss:1.3484389781951904 val_loss1.3325459957122803\n",
            "iteration 490 :train_loss:1.3484382629394531 val_loss1.3325457572937012\n",
            "iteration 491 :train_loss:1.3484383821487427 val_loss1.3325456380844116\n",
            "iteration 492 :train_loss:1.3484383821487427 val_loss1.3325453996658325\n",
            "iteration 493 :train_loss:1.348437786102295 val_loss1.3325450420379639\n",
            "iteration 494 :train_loss:1.3484376668930054 val_loss1.3325445652008057\n",
            "iteration 495 :train_loss:1.3484371900558472 val_loss1.3325448036193848\n",
            "iteration 496 :train_loss:1.3484370708465576 val_loss1.3325443267822266\n",
            "iteration 497 :train_loss:1.3484371900558472 val_loss1.332543969154358\n",
            "iteration 498 :train_loss:1.3484371900558472 val_loss1.3325440883636475\n",
            "iteration 499 :train_loss:1.3484365940093994 val_loss1.3325438499450684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "BmYI_WFUzW9_",
        "outputId": "e8f074c4-7c2d-4f6f-b1c4-caab3f5f0ea7"
      },
      "source": [
        "plotLearningCurve()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtqG8LbXzpRf",
        "outputId": "1fdc710b-7ef0-4ca5-c0f8-218f0d7afae2"
      },
      "source": [
        "predY=predict(parameters, test_X.T)\n",
        "print(\"Pred_Y:\",predY)\n",
        "MAPE = MAPE(test_Y,predY)\n",
        "print(\"MAPE:\",MAPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2.065869  2.0675807 2.064593  ... 2.0725608 2.0759635 2.0660887]], shape=(1, 3000), dtype=float32)\n",
            "Pred_Y: tf.Tensor([[2.065869  2.0675807 2.064593  ... 2.0725608 2.0759635 2.0660887]], shape=(1, 3000), dtype=float32)\n",
            "MAPE: tf.Tensor(60.67471, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN7JiyCOse6V"
      },
      "source": [
        "*  *** Is your model overfitting?***\n",
        "\n",
        "Answer: No, the model is not overfitting anywhere as there is least or no differene between Validation loss and Training loss. This is evident from the learning curves above.\n",
        "*   ***Does your validation loss continue to decrease all the way to the last iteration or does it flatten after a certain number of iterations? ***\n",
        "\n",
        "Answer: Validation loss almost flattened after 100 iterations.\n",
        "*   ***Does your training loss fluctuate or is it \n",
        "monotonically decreasing? ***\n",
        "\n",
        "Answer: Training and validation loss are at same consistency as observed in the plots above and hence got flattened after cetain number of iterations.\n",
        "*  *** Out of the hyperparameters you tried, what combination of learning rate, number of iterations, and the number of neurons in the hidden layer gave you the best validation loss?***\n",
        "\n",
        "Answer: Combination of learning rate = 0.01, number of iterartions=100 and number of neurons=100 gave me the least validation loss of 51.57 percent.\n",
        "As we increase iterations and changing nh, loss is being consistent with slight variation around 60 percent.\n",
        "\n",
        "\n"
      ]
    }
  ]
}